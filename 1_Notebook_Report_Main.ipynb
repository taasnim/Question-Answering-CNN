{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloze Form Question Answering Using Deep Learning\n",
    "\n",
    "Senadeera Damith Chamalke (G1702907E)<br />\n",
    "Mohiuddin Mohammed Tasnim (G1702965K)<br />\n",
    "Nguyen Thanh Tung (G1702496B)<br />\n",
    "Jwalapuram Prathyusha (G1802075H)<br />\n",
    "\n",
    "\n",
    "This project has been hosted on GitHub at https://github.com/tasnim007/Question-Answering-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    " \n",
    "Machine reading comprehension or question answering is a significant research problem in the field of natural language processing. Traditional natural language processing techniques used heavy, hand-crafted pre-processing of the context, like parsing the document and the question through grammatical and syntactical parsers, extracting verb frames or predicate-argument pairs, etc.\n",
    " \n",
    "Deep learning has emerged as a promising candidate for the task of question answering mainly due to the fact that deep learning models can learn features without heavy pre-processing. Therefore we thought it would be interesting to investigate the application of different deep learning techniques for this question answering task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    " \n",
    "Reading comprehension is a form of a question-answering problem that tests the ability of a machine to read and understand documents by providing a model with a document/context from which the model has to find the answer to a given query. The task which we specifically focused on is called `cloze form question answering`. The speciality of **cloze form** question answering is that the query only has a single word answer, which is present in the document/passage. \n",
    "\n",
    "For our task, we use the CNN dataset proposed by [Hermann et al (2015)](https://arxiv.org/abs/1506.03340). It was created by using news articles from CNN as the document/passage, and generating the query by using an abstractive summary of the passage and replacing one of the words in the query with a placeholder. The task is to predict this placeholder. Since it may be possible to solve the task by training a language model on other news articles, the creators anonymise the words (persons/organizations/locations, etc.) which are possible answer candidates by replacing them with unique, numbered placeholders, henceforth referred to as “entities”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    " \n",
    "The total data set of $387,420$ stories was already divided into a training set of $380,298$ stories, a validation set of $3,924$ stories and a testing set of $3,198$ stories. For the purpose of exploring this data set we analysed how the tokens and entities (candidate answers) are distributed in the passage. Also, another thing we tried to visualize is that how many times the correct answer occurs in the documents along with the distribution of tokens and entities in questions.\n",
    " \n",
    "Therefore we analysed separately the statistics for these divided training, validation and testing sets of data using the histogram distributions and central tendency measures for the above mentioned scenarios with the aid of matplotlib python library for data visualization. \n",
    "\n",
    "**Please refer to the separate notebook [2_Exploring_Data_Codes.ipynb] for codes and details of data exploration diagram generation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Training dataset\n",
    " \n",
    "In the training data set which consisted of $380,298$ examples, there were $118,432$ unique tokens and $527$ unique entities (candidate answers).\n",
    "\n",
    "<img src=\"images/vs_pie_1.png\"> \n",
    " \n",
    "### The Histogram for the distribution of number of tokens in training documents:\n",
    "\n",
    "<img src=\"images/vs_img_1.png\"> \n",
    "\n",
    "According to this histogram distribution, in a majority of the documents in the training corpus it can be seen that there are around $250$ to $800$ words and the distribution is skewed leftwards.\n",
    " \n",
    "In a training document it can include from a minimum number of $8$ tokens to a maximum number of $2000$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in training documents are given below.\n",
    " \n",
    "`The mode =  474`\n",
    "\n",
    "`The median = 700.0`\n",
    "\n",
    "`The mean = 761.81`\n",
    "\n",
    "### The Histogram for the distribution of number of entities in training documents.\n",
    " \n",
    "<img src=\"images/vs_img_2.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the training corpus it can be seen that there are around $30$ to $80$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a training document it can include from a minimum number of $1$ entity to a maximum number of $745$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in training documents are  given below.\n",
    " \n",
    "`The mode =  34`\n",
    "\n",
    "`The median = 49.0`\n",
    "\n",
    "`The mean = 56.09`\n",
    "\n",
    "\n",
    "### The Histogram for the distribution of number of occurrences of the answer entity in training docs.\n",
    " \n",
    "<img src=\"images/vs_img_3.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the training corpus it can be seen that the answer entity only occurs once and the distribution is skewed leftwards.\n",
    " \n",
    "In a training document the answer entity can occur from a minimum of $1$ time to a maximum of $61$ times.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of occurrences of the answer entity in training documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 4.0`\n",
    "\n",
    "`The mean = 6.2`\n",
    "\n",
    "### The Histogram for the distribution of number of tokens in training questions.\n",
    " \n",
    "<img src=\"images/vs_img_4.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the training corpus it can be seen that there are around $8 $to $18$ tokens and the distribution is skewed leftwards.\n",
    " \n",
    "In a training question it can include from a minimum number of $1$ tokens to a maximum number of $47$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in training questions are  given below.\n",
    " \n",
    "`The mode =  12`\n",
    "\n",
    "`The median = 12.0`\n",
    "\n",
    "`The mean = 12.47`\n",
    "\n",
    "### The histogram for the distribution of number of entities in training questions.\n",
    " \n",
    "<img src=\"images/vs_img_5.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the training corpus it can be seen that there are around $0$ to $3$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a training question it can include from a minimum number of $0$ entities to a maximum number of $9$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in training documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 1.0`\n",
    "\n",
    "`The mean = 1.11`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Validation dataset\n",
    " \n",
    "In the validation data set which consisted of $3,924$ examples, there were $24,204$ unique tokens and $187$ unique entities (candidate answers).\n",
    "\n",
    "<img src=\"images/vs_pie_2.png\">\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in validation documents.\n",
    "\n",
    "<img src=\"images/vs_img_6.png\">\n",
    "\n",
    "According to this histogram distribution, in a majority of the documents in the validation corpus it can be seen that there are around $300$ to $700$ words and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation document it can include from a minimum number of 94 tokens to a maximum number of 1989 tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in validation documents are given below.\n",
    " \n",
    "`The mode =  1102`\n",
    "\n",
    "`The median = 710.0`\n",
    "\n",
    "`The mean = 762.74`\n",
    "\n",
    "### The Histogram for the distribution of number of entities in validation documents.\n",
    " \n",
    "<img src=\"images/vs_img_7.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the validation corpus it can be seen that there are around $20$ to $70$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation document it can include from a minimum number of $2$ entity to a maximum number of $223$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in validation documents are  given below.\n",
    " \n",
    "`The mode =  40`\n",
    "\n",
    "`The median = 48.0`\n",
    "\n",
    "`The mean = 56.57`\n",
    "\n",
    "### The Histogram for the distribution of number of occurrences of the answer entity in validation docs.\n",
    " \n",
    "<img src=\"images/vs_img_8.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the validation corpus it can be seen that the answer entity is only occurred $1$ time and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation document the answer entity can occur from a minimum number of $1$ times to a maximum number of $44$ times.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of occurrences of the answer entity in validation documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 4.0`\n",
    "\n",
    "`The mean = 6.480632008154944`\n",
    "\n",
    "### The Histogram for the distribution of number of tokens in validation questions.\n",
    " \n",
    "<img src=\"images/vs_img_9.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the validation corpus it can be seen that there are around $10$ to $20$ tokens and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation question it can include from a minimum number of $3$ tokens to a maximum number of $37$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in validation questions are  given below.\n",
    " \n",
    "`The mode =  12`\n",
    "\n",
    "`The median = 13.0`\n",
    "\n",
    "`The mean = 13.289755351681958`\n",
    "\n",
    "### The Histogram for the distribution of number of entities in validation questions.\n",
    " \n",
    "<img src=\"images/vs_img_10.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the validation corpus it can be seen that there are around $0$ to $3$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation question it can include from a minimum number of $0$ entities to a maximum number of $8$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in validation documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 1.0`\n",
    "\n",
    "`The mean = 1.2313965341488278`\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Testing dataset\n",
    " \n",
    "In the testing data set which consisted of $3,198$ examples, there were $23,145$ unique tokens and $396$ unique entities (candidate answers).\n",
    "\n",
    "<img src=\"images/vs_pie_3.png\">\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in testing documents.\n",
    " \n",
    "<img src=\"images/vs_img_11.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the testing corpus it can be seen that there are around $250$ to $800$ words and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing document it can include from a minimum number of $89$ tokens to a maximum number of $1989$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in testing documents are given below.\n",
    " \n",
    "`The mode =  1095`\n",
    "\n",
    "`The median = 652.0`\n",
    "\n",
    "`The mean = 716.442464040025`\n",
    " \n",
    "### The Histogram for the distribution of number of entities in testing documents.\n",
    " \n",
    "<img src=\"images/vs_img_12.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the testing corpus it can be seen that there are around $1$ to $80$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing document it can include from a minimum number of $4$ entity to a maximum number of $584$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in testing documents are  given below.\n",
    " \n",
    "`The mode =  31`\n",
    "\n",
    "`The median = 43.0`\n",
    "\n",
    "`The mean = 51.515322076297686`\n",
    " \n",
    "### The Histogram for the distribution of number of occurrences of the answer entity in testing docs.\n",
    " \n",
    "<img src=\"images/vs_img_13.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the testing corpus it can be seen that the answer entity is only occurred $1$ time and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing document the answer entity can occur from a minimum number of $1$ times to a maximum number of $47$ times.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of occurrences of the answer entity in testing documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 4.0`\n",
    "\n",
    "`The mean = 6.363352095059412`\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in testing questions.\n",
    " \n",
    "<img src=\"images/vs_img_14.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the testing corpus it can be seen that there are around $8$ to $15$ tokens and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing question it can include from a minimum number of $3$ tokens to a maximum number of $37$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in testing questions are  given below.\n",
    " \n",
    "`The mode =  13`\n",
    "\n",
    "`The median = 13.0`\n",
    "\n",
    "`The mean = 13.72514071294559`\n",
    " \n",
    "### The Histogram for the distribution of number of entities in testing questions.\n",
    " \n",
    "<img src=\"images/vs_img_15.png\">\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the testing corpus it can be seen that there are around $0$ to $2$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing question it can include from a minimum number of $0$ entities to a maximum number of $6$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in testing documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 1.0`\n",
    "\n",
    "`The mean = 1.1106941838649156`\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation in the whole data set is that most frequent words are the articles like “the”, “to”. “of”, “an”, “a” and “in” as expected.\n",
    " \n",
    "Comparing all these distributions with respect to tokens and entities in all training, validation, and testing data sets we can see that there is a common left skewness and in general, all the distributions tend to be similar in the 3 sets of training, validation and testing data. Therefore, we can assume that the training of our models will work for the validation and testing sets. Also, another observation is that in all the 3 sets most of the times the answer entity only occurs one time in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Based Models Code and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you are running on the virtual environment mentioned in the course [Github repository](https://github.com/xbresson/CE7454_2018). If not please run the `environment.yml` file. \n",
    "\n",
    "\n",
    "If you are running for the first time please pip install the `tarfile` by executing the following command. It will be needed for extracting the downloaded `CNN dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries:\n",
    "First we need to import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import zipfile\n",
    "import tarfile\n",
    "import gzip\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU:\n",
    "\n",
    "It is highly recommended to run this code on GPU.\n",
    "\n",
    "- For *Unidirectional Attention Model* time for a single epoch on GPU (Nvidia GTX 1080 Ti): 0.7hr\n",
    "- For *Bidirectional Attention Model* time for a single epoch on GPU (Nvidia GTX 1080 Ti): 10.2hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0 #select gpu\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    \n",
    "else:\n",
    "    print('cuda not available')\n",
    "    gpu_id = -1\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CNN Dataset:\n",
    "\n",
    "Our CNN dataset's default directory is `./data/cnn/`. There are three files: `train.txt`, `test.txt`, and `dev.txt`.\n",
    "The following function checks whether the CNN dataset is in the default directory or not. If not, then it will download the dataset from [here](http://cs.stanford.edu/~danqi/data/cnn.tar.gz) and extract it to the default directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_CNN_dataset_exists(path_data='./data/'):\n",
    "    flag_train_data = os.path.isfile(path_data + 'cnn/train.txt')  \n",
    "    flag_test_data = os.path.isfile(path_data + 'cnn/test.txt') \n",
    "    flag_dev_data = os.path.isfile(path_data + 'cnn/dev.txt') \n",
    "    if flag_train_data==False or flag_test_data==False or flag_dev_data==False:\n",
    "        print('CNN dataset missing - downloading...')\n",
    "        if not os.path.exists(path_data):\n",
    "            os.makedirs(path_data)\n",
    "        url = \"http://cs.stanford.edu/~danqi/data/cnn.tar.gz\"\n",
    "        os.system('wget http://cs.stanford.edu/~danqi/data/cnn.tar.gz -P ./data/')\n",
    "        tar = tarfile.open('./data/cnn.tar.gz', \"r:gz\")\n",
    "        tar.extractall('./data/')\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"CNN dataset is already there!\")\n",
    "        \n",
    "\n",
    "check_CNN_dataset_exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train, test, and dev data:\n",
    "\n",
    "Original entity markers' indices are arbitrarily generated. [Danqi et. al.](https://arxiv.org/pdf/1606.02858v2.pdf) reported that relabeling the entity markers based on their first occurrence in the passage and question makes training to converge faster.\n",
    "\n",
    "Each of the `train.txt`, `test.txt`, and `dev.txt` file contains contents in the format: `Question, Answer, Document`.\n",
    "\n",
    "In the `load_data()` function, we read a file (`train.txt/test.txt/dev.txt`) and return the all the documents, questions, and answers in separate lists. If `relabeling=True`, we relabel the entity markers based on their first occurrence as stated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file, max_example=None, relabeling=True):\n",
    "    \"\"\"\n",
    "        load CNN data from {train | dev | test}.txt\n",
    "        max_example: if it is None, all the examples will be read.\n",
    "        relabeling: relabel the entities by their first occurence if it is True.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    f = open(in_file, 'r', encoding='utf-8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        question = line.strip().lower()\n",
    "        answer = f.readline().strip()\n",
    "        document = f.readline().strip().lower()\n",
    "\n",
    "        if relabeling:\n",
    "            q_words = question.split(' ')\n",
    "            d_words = document.split(' ')\n",
    "            assert answer in d_words\n",
    "\n",
    "            entity_dict = {}\n",
    "            entity_id = 0\n",
    "            for word in d_words + q_words:\n",
    "                if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                    entity_dict[word] = '@entity' + str(entity_id)\n",
    "                    entity_id += 1\n",
    "\n",
    "            q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "            d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "            answer = entity_dict[answer]\n",
    "\n",
    "            question = ' '.join(q_words)\n",
    "            document = ' '.join(d_words)\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        num_examples += 1\n",
    "\n",
    "        f.readline()\n",
    "        if (max_example is not None) and (num_examples >= max_example):\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    print('#Examples: %d' % len(documents))\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Dictionary\n",
    "\n",
    "In the `build_dict()` function, we build a dictionary of vocabularies. For building the dictionary, we keep the most frequent 120K words. We tried with smaller vocabulary size like 50K, but it did not give the optimal result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=120000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "    print('#Words: %d -> %d' % (len(word_count), len(ls)))\n",
    "    for key in ls[:5]:\n",
    "        print(key)\n",
    "    print('...')\n",
    "    for key in ls[-5:]:\n",
    "        print(key)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    \n",
    "    vocab_dict = {w[0]: index + 2 for (index, w) in enumerate(ls)}\n",
    "    vocab_dict['<UNK>'] = 0\n",
    "    vocab_dict['<PAD>'] = 1\n",
    "    \n",
    "    \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Data (train/test/dev):\n",
    "\n",
    "We need to vectorize our data (documents/questions/answers). For documents and questions, we use the `word_dict` dictionary for vectorizing, while for vectorizing answers we use the `entity_dict` dictionary. Basically, the indices of the dictionaries corresponding to the relevant words are replaced in input data in order to process the inputs into our deep learning models. Meanwhile, the maximum document and question lengths are also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(examples, word_dict, entity_dict):\n",
    "    \"\"\"\n",
    "        Vectorize `examples`.\n",
    "        in_x1, in_x2: sequences for document and question respecitvely.\n",
    "        in_y: label\n",
    "        in_l: whether the entity label occurs in the document.\n",
    "    \n",
    "    \"\"\"\n",
    "    in_x1 = []\n",
    "    in_x2 = []\n",
    "    in_l = np.zeros((len(examples[0]), len(entity_dict)))#.astype(config._floatX)\n",
    "    in_y = []\n",
    "    \n",
    "    max_d = 0\n",
    "    max_q = 0\n",
    "    \n",
    "    \n",
    "    for idx, (d, q, a) in enumerate(zip(examples[0], examples[1], examples[2])):\n",
    "        d_words = d.split(' ')\n",
    "        q_words = q.split(' ')\n",
    "        assert (a in d_words)\n",
    "        seq1 = [word_dict[w] if w in word_dict else 0 for w in d_words]\n",
    "        seq2 = [word_dict[w] if w in word_dict else 0 for w in q_words]\n",
    "        \n",
    "        if max_d < len(seq1):\n",
    "            max_d = len(seq1)\n",
    "        if max_q < len(seq2):\n",
    "            max_q = len(seq2)\n",
    "        \n",
    "        if (len(seq1) > 0) and (len(seq2) > 0):\n",
    "            in_x1.append(seq1)\n",
    "            in_x2.append(seq2)\n",
    "            in_l[idx, [entity_dict[w] for w in d_words if w in entity_dict]] = 1.0\n",
    "            in_y.append(entity_dict[a] if a in entity_dict else 0)\n",
    "        if (idx % 10000 == 0):\n",
    "            print('Vectorization: processed %d / %d' % (idx, len(examples[0])))\n",
    "\n",
    "\n",
    "    return in_x1, in_x2, in_l, in_y, max_d, max_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mini Batches:\n",
    "\n",
    "In order to get the advantage of using GPUs for running our models, we create minibatches of data before inputting them to our model. From this function, we can create the minibatches of data according to desired minibatch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data with Padding:\n",
    "\n",
    "This function is used to pad the data inputs which are less lengthier than the maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs, max_l):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    #max_len = np.max(lengths)\n",
    "    x = np.zeros((n_samples, max_l)).astype('int32')\n",
    "    #x_len = np.zeros((n_samples, 1)).astype(config._floatX)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        #x_len[idx,0] = len(seq)\n",
    "    return x, np.array(lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrating Final Input  Data to Model:\n",
    "\n",
    "This function calls the function to prepare minibatches and then the padding function to pad the the input data accordingly inorder to prepare the final input data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_examples(x1, x2, l, y, batch_size, max_d, max_q):\n",
    "    \"\"\"\n",
    "        Divide examples into batches of size `batch_size`.\n",
    "    \"\"\"\n",
    "    minibatches = get_minibatches(len(x1), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_x1 = [x1[t] for t in minibatch]\n",
    "        mb_x2 = [x2[t] for t in minibatch]\n",
    "        #mb_l = l[minibatch]\n",
    "        mb_y = [y[t] for t in minibatch]\n",
    "        mb_y = np.array(mb_y)\n",
    "        mb_x1, x1_len = prepare_data(mb_x1, max_d)\n",
    "        mb_x2, x2_len = prepare_data(mb_x2, max_q)\n",
    "        all_ex.append((mb_x1, x1_len, mb_x2, x2_len, mb_y))\n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pretrained Embeddings:\n",
    "\n",
    "We tried with three types of pretrained embeddings: [**Glove** from Stanford University](https://nlp.stanford.edu/projects/glove/), [**Fasttext** from facebook](https://fasttext.cc/), and `uniform random embeddings`. We used embedding dimension of 300. Following functions generates pretrained embeddings from glove/fasttext/random according to our specification.\n",
    "\n",
    "The default directory for glove pretrained embeddings is `./data/glove/`.\n",
    "The default directory for glove pretrained embeddings is `./data/fasttext/`\n",
    "If you want to use pretrained embeddings (glove/fasttext) and they are not in the default directory, it will be downloaded by the following codes. \n",
    "\n",
    "**Caution:<font color='red'> Dowloading pretrained embeddings may take some times. </font>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_embeddings_glove(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict` from glove pretrained embeddings.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(in_file)==False:\n",
    "        print('Glove pretrained embedding missing.. Downloading...')\n",
    "        if not os.path.exists('./data/glove/'):\n",
    "            os.makedirs('./data/glove/')\n",
    "        os.system('wget http://nlp.stanford.edu/data/glove.6B.zip -P ./data/glove/')\n",
    "        zip_ref = zipfile.ZipFile('./data/glove/glove.6B.zip', 'r')\n",
    "        zip_ref.extractall('./data/glove/')\n",
    "        zip_ref.close()\n",
    "        \n",
    "    \n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        for line in open(in_file, encoding='utf-8').readlines():\n",
    "            sp = line.split()\n",
    "            \n",
    "            #print(\"Length = \",len(sp))\n",
    "            \n",
    "            assert len(sp) == dim + 1 \n",
    "            if sp[0] in word_dict:\n",
    "                pre_trained += 1\n",
    "                embeddings[word_dict[sp[0]]] = [float(x) for x in sp[1:]]\n",
    "        print('Pre-trained: %d (%.2f%%)' %\n",
    "              (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings_fasttext(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`from fasttext pretrained embeddings.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(in_file)==False:\n",
    "        print('Fasttext pretrained embedding missing.. Downloading...')\n",
    "        if not os.path.exists('./data/fasttext/'):\n",
    "            os.makedirs('./data/fasttext/')\n",
    "        os.system('wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec -P ./data/fasttext/')\n",
    "\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading fasttext embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        with io.open(in_file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 0:\n",
    "                    split = line.split()\n",
    "                    assert len(split) == 2\n",
    "                    #assert _emb_dim_file == int(split[1])\n",
    "                else:\n",
    "                    word, vect = line.rstrip().split(' ', 1)        \n",
    "                    if word in word_dict:\n",
    "                        pre_trained += 1\n",
    "                        vect = np.fromstring(vect, sep=' ')\n",
    "                        #print(vect.shape)\n",
    "                        embeddings[word_dict[word]] = vect\n",
    "        print('Pre-trained: %d (%.2f%%)' % (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings_random(word_dict, dim):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict` using uniform random.\n",
    "    \"\"\"\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings(word_dict, dim, in_file=None, init_from='random'):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`\n",
    "    \"\"\"\n",
    "    if init_from == 'glove':\n",
    "        return gen_embeddings_glove(word_dict, dim, in_file)\n",
    "    elif init_from == 'fasttext':\n",
    "        return gen_embeddings_fasttext(word_dict, dim, in_file)\n",
    "    else:\n",
    "        return gen_embeddings_random(word_dict, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional Attention Model\n",
    "\n",
    "The following figure depicts our `Unidirectional Attention Model`.\n",
    "\n",
    "<img src=\"images/unidirectional_attention.jpg\"> \n",
    "Figure: Unidirectional Attention Model\n",
    "\n",
    "We first convert each word of the passage to the corresponding **word embedding vectors**. We run our model with both random and pretrained word embeddings like `glove` and `fasttext`. Our word embeddings are trainable in both of the cases. \n",
    "\n",
    "The word embeddings do not capture the context of the words in the sentence. For this reason, we use Recurrent Neural Networks (RNN). We use bidirectional RNNs. The benefit of bidirectional RNN over unidirectional one is that *in bidirectional RNNs a word does not only have the previous words information from the forward RNNs but also have the information from the future words from the backward RNNs*. By concatenating the outputs of the forward and the backward RNNs, we get the **Contextual Embeddings** for each word.\n",
    "\n",
    "For the RNNs, we tried with both LSTMs and GRUs.\n",
    "\n",
    "Similarly for questions, we first convert them to word embeddings. Then by using bidirectional RNNs, we create the **Query Vector** by taking only the last RNN output. Since our query size is small (One single sentence), the bidirectional RNNs (LSTM/GRU) can retain all the information into a single vector. Thus the Query Vector has all the information regarding the corresponding query input. \n",
    "\n",
    "Now the **goal** is to *compare the Query Vector and all the contextual embeddings, and select the pieces of information that are relevant to the question*. For this purpose, we use the `attention mechanism`. We use the *Multiplicative Attention* mechanism to pay attention from the query vector to each of the passage words represented by the contextual embeddings. This gives us the **Attention Scores**. To compute the probability distribution of the attention scores, we use **softmax**. \n",
    "\n",
    "Now we have the **Attention Distribution** values. We compute the **Weighted Score** by *multiplying the attention score to their respective contextual embeddings and summing them up*.\n",
    "\n",
    "Finally, we use a *linear layer* to get the **Logits**.\n",
    "\n",
    "\n",
    "We use **Cross Entropy Loss** for the loss calculation. \n",
    "\n",
    "For optimization, we use  **Stochastic Gradient Descent (SGD)** starting with learning rate $0.1$. We reduce the learning rate by a factor of $1.2$, if `dev_loss > 0.99* dev_loss_old`.\n",
    "\n",
    "We use minibatch SGD with batch size $128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unidirectional_Attention_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, num_layers=1, drp_rate=0.25):\n",
    "        super(Unidirectional_Attention_Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_gru = nn.GRU(embedding_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.ws = nn.Linear(hidden_size*2, hidden_size*2) # mult by 2 for bidirectional\n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):   \n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, hidden = self.context_gru(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            #Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            \n",
    "            return output, hidden\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ##For Documents/passages\n",
    "        doc_output, doc_hidden = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        # output shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        ques_fwd_h = ques_hidden[0:ques_hidden.size(0):2]\n",
    "        ques_bwd_h = ques_hidden[1:ques_hidden.size(0):2]\n",
    "        ques_hidden = torch.cat([ques_fwd_h, ques_bwd_h], dim=2)   \n",
    "        #print(\"After hid: \", ques_hidden.size())\n",
    "        \n",
    "        q_ws = self.ws(ques_hidden) #q_ws shape:  torch.Size([1, bs, 256])\n",
    "        #print(\"q_ws shape: \", q_ws.size()) \n",
    "        q_ws = q_ws.squeeze().unsqueeze(1) #q_ws shape:  torch.Size([bs, 1, 256])\n",
    "        q_ws.transpose_(1,2) #q_ws shape:  torch.Size([bs, 256, 1])\n",
    "        q_ws_p = torch.bmm(doc_output, q_ws).squeeze() # q_ws_p shape: torch.Size([bs, timesteps])\n",
    "        #print(\"q_ws_p shape: \", q_ws_p.size())\n",
    "        alpha = F.softmax(q_ws_p, dim=1) #alpha shape:  torch.Size([bs, 1808])\n",
    "        #print(\"alpha shape: \", alpha.size())\n",
    "        attention = torch.mul(alpha.unsqueeze(2), doc_output) #attention shape:  torch.Size([bs, 1808, 256])\n",
    "        #print(\"attention shape: \", attention.size())\n",
    "        attention = torch.sum(attention, dim=1) #After summing attention shape:  torch.Size([bs, 256])\n",
    "        #print(\"After summing attention shape: \", attention.size())\n",
    "        \n",
    "        logits = self.linear(attention) #logits shape:  torch.Size([bs, numClasses])\n",
    "        #print(\"logits shape: \", logits.size())\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        #loss = nn.MSELoss()(y,y_target)\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Train One Single Epoch:\n",
    "\n",
    "This function takes the input data and makes the input minibatches randomly shuffle first. Then once the epoch is over it updates the learning rate according to the given policy and outputs the training loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(net,optimizer,tr_data):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    run_loss = 0\n",
    "    run_nb_data = 0\n",
    "    run_acc = 0\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(tr_data)\n",
    "    \n",
    "    for ids, (mb_x1, x1_len, mb_x2, x2_len, mb_y) in enumerate(tr_data):\n",
    "        \n",
    "        \n",
    "        x1 = Variable( torch.LongTensor(mb_x1).type(dtypeLong) , requires_grad=False)\n",
    "        x2 = Variable( torch.LongTensor(mb_x2).type(dtypeLong) , requires_grad=False)\n",
    "        target = Variable( torch.LongTensor(mb_y).type(dtypeLong) , requires_grad=False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = net.forward(x1,x2,x1_len,x2_len)\n",
    "        \n",
    "        loss = net.loss(pred,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        run_loss += (loss.detach().item())*float(mb_x1.shape[0])\n",
    "        run_nb_data += float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "        _, batch_predicted_labels = torch.max(pred, dim=1) \n",
    "        \n",
    "        \n",
    "        acc = metrics.accuracy_score(np.array(target), batch_predicted_labels.cpu().numpy())\n",
    "        run_acc += acc*float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "    acc_tr = run_acc/run_nb_data\n",
    "    loss_tr = run_loss/run_nb_data\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    return loss_tr, acc_tr\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Test the Validation and the Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_one_epoch(net,tst_data):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    run_loss = 0\n",
    "    run_nb_data = 0\n",
    "    run_acc = 0\n",
    "    \n",
    "    for ids, (mb_x1, x1_len, mb_x2, x2_len, mb_y) in enumerate(tst_data):\n",
    "        \n",
    "        \n",
    "        x1 = Variable( torch.LongTensor(mb_x1).type(dtypeLong) , requires_grad=False)\n",
    "        x2 = Variable( torch.LongTensor(mb_x2).type(dtypeLong) , requires_grad=False)\n",
    "        target = Variable( torch.LongTensor(mb_y).type(dtypeLong) , requires_grad=False)\n",
    "        \n",
    "        \n",
    "        pred = net.forward(x1,x2,x1_len,x2_len)\n",
    "        \n",
    "        loss = net.loss(pred,target)\n",
    "        \n",
    "        run_loss += (loss.detach().item())*float(mb_x1.shape[0])\n",
    "        run_nb_data += float(mb_x1.shape[0])\n",
    "        \n",
    "        _, batch_predicted_labels = torch.max(pred, dim=1) \n",
    "        \n",
    "        \n",
    "        acc = metrics.accuracy_score(np.array(target), batch_predicted_labels.cpu().numpy())\n",
    "        run_acc += acc*float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "    acc_dev = run_acc/run_nb_data\n",
    "    loss_dev = run_loss/run_nb_data\n",
    "    \n",
    "    return loss_dev, acc_dev\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 380298\n",
      "#Examples: 3924\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "\n",
    "fin_train = \"./data/cnn/train.txt\"\n",
    "fin_dev = \"./data/cnn/dev.txt\"\n",
    "\n",
    "\n",
    "train_exps = load_data(fin_train, relabeling=True)\n",
    "dev_exps = load_data(fin_dev, relabeling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Words: 118432 -> 118432\n",
      "('the', 15383021)\n",
      "(',', 13757778)\n",
      "('.', 11782121)\n",
      "('to', 7208903)\n",
      "('\"', 6967510)\n",
      "...\n",
      "('slingers', 1)\n",
      "('multi-planet', 1)\n",
      "('johnstons', 1)\n",
      "('shir', 1)\n",
      "('khurma', 1)\n"
     ]
    }
   ],
   "source": [
    "#Building dictionaries\n",
    "\n",
    "\n",
    "word_dict = build_dict(train_exps[0] + train_exps[1])\n",
    "entity_markers = list(set([w for w in word_dict.keys() if w.startswith('@entity')] + train_exps[2]))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings\n",
    "Here you can choose your initial embeddings. Use either `pretrained embeddings` (glove/fasttext) or `uniform random embeddings`. **Uncomment the one you want to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 118436 x 300\n",
      "Loading fasttext embedding file: wiki.en.vec\n",
      "Pre-trained: 79469 (67.10%)\n"
     ]
    }
   ],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: processed 0 / 380298\n",
      "Vectorization: processed 10000 / 380298\n",
      "Vectorization: processed 20000 / 380298\n",
      "Vectorization: processed 30000 / 380298\n",
      "Vectorization: processed 40000 / 380298\n",
      "Vectorization: processed 50000 / 380298\n",
      "Vectorization: processed 60000 / 380298\n",
      "Vectorization: processed 70000 / 380298\n",
      "Vectorization: processed 80000 / 380298\n",
      "Vectorization: processed 90000 / 380298\n",
      "Vectorization: processed 100000 / 380298\n",
      "Vectorization: processed 110000 / 380298\n",
      "Vectorization: processed 120000 / 380298\n",
      "Vectorization: processed 130000 / 380298\n",
      "Vectorization: processed 140000 / 380298\n",
      "Vectorization: processed 150000 / 380298\n",
      "Vectorization: processed 160000 / 380298\n",
      "Vectorization: processed 170000 / 380298\n",
      "Vectorization: processed 180000 / 380298\n",
      "Vectorization: processed 190000 / 380298\n",
      "Vectorization: processed 200000 / 380298\n",
      "Vectorization: processed 210000 / 380298\n",
      "Vectorization: processed 220000 / 380298\n",
      "Vectorization: processed 230000 / 380298\n",
      "Vectorization: processed 240000 / 380298\n",
      "Vectorization: processed 250000 / 380298\n",
      "Vectorization: processed 260000 / 380298\n",
      "Vectorization: processed 270000 / 380298\n",
      "Vectorization: processed 280000 / 380298\n",
      "Vectorization: processed 290000 / 380298\n",
      "Vectorization: processed 300000 / 380298\n",
      "Vectorization: processed 310000 / 380298\n",
      "Vectorization: processed 320000 / 380298\n",
      "Vectorization: processed 330000 / 380298\n",
      "Vectorization: processed 340000 / 380298\n",
      "Vectorization: processed 350000 / 380298\n",
      "Vectorization: processed 360000 / 380298\n",
      "Vectorization: processed 370000 / 380298\n",
      "Vectorization: processed 380000 / 380298\n",
      "Vectorization: processed 0 / 3924\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 128, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_data[6][2][50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1 #learning rate\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 20\n",
    "drp = 0.25 #drop out rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damith/miniconda3/envs/deeplearning_project/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Unidirectional_Attention_Model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict),drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [epoch length:1852s | time from start:0.5h] \tlr=1.00e-01\tloss=2.650/2.245\tacc:0.286/0.392 \n",
      "Epoch 1 [epoch length:1839s | time from start:1.0h] \tlr=1.00e-01\tloss=2.259/2.001\tacc:0.410/0.492 \n",
      "Epoch 2 [epoch length:1840s | time from start:1.5h] \tlr=1.00e-01\tloss=1.870/1.610\tacc:0.521/0.579 \n",
      "Epoch 3 [epoch length:1839s | time from start:2.0h] \tlr=1.00e-01\tloss=1.580/1.420\tacc:0.600/0.634 \n",
      "Epoch 4 [epoch length:1848s | time from start:2.6h] \tlr=1.00e-01\tloss=1.410/1.333\tacc:0.641/0.659 \n",
      "Epoch 5 [epoch length:1841s | time from start:3.1h] \tlr=1.00e-01\tloss=1.303/1.298\tacc:0.669/0.663 \n",
      "Epoch 6 [epoch length:1836s | time from start:3.6h] \tlr=1.00e-01\tloss=1.225/1.242\tacc:0.688/0.682 \n",
      "Epoch 7 [epoch length:1840s | time from start:4.1h] \tlr=1.00e-01\tloss=1.168/1.229\tacc:0.703/0.688 \n",
      "Epoch 8 [epoch length:1836s | time from start:4.6h] \tlr=8.33e-02\tloss=1.118/1.242\tacc:0.715/0.681 \n",
      "Epoch 9 [epoch length:1843s | time from start:5.1h] \tlr=8.33e-02\tloss=1.058/1.199\tacc:0.730/0.693 \n",
      "Epoch 10 [epoch length:1840s | time from start:5.6h] \tlr=8.33e-02\tloss=1.025/1.165\tacc:0.739/0.712 \n",
      "Epoch 11 [epoch length:1837s | time from start:6.1h] \tlr=6.94e-02\tloss=0.993/1.194\tacc:0.747/0.708 \n",
      "Epoch 12 [epoch length:1837s | time from start:6.6h] \tlr=6.94e-02\tloss=0.947/1.155\tacc:0.759/0.712 \n",
      "Epoch 13 [epoch length:1839s | time from start:7.2h] \tlr=5.79e-02\tloss=0.922/1.182\tacc:0.766/0.701 \n",
      "Epoch 14 [epoch length:1842s | time from start:7.7h] \tlr=4.82e-02\tloss=0.883/1.175\tacc:0.776/0.707 \n",
      "Epoch 15 [epoch length:1840s | time from start:8.2h] \tlr=4.02e-02\tloss=0.849/1.164\tacc:0.786/0.710 \n",
      "Epoch 16 [epoch length:1839s | time from start:8.7h] \tlr=3.35e-02\tloss=0.819/1.189\tacc:0.794/0.703 \n",
      "Epoch 17 [epoch length:1835s | time from start:9.2h] \tlr=2.79e-02\tloss=0.793/1.188\tacc:0.801/0.707 \n",
      "Epoch 18 [epoch length:1835s | time from start:9.7h] \tlr=2.33e-02\tloss=0.772/1.183\tacc:0.807/0.707 \n",
      "Epoch 19 [epoch length:1841s | time from start:10.2h] \tlr=1.94e-02\tloss=0.753/1.186\tacc:0.812/0.712 \n"
     ]
    }
   ],
   "source": [
    "# save results in a .txt file\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "if not os.path.exists('./logs/'):\n",
    "    os.makedirs('./logs/')\n",
    "file_name = 'logs'+'/'+time_stamp + \"unidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"unidr_fastext_sgd\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 3198\n",
      "Vectorization: processed 0 / 3198\n"
     ]
    }
   ],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss and accuracy =  1.1368197198582113 \t 0.7229518449030644\n"
     ]
    }
   ],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Attention Model\n",
    "\n",
    "The following figure depicts our `Bidirectional Attention Model`.\n",
    "\n",
    "<img src=\"images/bidirectional_attention.jpg\"> \n",
    "Figure: Bidirectional Attention Model\n",
    "\n",
    "Here the first couple of layers are similar to `Unidirectional Attention Model`. Unlike unidirectional one, from the query we do not create a single *Query Vector*. Instead, we take all the output of the RNNs to create `Query Contextual Embeddings` just like the `Passage Contextual Embeddings`. \n",
    "Then we use the **Bidirectional Attention Mechanism**. Here every word in `Passage Contextual Embeddings` pays attention to every word in `Query Contextual Embeddings`, resulting `Passage to Query Attention Scores`. Similarly, every word in `Query Contextual Embeddings` pays attention to every word in `Passage Contextual Embeddings`, resulting `Query to Passage Attention Scores`. Finally, these attention scores and contextual embeddings are combined together. We followed the approach mentioned in [Seo et al.](https://arxiv.org/abs/1611.01603). The resultant one encodes *query aware representations* of context words.\n",
    "Next, we use another bidirectional RNN to capture the interactions among passage words conditioned on the query.\n",
    "Finally, we use three linear layers sequentially to get the **Logits**.\n",
    "\n",
    "We use **Cross Entropy Loss** for the loss calculation. \n",
    "\n",
    "For optimization, we use  **Stochastic Gradient Descent (SGD)** starting with learning rate $0.1$. We reduce the learning rate by a factor of $1.2$, if `dev_loss > 0.99* dev_loss_old`.\n",
    "\n",
    "We use minibatch SGD with batch size $8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bidirectional_Attention_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, maxlen, num_layers=1, drp_rate=0.25):\n",
    "        super(Bidirectional_Attention_Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.maxlen = maxlen\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_gru = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.sim_W = nn.Linear(hidden_size*6, 1)\n",
    "        \n",
    "        self.modeling_layer = nn.GRU(hidden_size*8, hidden_size, num_layers=2, \n",
    "                                     batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.output_layer_1 = nn.Linear(hidden_size*10, 1)\n",
    "        self.output_layer_2 = nn.Linear(maxlen, 1000)\n",
    "        self.output_layer_3 = nn.Linear(1000, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):\n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, hidden = self.context_gru(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            # Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            # output shape: (batch_size, seq_len, hidden_size * num_directions) --seq_len is the largest lengths in the minibatch\n",
    "            # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "            \n",
    "            return output, hidden\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ## contextual embedding for documents and ques\n",
    "        doc_output, doc_hidden = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        \n",
    "        ## Attention Flow\n",
    "        # Similarity Matrix calcuation\n",
    "        doc_seq_len = doc_output.size(1) # T\n",
    "        ques_seq_len = ques_output.size(1) # J\n",
    "        shape = (doc_output.size(0), doc_seq_len, ques_seq_len, 2*self.hidden_size) # (N, T, J, 2d)\n",
    "        #print(\"T: \", doc_seq_len, \" J: \", ques_seq_len, \" Shape: \", shape)\n",
    "        doc_output_extra = doc_output.unsqueeze(2)  # (N, T, 1, 2d)\n",
    "        doc_output_extra = doc_output_extra.expand(shape) # (N, T, J, 2d)\n",
    "        ques_output_extra = ques_output.unsqueeze(1)  # (N, 1, J, 2d)\n",
    "        ques_output_extra = ques_output_extra.expand(shape) # (N, T, J, 2d)\n",
    "        elmwise_mul = torch.mul(doc_output_extra, ques_output_extra) # (N, T, J, 2d)\n",
    "        #print(\"doc: \", doc_output_extra.size(), \" ques: \", ques_output_extra.size(), \" elem: \", elmwise_mul.size())\n",
    "        cat_data = torch.cat((doc_output_extra, ques_output_extra, elmwise_mul), 3) # (N, T, J, 6d), [h;u;h◦u]\n",
    "        similarity_matrix = self.sim_W(cat_data).squeeze() # (N, T, J)\n",
    "        #print(\"cat: \", cat_data.size(), \" similarity_matrix: \", similarity_matrix.size())\n",
    "        \n",
    "        \n",
    "        a = F.softmax(similarity_matrix, dim=2) # (bs, T, J)\n",
    "        doc2ques_attention = torch.bmm(a, ques_output) # (bs, T, 2*d)\n",
    "        #print(\"a: \", a.size(), \" doc2ques: \", doc2ques_attention.size())\n",
    "        b = F.softmax(torch.max(similarity_matrix, dim=2)[0], dim=1).unsqueeze(1) # (bs, 1, T)\n",
    "        ques2doc_attention = torch.bmm(b, doc_output).squeeze()  # (bs, 2d)\n",
    "        ques2doc_attention = ques2doc_attention.unsqueeze(1).expand(-1, doc_seq_len, -1) # (bs, T, 2*d)\n",
    "        # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
    "\n",
    "        G = torch.cat((doc_output, doc2ques_attention, doc_output.mul(doc2ques_attention), \n",
    "                       doc_output.mul(ques2doc_attention)), 2) # (bs, T, 8d)\n",
    "        #print(\"b: \", b.size(), \" ques2doc: \", ques2doc_attention.size(), \" G: \", G.size())\n",
    "        \n",
    "        ## Modeling Layer\n",
    "        M, _h = self.modeling_layer(G) # M: (bs, T, 2d)\n",
    "        \n",
    "        ## Output Layer\n",
    "        G_M = torch.cat((G, M), 2) # (bs, T, 10d)\n",
    "        G_M = self.output_layer_1(G_M).squeeze() # (bs, T)\n",
    "        G_M = F.pad(G_M, pad=(0,self.maxlen-G_M.size(1),0,0), mode='constant', value=0) # (bs, self.maxlen)\n",
    "\n",
    "        logits = F.relu(self.output_layer_2(G_M)) # (N, 1000)\n",
    "        logits = self.output_layer_3(logits) # (N, output_size)\n",
    "        #print(\"M: \", M.size(), \" G_M: \", G_M.size(), \" logits: \", logits.size())\n",
    "        \n",
    "        #return F.softmax(logits, dim=-1) \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        #loss = nn.MSELoss()(y,y_target)\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings\n",
    "Here you can choose your initial embeddings. Use either `pretrained embeddings` (glove/fasttext) or `uniform random embeddings`. **Uncomment the one you want to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 8, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 8, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 10\n",
    "drp = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Bidirectional_Attention_Model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict), maxlen=max_d, drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save results in a .txt file\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"bidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"Bidr_fasttext_gru\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Our plan was to incrementally improve the model. So we first started with a simple model. The model **does not consider attention**. This is our baseline model. The following figure demonstrates the model.\n",
    "\n",
    "<img src=\"images/baseline.jpg\"> \n",
    "Figure: Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, num_layers=1, drp_rate=0.25):\n",
    "        super(Basic_LSTM_model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):   \n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            \n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, (hidden, c) = self.context_lstm(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            #Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            c = c.index_select(1, idx_original)\n",
    "            \n",
    "            return output, hidden, c\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ##For Documents/questions\n",
    "        doc_output, doc_hidden, doc_c = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden, ques_c = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        # output shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        #Obtaining the final output of documents\n",
    "        docs_fwd_h = doc_hidden[0:doc_hidden.size(0):2]\n",
    "        docs_bwd_h = doc_hidden[1:doc_hidden.size(0):2]\n",
    "        docs_hidden = torch.cat([docs_fwd_h, docs_bwd_h], dim=2) \n",
    "        #docs_hidden shape:  torch.Size([1, bs, 256])\n",
    "        \n",
    "        #Obtaining the final output of questions\n",
    "        ques_fwd_h = ques_hidden[0:ques_hidden.size(0):2]\n",
    "        ques_bwd_h = ques_hidden[1:ques_hidden.size(0):2]\n",
    "        ques_hidden = torch.cat([ques_fwd_h, ques_bwd_h], dim=2) \n",
    "        #ques_hidden shape:  torch.Size([1, bs, 256])\n",
    "        \n",
    "        final_op = docs_hidden.squeeze() + ques_hidden.squeeze() \n",
    "        #final_op shape:  torch.Size([bs, 256])\n",
    "        \n",
    "        logits = self.linear(final_op) #logits shape:  torch.Size([bs, numClasses])\n",
    "        #print(\"logits shape: \", logits.size())\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings\n",
    "Here you can choose your initial embeddings. Use either `pretrained embeddings` (glove/fasttext) or `uniform random embeddings`. **Uncomment the one you want to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 8, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 8, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 10\n",
    "drp = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Basic_LSTM_model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict),drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save results in a .txt file\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"bidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"Basic_LSTM\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Attention Based Models\n",
    "\n",
    "\n",
    "### Numbers of parameters required for each of the models:\n",
    "\n",
    "\n",
    "| Model                           | #Parameters                    | \n",
    "| ------------------------------- |:------------------------------:| \n",
    "| Baseline                        |  $36106816$ ($36.10$ million)| \n",
    "| Unidirectional Attention        |  $36061928$ ($36.06$ million)| \n",
    "| Bidirectional Attention         |  $39574738$ ($39.57$ million)| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('There are {} ({:.2f} million) parameters in this neural network'.format(nb_param, nb_param/1e6))\n",
    "    print(nb_param)\n",
    "\n",
    "display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Required for a single epoch \n",
    "\n",
    "Time required for a single epoch of our attention based models in `Nvidia GTX 1080 Ti`:\n",
    "\n",
    "| Model                    | RNN Type | Single Epoch Time |\n",
    "|-------------------------:|----------|-------------------|\n",
    "| Baseline                 | GRU      | 0.3hr             |\n",
    "| Unidirectional Attention | GRU      | 0.5hr             |\n",
    "| Unidirectional Attention | LSTM     | 0.7hr             |\n",
    "| Bidirectional Attention  | GRU      | 5.2hrs            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the Attention Based Models\n",
    "\n",
    "#### Baseline Model:\n",
    "\n",
    "| RNN Type | Pretrained Embedding | Accuracy |\n",
    "|----------|----------------------|----------|\n",
    "| GRU      | Random               | $46.2$   |\n",
    "\n",
    "\n",
    "#### Unidirectional Attention Model\n",
    "\n",
    "| RNN Type | Pretrained Embedding | Accuracy |\n",
    "|----------|----------------------|----------|\n",
    "| GRU      | Random               | $66.87$  |\n",
    "| GRU      | Glove                | $71.21$  |\n",
    "| **GRU**  | **Fasttext**         | $72.29$  |\n",
    "| LSTM     | Random               | $67.03$  |\n",
    "| LSTM     | Glove                | $71.30$  |\n",
    "| LSTM     | Fasttext             | $71.58$  |\n",
    "\n",
    "\n",
    "#### Bidirectional Attention Model\n",
    "\n",
    "| RNN Type | Pretrained Embedding | Accuracy |\n",
    "|----------|----------------------|----------|\n",
    "| GRU      | Random               | $55.69$  |\n",
    "| GRU      | Glove                | $59.32$  |\n",
    "| GRU      | Fasttext             | $60.09$  |\n",
    "\n",
    "\n",
    "#### Best Result Variance\n",
    "\n",
    "Our best result was obtained from `Unidirectional Attention Model` using `GRU` with `Fasttext pretrained embeddings`. The following table shows the variance of the accuracies for 3 runs of the best model.\n",
    "\n",
    "|       | Accuracy |\n",
    "|-------|----------|\n",
    "| Run#1 | $71.57$    |\n",
    "| Run#2 | $72.29$    |\n",
    "| Run#3 | $71.69$    |\n",
    "| **Mean**| **71.85**  |\n",
    "| **Std**   | **0.39** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis of the Attention Based Models\n",
    "\n",
    "\n",
    "From the above tables, we can see that we get the best accuracy from `Unidirectional Attention Model` using `GRU` with `Fasttext pretrained embedding`. \n",
    "\n",
    "Our baseline model does not perform on a par with the attention models. The reason is obvious - the model is very simple. It is not possible to capture all the passage information in a single `Passage Vector`. So it performs badly compared to other models.\n",
    "\n",
    "Our `Unidirectional Attention Model` performs quite good. We can see the efficacy of using `pretrained embeddings` like glove or fasttext over `random embeddings` from the results. `Fasttext` is outperforming `glove`. The reason behind this is - alongside contexts fasttext also considers `character n-grams`. This enables fasttext to generate better word embeddings for rare words. \n",
    "We tried with both GRU and LSTM. From the result, we can see there performances are almost similar. But LSTM takes more time to train.\n",
    "\n",
    "Our `Bidirectional Attention Model` seemed to be promising. But it fails to produce a better result compared to `Unidirectional Attention Model`. We analyzed the outputs of the model and found that our `Bidirectional Attention Model` overfits the training data. That is why it fails to generalize and performs poorly on test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Over Attention Baseline Model\n",
    "\n",
    "<img src=\"images/attention_over_attention_baseline.jpeg.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this baseline model, we do as follows:\n",
    "1. **Contextual Embedding**:<br>\n",
    "h(D)=bi-GRU(D)<br>\n",
    "h(Q)=bi-GRU(Q)<br>\n",
    "2. **Parwise Matching Score**: <br>\n",
    "$M(i,j)=h_D(i)^Th_Q(j)$<br>\n",
    "3. **Compute Query-to-Document Attention and vice-versa**:<br>\n",
    "$\\alpha(t)=softmax(M(1,t),M(2,t),...,M(|D|,t)$ for t=1,2,...|Q|<br>\n",
    "$\\beta(t)=softmax(M(t,1),M(t,2),...,M(t,|Q|)$ for t=1,2,...|D|<br>\n",
    "for |D| is the number of tokens in document <br>\n",
    "and |Q| is the number of tokens in query <br>\n",
    "4. **Attention over Attention**:<br>\n",
    "$\\beta=\\frac{1}{|D|}\\sum_{t=1}^{|D|}\\beta(t)$<br>\n",
    "$s=\\alpha^T\\beta$<br>\n",
    "5. **Prediction**: <br>\n",
    "$P(w|D,Q)=\\sum_{i \\in I(w,D)}s_i$<br>\n",
    "where I(w,D) indicate the position that word w appears in D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of the Baseline Model:\n",
    "-  As the number of tokens in the Document usually is very big, in this data, the max length is 2000, the gru may probably not capture the that long dependency well\n",
    "- Not only that, gru may cause the running time very long, so we should find a way to make model run faster\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Model\n",
    "-  Transformer is the model ultilizing the attention mechanism that compute the representation of sequence parallel:\n",
    "<img src=\"images/Transformer_Model.jpg\"> \n",
    "\n",
    "- As here we only need to **replace gru** by the different mechanism to overcome it cons, so we only need to use the Transformer Encoder part\n",
    "<img src=\"images/Transformer_Encoder.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computatio of the Representation of Inputs in the Proposed Model:\n",
    "-  Attention mechanism: $Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ <br>\n",
    "This one generalize the normal attention mechanism we know with RNN: for example, in the encoder-decoder scheme, K and V can be the representation of data in the encoder side, and Q is the vector we want to find the attention on K and V.\n",
    "- Multi-head Attention: $Multihead(Q,K,V)=concat(head_0,head_1,head_h)W^O$ <br>\n",
    "where $head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$\n",
    "\n",
    "- Feed Forward: $FFN(x)=RELU(xW_1+b_1)W_2 +b_2$\n",
    "- Positional Encoding: We represent position of the token by <br>\n",
    "$PE(pos,2i)=sin(\\frac{pos}{1000^\\frac{2i}{d_{model}}})$<br>\n",
    "$PE(pos,2i+1)=cos(\\frac{pos}{1000^\\frac{2i}{d_{model}}})$<br>\n",
    "Please note that PE of pos here is a vector of size $d_{model}$ whose index 2i or 2i+1 is computed by those above formulas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only report the results because the proposed model has different data input pipeline and different program structure. Please refer to the separate notebook __[3_Transformer_Encoder_Attention_over_Attention_Notebook.ipynb]__ for more details.\n",
    "\n",
    "Please find the Github repository for the Transformer Network Based Model [here](https://github.com/tungngthanh/Transformer_Encoder_AoAreader/blob/master/Analyze%20Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the Proposed Model\n",
    "\n",
    "|Randomized Embedding                          | Accuracy | Number of parameters | Training speed |   \n",
    "|----------------------------------------------|----------|----------------------|----------------|\n",
    "| Attention over Attention                     | 69.89%   | 46,835,392           | 13,874 s/epoch |   \n",
    "| Transformer Encoder Attention over Attention | 70.86%   | 47,721,984           | 5,582 s/epoch  |   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis**\n",
    "- In here we can see that Transformer Encoder gives a boost of around 1 percentage which show that Transformer Encoder is a good alternative to the GRU. \n",
    "- More over, one of the best properties of Transformer is allowing parallel computing as all of its components can do that. Here you can see that the training speed is faster, only around 1 and half hour for 1 epoch while the original took nearly 4 hours for 1 epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer Network Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for the Proposed Model\n",
    "\n",
    "Existing state-of-the-art solutions to the Cloze-form reading comprehension on the CNN dataset typically calculate a probability distribution over the entities in the vocabulary. Without using explicit information about entities (which basically provides an answer candidate list - unintended consequences of anonymising the entities), one way to solve the reading comprehension question-answering problem in an ideal case is to try to model the position/index of the answer in the input passage itself. Pointer Networks (Vinyals, 2015) is a solution that models attention directly over the input.\n",
    "\n",
    "This is a pointer-networks inspired solution to RC, but there are various issues that come up with such an adaptation. One of the main problems is that the dataset provides the solution entity, but does not specify the exact context/sentence where this entity occurs - which means that the entity may appear any number of times in the passage, in different contexts, but we do not know which context is the most useful for predicting the answer to the query. This is problematic because it is necessary to know how many positions are being predicted during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "\n",
    "The original pointer network attention is $v(\\tanh (W_1 E + W_2 D))$ where E is the encoder output and D is the decoder output, and $W_1$, $W_2$ and V are outputs from linear layers. Here, the addition is changed to a dot product, since similarity between the passage (E) and query (D) representations (here obtained through a GRU layer) is required and SELU is used instead of tanh as the non-linear activation function. The final output is probabilities for each index in the passage obtained from a softmax operation.\n",
    "\n",
    "\n",
    "Number of parameters: 12,717,312 (with POS-tags) 12,660,480 (without POS-tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Strategies\n",
    "\n",
    "Four different strategies were tried to find a loss that works in this scenario (multiple, differing number of ones in answer one-hot matrix), for which accuracies on the development set (without POS) are reported. The accuracy is calculated by getting the index with the highest predicted probability, and if it matches _any_ index in the list of known answer indices, it is considered a match - since the referred entity would be the same no matter which index in the list is predicted. \n",
    "\n",
    "(1) Train directly with one-hot answer matrix, and do \n",
    "    (a) similarity between passage and query final representations: Unsuccessful (dev acc: __0.4%__)\n",
    "    (b) dot-product for each word in the passage with query representation. Unsuccessful, but works better with (4). (dev acc: __1.86%__)\n",
    "    (c) similarity between passage and query as encoder output, with answer-matrix as decoder output. Unsuccessful. (dev acc: __1.04%__)\n",
    "\n",
    "(2) Train with query expanded to passage length. Essentially trying to match query-length context in the passage; since the correct entity context is just one sentence in the passage. Unsuccessful, but works better when used with (4). (dev acc: __2.37%__)\n",
    "\n",
    "(3) Train with one-hot answer matrix for first 3 epochs or so; then reduce the number of ones predicted in the answer by picking the top K predicted already to train. Reduce K for each epoch until K=1. Also tried with initialising net weights to identity matrices before training. The idea is, if the passage words and query similarity provides a strong enough signal, the answer entity with the correct context is more likely to be preferred early, and with repeated filtering, the answer entity in the correct context might be in the top K and eventually top 1. Unsuccessful. (dev acc: __1.03%__)\n",
    "\n",
    "(4) Sum over the probabilities of all the known answer/gold indices. Target sum is one - basically predict zero probability for all words except answer entities, but individual probabilities are not important. Collectively try to maximise all index probabilities, under the assumption that the correct context will provide maximum contribution. Two parts to this: \n",
    "    (a) Train with decoder input=input_passage. Maximise sum(softmax(input_passage * attention weights)): Unsuccessful. (dev acc: __3.628%__) \n",
    "    (b) Remove decoder. Directly maximise sum(softmax(pointer_attention_weights)): somewhat (relatively) better. (dev acc: __6.67%__)\n",
    "\n",
    "(5) __Possible future work__: \n",
    "    (a) Represent each word in passage by similarity with bi-directional query-size-window context, not including the word itself. Essentially, because the entities/placeholders are randomly initialised and have no similarity with the query, the dependence is entirely on the surrounding context. \n",
    "    (b) Model by stride: pick a reasonable stride size in the passage within which answer entity may not repeat. Learn attention weights for similarity with query with each stride then combine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only report the proposed model description and the training strategies because the proposed model has different data input pipeline and different program structure. Please refer to the separate notebook __[4_Pointer_Network_Model_Notebook.ipynb]__ for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Result for Pointer Network Based Model\n",
    "\n",
    "__Final test data accuracies__: __6.44%__ (without POS) __5.62%__ (with POS) \n",
    "\n",
    "Possible reasons for poor performance:\n",
    "- Target is a unique, sparse matrix of document size, the relevant answer entities are randomly initialised, and to predict indices, the design does not necessarily provide signals from the most useful context.\n",
    "- Loss calculation is not entirely straightforward.\n",
    "- Using a better contextual embedding or more layers/parameters may be helpful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We tried different approaches to solve a Cloze form question answering problem, and both the best performing models perform at par with each other and use attention. Further improvement may be possible using better embedding techniques for words such as ELMo, Bert, etc. and by incorporating some linguistic information like part of speech tags or dependency information more effectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
