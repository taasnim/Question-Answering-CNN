{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention over attention baseline model\n",
    "\n",
    "<img src=\"attention_over_attention_baseline.jpeg.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this baseline model, we do as follows:\n",
    "1. **Contextual Embedding**:<br>\n",
    "h(D)=bi-GRU(D)<br>\n",
    "h(Q)=bi-GRU(Q)<br>\n",
    "2. **Parwise Matching Score**: <br>\n",
    "$M(i,j)=h_D(i)^Th_Q(j)$<br>\n",
    "3. **Compute Query-to-Document Attention and vice-versa**:<br>\n",
    "$\\alpha(t)=softmax(M(1,t),M(2,t),...,M(|D|,t)$ for t=1,2,...|Q|<br>\n",
    "$\\beta(t)=softmax(M(t,1),M(t,2),...,M(t,|Q|)$ for t=1,2,...|D|<br>\n",
    "for |D| is the number of tokens in document <br>\n",
    "and |Q| is the number of tokens in query <br>\n",
    "4. **Attention over Attention**:<br>\n",
    "$\\beta=\\frac{1}{|D|}\\sum_{t=1}^{|D|}\\beta(t)$<br>\n",
    "$s=\\alpha^T\\beta$<br>\n",
    "5. **Prediction**: <br>\n",
    "$P(w|D,Q)=\\sum_{i \\in I(w,D)}s_i$<br>\n",
    "where I(w,D) indicate the position that word w appears in D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawback of this model:\n",
    "-  As the number of tokens in the Document usually is very big, in this data, the max length is 2000, the gru may probably not capture the that long dependency well\n",
    "- Not only that, gru may cause the running time very long, so we should find a way to make model run faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed model\n",
    "-  Transformer is the model ultilizing the attention mechanism that compute the representation of sequence parallel:\n",
    "<img src=\"Transformer_Model.jpg\"> \n",
    "\n",
    "- As here we only need to **replace gru** by the different mechanism to overcome it cons, so we only need to use the Transformer Encoder part\n",
    "<img src=\"Transformer_Encoder.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this component, we compute the representation of input as follow:\n",
    "-  Attention mechanism: $Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ <br>\n",
    "This one generalize the normal attention mechanism we know with RNN: for example, in the encoder-decoder scheme, K and V can be the representation of data in the encoder side, and Q is the vector we want to find the attention on K and V.\n",
    "- Multi-head Attention: $Multihead(Q,K,V)=concat(head_0,head_1,head_h)W^O$ <br>\n",
    "where $head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$\n",
    "\n",
    "- Feed Forward: $FFN(x)=RELU(xW_1+b_1)W_2 +b_2$\n",
    "- Positional Encoding: We represent position of the token by <br>\n",
    "$PE(pos,2i)=sin(\\frac{pos}{1000^\\frac{2i}{d_{model}}})$<br>\n",
    "$PE(pos,2i+1)=cos(\\frac{pos}{1000^\\frac{2i}{d_{model}}})$<br>\n",
    "Please note that PE of pos here is a vector of size $d_{model}$ whose index 2i or 2i+1 is computed by those above formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the transformer encoder and then run the model with replacing bidirectional-gru by transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data process for this model is different from other model, Please refer the readme file for details of data process. Here we already save the data into pytorch support form and you can find it in the link https://drive.google.com/open?id=11ii6U7-Nz2bVm8zKBff0H1K9uuH7QC-i <br>\n",
    "Please copy data files in cnn_data(google drive folder) to directory ./data/cnn/\n",
    "And also copy data files in cnn_models( google drive folder) to directory ./cnn_models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if you have the notebook and the folder separately, to run this notebook sucessfully, please put this note book has the same parents as aoareader/data/model_cnn/models as follows <br>\n",
    ". <br>\n",
    "├──Analyze Model.ipynb <br>\n",
    "├── aoareader <br>\n",
    "├── data <br>\n",
    "├── model_cnn <br>\n",
    "├── models <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from aoareader import Constants\n",
    "import aoareader as reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we have the positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    position = np.arange(length)\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = (\n",
    "                    math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                    (float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * np.exp(\n",
    "                    np.arange(num_timescales).astype(np.float) * -log_timescale_increment)\n",
    "    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n",
    "\n",
    "\n",
    "    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n",
    "    signal = np.pad(signal, [[0, 0], [0, channels % 2]], \n",
    "                    'constant', constant_values=[0.0, 0.0])\n",
    "    signal =  signal.reshape([1, length, channels])\n",
    "\n",
    "    return torch.from_numpy(signal).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the multiple head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_depth, total_key_depth, total_value_depth, output_depth, \n",
    "                 num_heads, bias_mask=None, dropout=0.0):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        if total_key_depth % num_heads != 0:\n",
    "            raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n",
    "                             \"attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "        if total_value_depth % num_heads != 0:\n",
    "            raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n",
    "                             \"attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "            \n",
    "        self.num_heads = num_heads\n",
    "        self.query_scale = (total_key_depth//num_heads)**-0.5\n",
    "        self.bias_mask = bias_mask\n",
    "        \n",
    "        # Key and query depth will be same\n",
    "        self.query_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n",
    "        self.key_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n",
    "        self.value_linear = nn.Linear(input_depth, total_value_depth, bias=False)\n",
    "        self.output_linear = nn.Linear(total_value_depth, output_depth, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _split_heads(self, x):\n",
    "        \n",
    "        if len(x.shape) != 3:\n",
    "            raise ValueError(\"x must have rank 3\")\n",
    "        shape = x.shape\n",
    "        return x.view(shape[0], shape[1], self.num_heads, shape[2]//self.num_heads).permute(0, 2, 1, 3)\n",
    "    \n",
    "    def _merge_heads(self, x):\n",
    "        \n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(\"x must have rank 4\")\n",
    "        shape = x.shape\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(shape[0], shape[2], shape[3]*self.num_heads)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        # Do a linear for each component\n",
    "        queries = self.query_linear(queries)\n",
    "        keys = self.key_linear(keys)\n",
    "        values = self.value_linear(values)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        queries = self._split_heads(queries)\n",
    "        keys = self._split_heads(keys)\n",
    "        values = self._split_heads(values)\n",
    "        \n",
    "        # Scale queries\n",
    "        queries *= self.query_scale\n",
    "        \n",
    "        # Combine queries and keys\n",
    "        logits = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n",
    "        \n",
    "        # Add bias to mask future values\n",
    "        if self.bias_mask is not None:\n",
    "            logits += self.bias_mask[:, :, :logits.shape[-2], :logits.shape[-1]].type_as(logits.data)\n",
    "        \n",
    "        # Convert to probabilites\n",
    "        weights = nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Dropout\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # Combine with values to get context\n",
    "        contexts = torch.matmul(weights, values)\n",
    "        \n",
    "        # Merge heads\n",
    "        contexts = self._merge_heads(contexts)\n",
    "        #contexts = torch.tanh(contexts)\n",
    "        \n",
    "        # Linear to get output\n",
    "        outputs = self.output_linear(contexts)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Feed Forward, Conv network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Does a Linear + RELU + Linear on each of the timesteps\n",
    "    \"\"\"\n",
    "    def __init__(self, input_depth, filter_size, output_depth, layer_config='ll', padding='left', dropout=0.0):\n",
    "        \n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        sizes = ([(input_depth, filter_size)] + \n",
    "                 [(filter_size, filter_size)]*(len(layer_config)-2) + \n",
    "                 [(filter_size, output_depth)])\n",
    "\n",
    "        for lc, s in zip(list(layer_config), sizes):\n",
    "            if lc == 'l':\n",
    "                layers.append(nn.Linear(*s))\n",
    "            elif lc == 'c':\n",
    "                layers.append(Conv(*s, kernel_size=3, pad_type=padding))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown layer type {}\".format(lc))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers):\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "class Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, kernel_size, pad_type):\n",
    "        \n",
    "        super(Conv, self).__init__()\n",
    "        padding = (kernel_size - 1, 0) if pad_type == 'left' else (kernel_size//2, (kernel_size - 1)//2)\n",
    "        self.pad = nn.ConstantPad1d(padding, 0)\n",
    "        self.conv = nn.Conv1d(input_size, output_size, kernel_size=kernel_size, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.pad(inputs.permute(0, 2, 1))\n",
    "        outputs = self.conv(inputs).permute(0, 2, 1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then LayerNorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the MultiheadAttention, LayerNorm, FeedForward and then  LayerNorm (as the figure above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask=None, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "                \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n",
    "                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n",
    "        \n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding = 'both', \n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha = LayerNorm(hidden_size)\n",
    "        self.layer_norm_ffn = LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_mha(x)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        y = self.multi_head_attention(x_norm, x_norm, x_norm)\n",
    "        \n",
    "        # Dropout and residual\n",
    "        x = self.dropout(x + y)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        \n",
    "        # Positionwise Feedforward\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        \n",
    "        # Dropout and residual\n",
    "        y = self.dropout(x + y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Positional Encoding we can define the transformer encoder now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n",
    "                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n",
    "                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n",
    "        \n",
    "        params =(hidden_size, \n",
    "                 total_key_depth or hidden_size,\n",
    "                 total_value_depth or hidden_size,\n",
    "                 filter_size, \n",
    "                 num_heads, \n",
    "                 _gen_bias_mask(max_length) if use_mask else None,\n",
    "                 layer_dropout, \n",
    "                 attention_dropout, \n",
    "                 relu_dropout)\n",
    "\n",
    "        self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n",
    "        self.enc = nn.Sequential(*[EncoderLayer(*params) for l in range(num_layers)])\n",
    "        \n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #Add input dropout\n",
    "\n",
    "        x = self.input_dropout(inputs)\n",
    "        \n",
    "        # Project to hidden size\n",
    "        x = self.embedding_proj(x)\n",
    "        \n",
    "        # Add timing signal\n",
    "        x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n",
    "        \n",
    "        y = self.enc(x)\n",
    "        \n",
    "        y = self.layer_norm(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test our implementation of transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "  (enc): Sequential(\n",
       "    (0): EncoderLayer(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (key_linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (value_linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (output_linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "      (positionwise_feed_forward): PositionwiseFeedForward(\n",
       "        (layers): ModuleList(\n",
       "          (0): Conv(\n",
       "            (pad): ConstantPad1d(padding=(1, 1), value=0)\n",
       "            (conv): Conv1d(384, 64, kernel_size=(3,), stride=(1,))\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (pad): ConstantPad1d(padding=(1, 1), value=0)\n",
       "            (conv): Conv1d(64, 384, kernel_size=(3,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0)\n",
       "      (layer_norm_mha): LayerNorm()\n",
       "      (layer_norm_ffn): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm()\n",
       "  (input_dropout): Dropout(p=0.0)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_transformer = Encoder(384, 384, 1, 1, 384, 384,64, max_length=2000, input_dropout=0.0, layer_dropout=0.0,\n",
    "                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False)\n",
    "check_transformer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 564, 384])\n"
     ]
    }
   ],
   "source": [
    "def sort_batch(data, seq_len):\n",
    "    sorted_seq_len, sorted_idx = torch.sort(seq_len, dim=0, descending=True)\n",
    "    sorted_data = data[sorted_idx.data]\n",
    "    _, reverse_idx = torch.sort(sorted_idx, dim=0, descending=False)\n",
    "    return sorted_data, sorted_seq_len.cuda(), reverse_idx.cuda()\n",
    "vocab_dict = torch.load('data/dict.pt')\n",
    "train_data= torch.load('data/train.txt.pt')\n",
    "train_dataset = reader.Dataset(train_data, 64, True)\n",
    "(batch_docs, batch_docs_len, doc_mask), (batch_querys, batch_querys_len, query_mask), batch_answers, candidates = train_dataset[0]\n",
    "check_embedding=nn.Embedding(vocab_dict.size(), 384, padding_idx=Constants.PAD).cuda()\n",
    "s_docs, s_docs_len, reverse_docs_idx = sort_batch(batch_docs, batch_docs_len)\n",
    "docs_embedding=check_embedding(s_docs)\n",
    "docs_outputs = check_transformer(docs_embedding)\n",
    "print(docs_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you do not have enough gpu memory, you may consider to restart kernel and run from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aoareader as reader\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here create a class to have all paraeters.It is for the case that we want to train from scratch.\n",
    "For convinience, we set the valid data as the test data.\n",
    "To fully understand the parameters, please refer to file train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'traindata': 'data/cnn/train.txt.pt', 'validdata': 'data/cnn/test.txt.pt', 'dict': 'data/cnn/dict.pt', 'save_model': 'model_cnn', 'train_from': '', 'hidden_size': 384, 'embed_size': 384, 'batch_size': 64, 'dropout': 0.1, 'start_epoch': 1, 'epochs': 13, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'gpu': 0, 'log_interval': 100}\n"
     ]
    }
   ],
   "source": [
    "class opt(object):\n",
    "    def __init__(self):\n",
    "        self.traindata='data/cnn/train.txt.pt'\n",
    "        self.validdata='data/cnn/test.txt.pt'\n",
    "        self.dict='data/cnn/dict.pt'\n",
    "        self.save_model='model_cnn'\n",
    "        self.train_from=''\n",
    "        self.hidden_size=384\n",
    "        self.embed_size=384\n",
    "        self.batch_size=64\n",
    "        self.dropout=0.1\n",
    "        self.start_epoch=1\n",
    "        self.epochs=13\n",
    "        self.learning_rate=0.001\n",
    "        self.weight_decay=0.0001\n",
    "        self.gpu=0\n",
    "        self.log_interval=100\n",
    "opt=opt()\n",
    "print(vars(opt))\n",
    "if opt.gpu:\n",
    "    torch.cuda.set_device(opt.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and evalutation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_func(answers, pred_answers, answer_probs):\n",
    "    num_correct = (answers == pred_answers).sum().squeeze().data[0]\n",
    "    loss = - torch.mean(torch.log(answer_probs),0, keepdim=True)\n",
    "    return loss.cuda(), num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data):\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    total_correct = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(len(data)):\n",
    "        (batch_docs, batch_docs_len, doc_mask), (batch_querys, batch_querys_len, query_mask), batch_answers, candidates = data[i]\n",
    "\n",
    "        pred_answers, probs = model(batch_docs, batch_docs_len, doc_mask,\n",
    "                                    batch_querys, batch_querys_len, query_mask,\n",
    "                                    answers=batch_answers, candidates=candidates)\n",
    "\n",
    "        loss, num_correct = loss_func(batch_answers, pred_answers, probs)\n",
    "\n",
    "        total_in_minibatch = batch_answers.size(0)\n",
    "        total_loss += loss.data[0] * total_in_minibatch\n",
    "        total_correct += num_correct\n",
    "        total += total_in_minibatch\n",
    "        del loss, pred_answers, probs\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / total, total_correct.type(torch.float) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, trainData, validData, optimizer: torch.optim.Adam):\n",
    "    print(model)\n",
    "    start_time = time.time()\n",
    "\n",
    "    def trainEpoch(epoch):\n",
    "        trainData.shuffle()\n",
    "\n",
    "        total_loss, total, total_num_correct = 0, 0, 0\n",
    "        report_loss, report_total, report_num_correct = 0, 0, 0\n",
    "        for i in range(len(trainData)):\n",
    "            (batch_docs, batch_docs_len, doc_mask), (batch_querys, batch_querys_len, query_mask), batch_answers, candidates = trainData[i]\n",
    "\n",
    "            model.zero_grad()\n",
    "            pred_answers, answer_probs = model(batch_docs, batch_docs_len, doc_mask, batch_querys, batch_querys_len, query_mask,answers=batch_answers, candidates=candidates)\n",
    "\n",
    "            loss, num_correct = loss_func(batch_answers, pred_answers, answer_probs)\n",
    "\n",
    "            loss.backward()\n",
    "            for parameter in model.parameters():\n",
    "                parameter.grad.data.clamp_(-5.0, 5.0)\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            total_in_minibatch = batch_answers.size(0)\n",
    "\n",
    "            report_loss += loss.data[0] * total_in_minibatch\n",
    "            report_num_correct += num_correct\n",
    "            report_total += total_in_minibatch\n",
    "\n",
    "            total_loss += loss.data[0] * total_in_minibatch\n",
    "            total_num_correct += num_correct\n",
    "            total += total_in_minibatch\n",
    "            if i % opt.log_interval == 0:\n",
    "                print(\"Epoch %2d, %5d/%5d; avg loss: %.2f; acc: %2.2f;  %6.0f s elapsed\" %\n",
    "                      (epoch, i+1, len(trainData),\n",
    "                       report_loss / report_total,\n",
    "                       (100.0*report_num_correct / report_total),\n",
    "                       time.time()-start_time))\n",
    "                valid_loss, valid_acc = eval(model, validData)\n",
    "                print('=' * 20)\n",
    "                print('Evaluating on validation set:')\n",
    "                print('Validation loss: %.2f' % valid_loss)\n",
    "                print('Validation accuracy: %2.2f' % (valid_acc * 100.0))\n",
    "                print('=' * 20)\n",
    "\n",
    "                report_loss = 0.0\n",
    "                report_total = 0.0\n",
    "                report_num_correct = 0.0\n",
    "            del loss, pred_answers, answer_probs\n",
    "\n",
    "        return total_loss / total, total_num_correct / total\n",
    "\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        print('')\n",
    "\n",
    "        #  (1) train for one epoch on the training set\n",
    "        train_loss, train_acc = trainEpoch(epoch)\n",
    "        print('Epoch %d:\\t average loss: %.2f\\t train accuracy: %g' % (epoch, train_loss, train_acc*100))\n",
    "\n",
    "        #  (2) evaluate on the validation set\n",
    "        valid_loss, valid_acc = eval(model, validData)\n",
    "        print('=' * 20)\n",
    "        print('Evaluating on validation set:')\n",
    "        print('Validation loss: %.2f' % valid_loss)\n",
    "        print('Validation accuracy: %2.2f' % (valid_acc*100.0))\n",
    "        print('=' * 20)\n",
    "\n",
    "        model_state_dict = model.state_dict()\n",
    "        optimizer_state_dict = optimizer.state_dict()\n",
    "        #  (4) drop a checkpoint\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer_state_dict,\n",
    "            'opt': opt,\n",
    "        }\n",
    "        torch.save(checkpoint,\n",
    "                   'model_cnn/%s_epoch%d_acc_%.2f.pt' % (opt.save_model, epoch, 100*valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set our model train from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictrionary from  data/cnn/dict.pt\n",
      "Loading train data from  data/cnn/train.txt.pt\n",
      "Loading valid data from  data/cnn/test.txt.pt\n",
      " * vocabulary size = 119656\n",
      " * number of training samples. 380298\n",
      " * maximum batch size. 64\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttnguyen/Projects/Transformer_Encoder_AoAreader/aoareader/AoAReader.py:54: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  weigth_init.orthogonal(weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint at \n",
      "* number of parameters: 46835392\n",
      "AoAReader(\n",
      "  (embedding): Embedding(119656, 384, padding_idx=0)\n",
      "  (transformer): Encoder(\n",
      "    (embedding_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "    (enc): Sequential(\n",
      "      (0): EncoderLayer(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (query_linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (key_linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value_linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (output_linear): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (positionwise_feed_forward): PositionwiseFeedForward(\n",
      "          (layers): ModuleList(\n",
      "            (0): Conv(\n",
      "              (pad): ConstantPad1d(padding=(1, 1), value=0)\n",
      "              (conv): Conv1d(384, 64, kernel_size=(3,), stride=(1,))\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (pad): ConstantPad1d(padding=(1, 1), value=0)\n",
      "              (conv): Conv1d(64, 384, kernel_size=(3,), stride=(1,))\n",
      "            )\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0)\n",
      "        (layer_norm_mha): LayerNorm()\n",
      "        (layer_norm_ffn): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (input_dropout): Dropout(p=0.0)\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttnguyen/anaconda3/envs/py36torch/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,     1/ 5943; avg loss: 1.05; acc: 65.00;       2 s elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttnguyen/Projects/Transformer_Encoder_AoAreader/aoareader/Dataset.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  b = Variable(b, volatile=self.volatile, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  1,   101/ 5943; avg loss: 0.96; acc: 71.00;      96 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  1,   201/ 5943; avg loss: 0.96; acc: 71.00;     194 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  1,   301/ 5943; avg loss: 0.98; acc: 70.00;     293 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.07\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  1,   401/ 5943; avg loss: 0.98; acc: 70.00;     387 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.05\n",
      "Validation accuracy: 68.61\n",
      "====================\n",
      "Epoch  1,   501/ 5943; avg loss: 0.98; acc: 71.00;     479 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.26\n",
      "====================\n",
      "Epoch  1,   601/ 5943; avg loss: 0.97; acc: 70.00;     575 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.05\n",
      "Validation accuracy: 68.61\n",
      "====================\n",
      "Epoch  1,   701/ 5943; avg loss: 0.99; acc: 70.00;     674 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  1,   801/ 5943; avg loss: 1.04; acc: 70.00;     768 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.36\n",
      "====================\n",
      "Epoch  1,   901/ 5943; avg loss: 1.01; acc: 69.00;     869 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  1,  1001/ 5943; avg loss: 1.00; acc: 70.00;     964 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  1,  1101/ 5943; avg loss: 1.03; acc: 69.00;    1058 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.86\n",
      "====================\n",
      "Epoch  1,  1201/ 5943; avg loss: 1.03; acc: 69.00;    1152 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  1,  1301/ 5943; avg loss: 1.03; acc: 69.00;    1245 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 68.89\n",
      "====================\n",
      "Epoch  1,  1401/ 5943; avg loss: 1.05; acc: 69.00;    1341 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  1,  1501/ 5943; avg loss: 1.01; acc: 70.00;    1434 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  1,  1601/ 5943; avg loss: 1.00; acc: 70.00;    1526 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.17\n",
      "====================\n",
      "Epoch  1,  1701/ 5943; avg loss: 1.04; acc: 69.00;    1618 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  1,  1801/ 5943; avg loss: 1.02; acc: 70.00;    1712 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  1,  1901/ 5943; avg loss: 1.04; acc: 68.00;    1805 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.92\n",
      "====================\n",
      "Epoch  1,  2001/ 5943; avg loss: 1.02; acc: 69.00;    1898 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  1,  2101/ 5943; avg loss: 1.04; acc: 69.00;    1991 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  1,  2201/ 5943; avg loss: 1.04; acc: 69.00;    2083 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.95\n",
      "====================\n",
      "Epoch  1,  2301/ 5943; avg loss: 1.03; acc: 70.00;    2178 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  1,  2401/ 5943; avg loss: 1.03; acc: 69.00;    2271 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.48\n",
      "====================\n",
      "Epoch  1,  2501/ 5943; avg loss: 1.01; acc: 70.00;    2363 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  1,  2601/ 5943; avg loss: 1.02; acc: 69.00;    2455 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.64\n",
      "====================\n",
      "Epoch  1,  2701/ 5943; avg loss: 1.01; acc: 70.00;    2547 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.42\n",
      "====================\n",
      "Epoch  1,  2801/ 5943; avg loss: 1.07; acc: 68.00;    2639 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.20\n",
      "====================\n",
      "Epoch  1,  2901/ 5943; avg loss: 1.03; acc: 69.00;    2731 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  1,  3001/ 5943; avg loss: 1.04; acc: 69.00;    2822 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  1,  3101/ 5943; avg loss: 1.05; acc: 68.00;    2915 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.42\n",
      "====================\n",
      "Epoch  1,  3201/ 5943; avg loss: 1.03; acc: 69.00;    3007 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  1,  3301/ 5943; avg loss: 1.05; acc: 68.00;    3102 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  1,  3401/ 5943; avg loss: 1.04; acc: 69.00;    3195 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.67\n",
      "====================\n",
      "Epoch  1,  3501/ 5943; avg loss: 1.04; acc: 69.00;    3289 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.82\n",
      "====================\n",
      "Epoch  1,  3601/ 5943; avg loss: 1.04; acc: 70.00;    3384 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.01\n",
      "====================\n",
      "Epoch  1,  3701/ 5943; avg loss: 1.06; acc: 68.00;    3475 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 68.54\n",
      "====================\n",
      "Epoch  1,  3801/ 5943; avg loss: 1.03; acc: 70.00;    3567 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.36\n",
      "====================\n",
      "Epoch  1,  3901/ 5943; avg loss: 1.06; acc: 68.00;    3658 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.79\n",
      "====================\n",
      "Epoch  1,  4001/ 5943; avg loss: 1.02; acc: 69.00;    3749 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.20\n",
      "====================\n",
      "Epoch  1,  4101/ 5943; avg loss: 1.01; acc: 70.00;    3841 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.82\n",
      "====================\n",
      "Epoch  1,  4201/ 5943; avg loss: 1.07; acc: 68.00;    3932 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  1,  4301/ 5943; avg loss: 1.06; acc: 68.00;    4021 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.32\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,  4401/ 5943; avg loss: 1.04; acc: 68.00;    4111 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 68.95\n",
      "====================\n",
      "Epoch  1,  4501/ 5943; avg loss: 1.02; acc: 69.00;    4204 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  1,  4601/ 5943; avg loss: 1.06; acc: 69.00;    4295 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.67\n",
      "====================\n",
      "Epoch  1,  4701/ 5943; avg loss: 1.06; acc: 68.00;    4383 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.64\n",
      "====================\n",
      "Epoch  1,  4801/ 5943; avg loss: 1.02; acc: 69.00;    4475 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.01\n",
      "====================\n",
      "Epoch  1,  4901/ 5943; avg loss: 1.06; acc: 68.00;    4566 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  1,  5001/ 5943; avg loss: 1.07; acc: 68.00;    4658 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.89\n",
      "====================\n",
      "Epoch  1,  5101/ 5943; avg loss: 1.06; acc: 68.00;    4748 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  1,  5201/ 5943; avg loss: 1.07; acc: 68.00;    4843 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.17\n",
      "====================\n",
      "Epoch  1,  5301/ 5943; avg loss: 1.07; acc: 69.00;    4936 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  1,  5401/ 5943; avg loss: 1.04; acc: 69.00;    5029 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 68.89\n",
      "====================\n",
      "Epoch  1,  5501/ 5943; avg loss: 1.06; acc: 69.00;    5122 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  1,  5601/ 5943; avg loss: 1.07; acc: 68.00;    5217 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.36\n",
      "====================\n",
      "Epoch  1,  5701/ 5943; avg loss: 1.07; acc: 68.00;    5309 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  1,  5801/ 5943; avg loss: 1.05; acc: 69.00;    5405 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.89\n",
      "====================\n",
      "Epoch  1,  5901/ 5943; avg loss: 1.07; acc: 68.00;    5497 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.89\n",
      "====================\n",
      "Epoch 1:\t average loss: 1.03\t train accuracy: 0\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.08\n",
      "====================\n",
      "\n",
      "Epoch  2,     1/ 5943; avg loss: 0.97; acc: 70.00;    5582 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  2,   101/ 5943; avg loss: 0.96; acc: 71.00;    5673 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.73\n",
      "====================\n",
      "Epoch  2,   201/ 5943; avg loss: 0.92; acc: 72.00;    5764 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.26\n",
      "====================\n",
      "Epoch  2,   301/ 5943; avg loss: 0.93; acc: 72.00;    5854 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.79\n",
      "====================\n",
      "Epoch  2,   401/ 5943; avg loss: 0.95; acc: 72.00;    5947 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  2,   501/ 5943; avg loss: 0.97; acc: 71.00;    6036 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.73\n",
      "====================\n",
      "Epoch  2,   601/ 5943; avg loss: 0.98; acc: 71.00;    6130 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 68.70\n",
      "====================\n",
      "Epoch  2,   701/ 5943; avg loss: 1.00; acc: 70.00;    6224 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  2,   801/ 5943; avg loss: 0.99; acc: 71.00;    6318 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  2,   901/ 5943; avg loss: 0.97; acc: 71.00;    6409 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  2,  1001/ 5943; avg loss: 0.96; acc: 72.00;    6501 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.82\n",
      "====================\n",
      "Epoch  2,  1101/ 5943; avg loss: 0.97; acc: 70.00;    6591 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.42\n",
      "====================\n",
      "Epoch  2,  1201/ 5943; avg loss: 1.00; acc: 70.00;    6684 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  2,  1301/ 5943; avg loss: 0.98; acc: 70.00;    6777 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  2,  1401/ 5943; avg loss: 1.00; acc: 70.00;    6870 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  2,  1501/ 5943; avg loss: 0.99; acc: 70.00;    6963 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 68.82\n",
      "====================\n",
      "Epoch  2,  1601/ 5943; avg loss: 1.00; acc: 70.00;    7056 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  2,  1701/ 5943; avg loss: 1.01; acc: 70.00;    7149 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 68.64\n",
      "====================\n",
      "Epoch  2,  1801/ 5943; avg loss: 1.02; acc: 69.00;    7240 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.14\n",
      "====================\n",
      "Epoch  2,  1901/ 5943; avg loss: 1.03; acc: 69.00;    7331 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.26\n",
      "====================\n",
      "Epoch  2,  2001/ 5943; avg loss: 1.03; acc: 70.00;    7422 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.79\n",
      "====================\n",
      "Epoch  2,  2101/ 5943; avg loss: 1.00; acc: 70.00;    7513 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.11\n",
      "====================\n",
      "Epoch  2,  2201/ 5943; avg loss: 0.99; acc: 70.00;    7604 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  2,  2301/ 5943; avg loss: 1.00; acc: 70.00;    7693 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.82\n",
      "====================\n",
      "Epoch  2,  2401/ 5943; avg loss: 1.02; acc: 70.00;    7785 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  2,  2501/ 5943; avg loss: 1.03; acc: 69.00;    7878 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  2,  2601/ 5943; avg loss: 1.00; acc: 70.00;    7966 s elapsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  2,  2701/ 5943; avg loss: 1.04; acc: 70.00;    8058 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.01\n",
      "====================\n",
      "Epoch  2,  2801/ 5943; avg loss: 1.03; acc: 70.00;    8153 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.67\n",
      "====================\n",
      "Epoch  2,  2901/ 5943; avg loss: 1.06; acc: 69.00;    8248 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.73\n",
      "====================\n",
      "Epoch  2,  3001/ 5943; avg loss: 1.04; acc: 68.00;    8341 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  2,  3101/ 5943; avg loss: 1.04; acc: 69.00;    8432 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.08\n",
      "====================\n",
      "Epoch  2,  3201/ 5943; avg loss: 1.04; acc: 69.00;    8525 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  2,  3301/ 5943; avg loss: 1.04; acc: 69.00;    8616 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.36\n",
      "====================\n",
      "Epoch  2,  3401/ 5943; avg loss: 1.05; acc: 69.00;    8707 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.26\n",
      "====================\n",
      "Epoch  2,  3501/ 5943; avg loss: 1.05; acc: 68.00;    8803 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  2,  3601/ 5943; avg loss: 1.02; acc: 69.00;    8894 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.08\n",
      "====================\n",
      "Epoch  2,  3701/ 5943; avg loss: 1.02; acc: 70.00;    8985 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.95\n",
      "====================\n",
      "Epoch  2,  3801/ 5943; avg loss: 1.02; acc: 70.00;    9077 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.67\n",
      "====================\n",
      "Epoch  2,  3901/ 5943; avg loss: 1.03; acc: 69.00;    9172 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  2,  4001/ 5943; avg loss: 1.04; acc: 69.00;    9265 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  2,  4101/ 5943; avg loss: 1.03; acc: 69.00;    9360 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  2,  4201/ 5943; avg loss: 1.05; acc: 69.00;    9450 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  2,  4301/ 5943; avg loss: 1.07; acc: 67.00;    9544 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  2,  4401/ 5943; avg loss: 1.06; acc: 68.00;    9635 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.20\n",
      "====================\n",
      "Epoch  2,  4501/ 5943; avg loss: 1.06; acc: 68.00;    9727 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.64\n",
      "====================\n",
      "Epoch  2,  4601/ 5943; avg loss: 1.02; acc: 69.00;    9819 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.42\n",
      "====================\n",
      "Epoch  2,  4701/ 5943; avg loss: 1.02; acc: 69.00;    9915 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  2,  4801/ 5943; avg loss: 1.04; acc: 69.00;   10007 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  2,  4901/ 5943; avg loss: 1.06; acc: 68.00;   10100 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  2,  5001/ 5943; avg loss: 1.03; acc: 69.00;   10192 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  2,  5101/ 5943; avg loss: 1.06; acc: 69.00;   10286 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  2,  5201/ 5943; avg loss: 1.06; acc: 69.00;   10379 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  2,  5301/ 5943; avg loss: 1.04; acc: 69.00;   10473 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 0.99\n",
      "Validation accuracy: 70.86\n",
      "====================\n",
      "Epoch  2,  5401/ 5943; avg loss: 1.04; acc: 69.00;   10568 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  2,  5501/ 5943; avg loss: 1.06; acc: 69.00;   10659 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 70.42\n",
      "====================\n",
      "Epoch  2,  5601/ 5943; avg loss: 1.05; acc: 69.00;   10751 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  2,  5701/ 5943; avg loss: 1.05; acc: 69.00;   10841 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 70.29\n",
      "====================\n",
      "Epoch  2,  5801/ 5943; avg loss: 1.06; acc: 68.00;   10935 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.82\n",
      "====================\n",
      "Epoch  2,  5901/ 5943; avg loss: 1.07; acc: 67.00;   11027 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.57\n",
      "====================\n",
      "Epoch 2:\t average loss: 1.02\t train accuracy: 0\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.51\n",
      "====================\n",
      "\n",
      "Epoch  3,     1/ 5943; avg loss: 0.71; acc: 82.00;   11112 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  3,   101/ 5943; avg loss: 0.92; acc: 72.00;   11205 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  3,   201/ 5943; avg loss: 0.95; acc: 72.00;   11297 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  3,   301/ 5943; avg loss: 0.93; acc: 72.00;   11393 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 70.20\n",
      "====================\n",
      "Epoch  3,   401/ 5943; avg loss: 0.92; acc: 72.00;   11486 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.14\n",
      "====================\n",
      "Epoch  3,   501/ 5943; avg loss: 0.97; acc: 71.00;   11579 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.14\n",
      "====================\n",
      "Epoch  3,   601/ 5943; avg loss: 0.99; acc: 70.00;   11670 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.17\n",
      "====================\n",
      "Epoch  3,   701/ 5943; avg loss: 1.00; acc: 70.00;   11763 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  3,   801/ 5943; avg loss: 0.95; acc: 71.00;   11853 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.98\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3,   901/ 5943; avg loss: 0.96; acc: 72.00;   11946 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  3,  1001/ 5943; avg loss: 1.01; acc: 70.00;   12044 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  3,  1101/ 5943; avg loss: 0.96; acc: 71.00;   12135 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  3,  1201/ 5943; avg loss: 0.95; acc: 71.00;   12227 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.64\n",
      "====================\n",
      "Epoch  3,  1301/ 5943; avg loss: 0.99; acc: 70.00;   12319 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.73\n",
      "====================\n",
      "Epoch  3,  1401/ 5943; avg loss: 0.97; acc: 70.00;   12412 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.26\n",
      "====================\n",
      "Epoch  3,  1501/ 5943; avg loss: 1.01; acc: 70.00;   12503 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 70.08\n",
      "====================\n",
      "Epoch  3,  1601/ 5943; avg loss: 1.02; acc: 69.00;   12595 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.73\n",
      "====================\n",
      "Epoch  3,  1701/ 5943; avg loss: 0.99; acc: 70.00;   12686 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  3,  1801/ 5943; avg loss: 1.01; acc: 70.00;   12777 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.29\n",
      "====================\n",
      "Epoch  3,  1901/ 5943; avg loss: 0.97; acc: 71.00;   12869 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  3,  2001/ 5943; avg loss: 1.02; acc: 70.00;   12961 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  3,  2101/ 5943; avg loss: 0.98; acc: 71.00;   13052 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  3,  2201/ 5943; avg loss: 1.00; acc: 70.00;   13143 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.86\n",
      "====================\n",
      "Epoch  3,  2301/ 5943; avg loss: 1.00; acc: 69.00;   13235 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  3,  2401/ 5943; avg loss: 0.98; acc: 69.00;   13326 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  3,  2501/ 5943; avg loss: 1.00; acc: 70.00;   13419 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.51\n",
      "====================\n",
      "Epoch  3,  2601/ 5943; avg loss: 1.06; acc: 68.00;   13515 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  3,  2701/ 5943; avg loss: 1.04; acc: 69.00;   13609 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 70.08\n",
      "====================\n",
      "Epoch  3,  2801/ 5943; avg loss: 0.99; acc: 70.00;   13701 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  3,  2901/ 5943; avg loss: 1.02; acc: 69.00;   13795 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 68.92\n",
      "====================\n",
      "Epoch  3,  3001/ 5943; avg loss: 1.00; acc: 69.00;   13890 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.92\n",
      "====================\n",
      "Epoch  3,  3101/ 5943; avg loss: 1.03; acc: 69.00;   13985 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  3,  3201/ 5943; avg loss: 1.03; acc: 68.00;   14080 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.01\n",
      "====================\n",
      "Epoch  3,  3301/ 5943; avg loss: 1.04; acc: 69.00;   14171 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.42\n",
      "====================\n",
      "Epoch  3,  3401/ 5943; avg loss: 0.99; acc: 70.00;   14264 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 70.61\n",
      "====================\n",
      "Epoch  3,  3501/ 5943; avg loss: 1.01; acc: 69.00;   14355 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.23\n",
      "====================\n",
      "Epoch  3,  3601/ 5943; avg loss: 1.05; acc: 69.00;   14447 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.26\n",
      "====================\n",
      "Epoch  3,  3701/ 5943; avg loss: 1.02; acc: 69.00;   14539 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.86\n",
      "====================\n",
      "Epoch  3,  3801/ 5943; avg loss: 1.04; acc: 69.00;   14633 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.51\n",
      "====================\n",
      "Epoch  3,  3901/ 5943; avg loss: 1.03; acc: 70.00;   14728 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.95\n",
      "====================\n",
      "Epoch  3,  4001/ 5943; avg loss: 1.02; acc: 69.00;   14820 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.26\n",
      "====================\n",
      "Epoch  3,  4101/ 5943; avg loss: 1.03; acc: 69.00;   14913 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.14\n",
      "====================\n",
      "Epoch  3,  4201/ 5943; avg loss: 1.04; acc: 69.00;   15005 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.17\n",
      "====================\n",
      "Epoch  3,  4301/ 5943; avg loss: 1.02; acc: 69.00;   15097 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  3,  4401/ 5943; avg loss: 1.05; acc: 69.00;   15187 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.33\n",
      "====================\n",
      "Epoch  3,  4501/ 5943; avg loss: 1.05; acc: 68.00;   15281 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.73\n",
      "====================\n",
      "Epoch  3,  4601/ 5943; avg loss: 1.05; acc: 69.00;   15373 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.39\n",
      "====================\n",
      "Epoch  3,  4701/ 5943; avg loss: 1.01; acc: 70.00;   15467 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  3,  4801/ 5943; avg loss: 1.03; acc: 69.00;   15562 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.14\n",
      "====================\n",
      "Epoch  3,  4901/ 5943; avg loss: 1.02; acc: 69.00;   15655 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.51\n",
      "====================\n",
      "Epoch  3,  5001/ 5943; avg loss: 1.03; acc: 70.00;   15745 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  3,  5101/ 5943; avg loss: 1.03; acc: 69.00;   15835 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  3,  5201/ 5943; avg loss: 1.03; acc: 69.00;   15927 s elapsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.86\n",
      "====================\n",
      "Epoch  3,  5301/ 5943; avg loss: 1.02; acc: 70.00;   16021 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  3,  5401/ 5943; avg loss: 1.02; acc: 69.00;   16112 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.17\n",
      "====================\n",
      "Epoch  3,  5501/ 5943; avg loss: 1.02; acc: 69.00;   16206 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.89\n",
      "====================\n",
      "Epoch  3,  5601/ 5943; avg loss: 1.03; acc: 70.00;   16300 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.48\n",
      "====================\n",
      "Epoch  3,  5701/ 5943; avg loss: 1.04; acc: 69.00;   16392 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  3,  5801/ 5943; avg loss: 1.04; acc: 69.00;   16485 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.14\n",
      "====================\n",
      "Epoch  3,  5901/ 5943; avg loss: 1.02; acc: 69.00;   16576 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.79\n",
      "====================\n",
      "Epoch 3:\t average loss: 1.01\t train accuracy: 0\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.64\n",
      "====================\n",
      "\n",
      "Epoch  4,     1/ 5943; avg loss: 0.65; acc: 76.00;   16660 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.67\n",
      "====================\n",
      "Epoch  4,   101/ 5943; avg loss: 0.91; acc: 72.00;   16752 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.23\n",
      "====================\n",
      "Epoch  4,   201/ 5943; avg loss: 0.94; acc: 72.00;   16845 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.67\n",
      "====================\n",
      "Epoch  4,   301/ 5943; avg loss: 0.91; acc: 72.00;   16938 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 70.23\n",
      "====================\n",
      "Epoch  4,   401/ 5943; avg loss: 0.95; acc: 71.00;   17030 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  4,   501/ 5943; avg loss: 0.97; acc: 71.00;   17124 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  4,   601/ 5943; avg loss: 0.94; acc: 71.00;   17215 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  4,   701/ 5943; avg loss: 0.97; acc: 71.00;   17309 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  4,   801/ 5943; avg loss: 0.92; acc: 72.00;   17401 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.20\n",
      "====================\n",
      "Epoch  4,   901/ 5943; avg loss: 0.96; acc: 71.00;   17493 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 69.92\n",
      "====================\n",
      "Epoch  4,  1001/ 5943; avg loss: 0.94; acc: 72.00;   17584 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  4,  1101/ 5943; avg loss: 0.98; acc: 71.00;   17675 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.07\n",
      "====================\n",
      "Epoch  4,  1201/ 5943; avg loss: 0.97; acc: 71.00;   17768 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.79\n",
      "====================\n",
      "Epoch  4,  1301/ 5943; avg loss: 0.98; acc: 71.00;   17863 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.95\n",
      "====================\n",
      "Epoch  4,  1401/ 5943; avg loss: 0.97; acc: 71.00;   17955 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.39\n",
      "====================\n",
      "Epoch  4,  1501/ 5943; avg loss: 0.99; acc: 69.00;   18047 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.36\n",
      "====================\n",
      "Epoch  4,  1601/ 5943; avg loss: 1.01; acc: 70.00;   18139 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  4,  1701/ 5943; avg loss: 0.99; acc: 71.00;   18233 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.51\n",
      "====================\n",
      "Epoch  4,  1801/ 5943; avg loss: 0.97; acc: 71.00;   18323 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  4,  1901/ 5943; avg loss: 0.97; acc: 70.00;   18416 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.64\n",
      "====================\n",
      "Epoch  4,  2001/ 5943; avg loss: 0.95; acc: 71.00;   18508 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.76\n",
      "====================\n",
      "Epoch  4,  2101/ 5943; avg loss: 0.98; acc: 71.00;   18602 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.32\n",
      "====================\n",
      "Epoch  4,  2201/ 5943; avg loss: 1.00; acc: 71.00;   18694 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.64\n",
      "====================\n",
      "Epoch  4,  2301/ 5943; avg loss: 1.01; acc: 69.00;   18787 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  4,  2401/ 5943; avg loss: 0.99; acc: 70.00;   18880 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.20\n",
      "====================\n",
      "Epoch  4,  2501/ 5943; avg loss: 0.98; acc: 71.00;   18975 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  4,  2601/ 5943; avg loss: 0.98; acc: 71.00;   19070 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.86\n",
      "====================\n",
      "Epoch  4,  2701/ 5943; avg loss: 1.01; acc: 70.00;   19162 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.95\n",
      "====================\n",
      "Epoch  4,  2801/ 5943; avg loss: 0.99; acc: 71.00;   19259 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  4,  2901/ 5943; avg loss: 0.97; acc: 70.00;   19350 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  4,  3001/ 5943; avg loss: 1.05; acc: 68.00;   19443 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  4,  3101/ 5943; avg loss: 1.01; acc: 70.00;   19534 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  4,  3201/ 5943; avg loss: 1.02; acc: 69.00;   19625 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.61\n",
      "====================\n",
      "Epoch  4,  3301/ 5943; avg loss: 1.03; acc: 69.00;   19718 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  4,  3401/ 5943; avg loss: 1.01; acc: 70.00;   19812 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.03\n",
      "Validation accuracy: 69.48\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4,  3501/ 5943; avg loss: 1.01; acc: 70.00;   19905 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  4,  3601/ 5943; avg loss: 1.00; acc: 70.00;   20001 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.98\n",
      "====================\n",
      "Epoch  4,  3701/ 5943; avg loss: 0.99; acc: 70.00;   20092 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.92\n",
      "====================\n",
      "Epoch  4,  3801/ 5943; avg loss: 0.99; acc: 70.00;   20182 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.26\n",
      "====================\n",
      "Epoch  4,  3901/ 5943; avg loss: 1.02; acc: 70.00;   20276 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.57\n",
      "====================\n",
      "Epoch  4,  4001/ 5943; avg loss: 1.05; acc: 68.00;   20370 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.54\n",
      "====================\n",
      "Epoch  4,  4101/ 5943; avg loss: 1.00; acc: 70.00;   20461 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  4,  4201/ 5943; avg loss: 1.03; acc: 69.00;   20552 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.82\n",
      "====================\n",
      "Epoch  4,  4301/ 5943; avg loss: 1.01; acc: 69.00;   20642 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  4,  4401/ 5943; avg loss: 1.04; acc: 69.00;   20734 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.08\n",
      "====================\n",
      "Epoch  4,  4501/ 5943; avg loss: 1.01; acc: 70.00;   20826 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  4,  4601/ 5943; avg loss: 1.01; acc: 70.00;   20919 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  4,  4701/ 5943; avg loss: 1.04; acc: 69.00;   21013 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  4,  4801/ 5943; avg loss: 1.00; acc: 70.00;   21104 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.70\n",
      "====================\n",
      "Epoch  4,  4901/ 5943; avg loss: 1.01; acc: 70.00;   21195 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.57\n",
      "====================\n",
      "Epoch  4,  5001/ 5943; avg loss: 1.01; acc: 70.00;   21287 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.04\n",
      "====================\n",
      "Epoch  4,  5101/ 5943; avg loss: 1.06; acc: 68.00;   21380 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.20\n",
      "====================\n",
      "Epoch  4,  5201/ 5943; avg loss: 1.04; acc: 70.00;   21472 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.92\n",
      "====================\n",
      "Epoch  4,  5301/ 5943; avg loss: 0.99; acc: 70.00;   21564 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.29\n",
      "====================\n",
      "Epoch  4,  5401/ 5943; avg loss: 1.03; acc: 69.00;   21655 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.29\n",
      "====================\n",
      "Epoch  4,  5501/ 5943; avg loss: 1.00; acc: 70.00;   21745 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.00\n",
      "Validation accuracy: 69.95\n",
      "====================\n",
      "Epoch  4,  5601/ 5943; avg loss: 1.05; acc: 68.00;   21836 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.01\n",
      "====================\n",
      "Epoch  4,  5701/ 5943; avg loss: 1.06; acc: 68.00;   21927 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 70.11\n",
      "====================\n",
      "Epoch  4,  5801/ 5943; avg loss: 1.01; acc: 70.00;   22021 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.45\n",
      "====================\n",
      "Epoch  4,  5901/ 5943; avg loss: 1.04; acc: 69.00;   22110 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.01\n",
      "Validation accuracy: 69.42\n",
      "====================\n",
      "Epoch 4:\t average loss: 1.00\t train accuracy: 0\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.26\n",
      "====================\n",
      "\n",
      "Epoch  5,     1/ 5943; avg loss: 0.88; acc: 76.00;   22195 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.02\n",
      "Validation accuracy: 69.20\n",
      "====================\n",
      "Epoch  5,   101/ 5943; avg loss: 0.91; acc: 73.00;   22288 s elapsed\n",
      "====================\n",
      "Evaluating on validation set:\n",
      "Validation loss: 1.04\n",
      "Validation accuracy: 68.82\n",
      "====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-18ca757e1953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'* number of parameters: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d457ebb4c77b>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, trainData, validData, optimizer)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#  (1) train for one epoch on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %d:\\t average loss: %.2f\\t train accuracy: %g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d457ebb4c77b>\u001b[0m in \u001b[0;36mtrainEpoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpred_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_docs_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_querys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_querys_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Transformer_Encoder_AoAreader/aoareader/AoAReader.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, docs_input, docs_len, doc_mask, querys_input, querys_len, query_mask, candidates, answers)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                     \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt.train_from='model_cnn/model_cnn_epoch17_acc_69.48.pt'\n",
    "train_from = opt.train_from\n",
    "if opt.train_from:\n",
    "    train_from = True\n",
    "    checkpoint = torch.load(opt.train_from)\n",
    "    opt = checkpoint['opt']\n",
    "    opt.hidden_size=opt.gru_size\n",
    "\n",
    "print(\"Loading dictrionary from \", opt.dict)\n",
    "vocab_dict = torch.load(opt.dict)\n",
    "print(\"Loading train data from \", opt.traindata)\n",
    "train_data = torch.load(opt.traindata)\n",
    "print(\"Loading valid data from \", opt.validdata)\n",
    "valid_data = torch.load(opt.validdata)\n",
    "\n",
    "train_dataset = reader.Dataset(train_data, opt.batch_size, True)\n",
    "valid_dataset = reader.Dataset(valid_data, opt.batch_size, True, volatile=True)\n",
    "\n",
    "print(' * vocabulary size = %d' %\n",
    "      (vocab_dict.size()))\n",
    "print(' * number of training samples. %d' %\n",
    "      len(train_data['answers']))\n",
    "print(' * maximum batch size. %d' % opt.batch_size)\n",
    "\n",
    "print('Building model...')\n",
    "\n",
    "model = reader.AoAReader(vocab_dict, dropout_rate=opt.dropout, embed_dim=opt.embed_size, hidden_dim=opt.hidden_size)\n",
    "# no way on CPU\n",
    "model.cuda()\n",
    "# for parameter in model.parameters():\n",
    "#     print(parameter)\n",
    "if train_from:\n",
    "    print('Loading model from checkpoint at %s' % opt.train_from)\n",
    "    chk_model = checkpoint['model']\n",
    "    model.load_state_dict(chk_model)\n",
    "    opt.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=opt.learning_rate, weight_decay=opt.weight_decay)\n",
    "\n",
    "if train_from:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "nParams = sum([p.nelement() for p in model.parameters()])\n",
    "print('* number of parameters: %d' % nParams)\n",
    "\n",
    "trainModel(model, train_dataset, valid_dataset, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Randomized Embedding                          | Accuracy | Number of parameters | Training speed |   \n",
    "|----------------------------------------------|----------|----------------------|----------------|\n",
    "| Attention over Attention                     | 69.89%   | 46,835,392           | 13,874 s/epoch |   \n",
    "| Transformer Encoder Attention over Attention | 70.86%   | 47,721,984           | 5,582 s/epoch  |   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Analysis**\n",
    "- In here we can see that Transformer Encoder gives a boost of around 1 percentage which show that Transformer Encoder is a good alternative to the GRU. \n",
    "- More over, one of the best properties of Transformer is allowing parallel computing as all of its components can do that. Here you can see that the training speed is faster, only around 1 and half hour for 1 epoch while the original took nearly 4 hours for 1 epoch.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
