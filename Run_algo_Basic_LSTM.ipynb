{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import zipfile\n",
    "import tarfile\n",
    "import gzip\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0 #select gou\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    \n",
    "else:\n",
    "    print('cuda not available')\n",
    "    gpu_id = -1\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file, max_example=None, relabeling=True):\n",
    "\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    f = open(in_file, 'r', encoding='utf-8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        question = line.strip().lower()\n",
    "        answer = f.readline().strip()\n",
    "        document = f.readline().strip().lower()\n",
    "\n",
    "        if relabeling:\n",
    "            q_words = question.split(' ')\n",
    "            d_words = document.split(' ')\n",
    "            assert answer in d_words\n",
    "\n",
    "            entity_dict = {}\n",
    "            entity_id = 0\n",
    "            for word in d_words + q_words:\n",
    "                if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                    entity_dict[word] = '@entity' + str(entity_id)\n",
    "                    entity_id += 1\n",
    "\n",
    "            q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "            d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "            answer = entity_dict[answer]\n",
    "\n",
    "            question = ' '.join(q_words)\n",
    "            document = ' '.join(d_words)\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        num_examples += 1\n",
    "\n",
    "        f.readline()\n",
    "        if (max_example is not None) and (num_examples >= max_example):\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    print('#Examples: %d' % len(documents))\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=120000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "    print('#Words: %d -> %d' % (len(word_count), len(ls)))\n",
    "    for key in ls[:5]:\n",
    "        print(key)\n",
    "    print('...')\n",
    "    for key in ls[-5:]:\n",
    "        print(key)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    \n",
    "    vocab_dict = {w[0]: index + 2 for (index, w) in enumerate(ls)}\n",
    "    vocab_dict['<UNK>'] = 0\n",
    "    vocab_dict['<PAD>'] = 1\n",
    "    \n",
    "    \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(examples, word_dict, entity_dict):\n",
    "    \"\"\"\n",
    "        Vectorize `examples`.\n",
    "        in_x1, in_x2: sequences for document and question respecitvely.\n",
    "        in_y: label\n",
    "        in_l: whether the entity label occurs in the document.\n",
    "    \n",
    "    \"\"\"\n",
    "    in_x1 = []\n",
    "    in_x2 = []\n",
    "    in_l = np.zeros((len(examples[0]), len(entity_dict)))#.astype(config._floatX)\n",
    "    in_y = []\n",
    "    \n",
    "    max_d = 0\n",
    "    max_q = 0\n",
    "    \n",
    "    \n",
    "    for idx, (d, q, a) in enumerate(zip(examples[0], examples[1], examples[2])):\n",
    "        d_words = d.split(' ')\n",
    "        q_words = q.split(' ')\n",
    "        assert (a in d_words)\n",
    "        seq1 = [word_dict[w] if w in word_dict else 0 for w in d_words]\n",
    "        seq2 = [word_dict[w] if w in word_dict else 0 for w in q_words]\n",
    "        \n",
    "        if max_d < len(seq1):\n",
    "            max_d = len(seq1)\n",
    "        if max_q < len(seq2):\n",
    "            max_q = len(seq2)\n",
    "        \n",
    "        if (len(seq1) > 0) and (len(seq2) > 0):\n",
    "            in_x1.append(seq1)\n",
    "            in_x2.append(seq2)\n",
    "            in_l[idx, [entity_dict[w] for w in d_words if w in entity_dict]] = 1.0\n",
    "            in_y.append(entity_dict[a] if a in entity_dict else 0)\n",
    "        if (idx % 10000 == 0):\n",
    "            print('Vectorization: processed %d / %d' % (idx, len(examples[0])))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return in_x1, in_x2, in_l, in_y, max_d, max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seqs, max_l):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    \n",
    "    x = np.zeros((n_samples, max_l)).astype('int32')\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        \n",
    "    return x, np.array(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_examples(x1, x2, l, y, batch_size, max_d, max_q):\n",
    "    \"\"\"\n",
    "        Divide examples into batches of size `batch_size`.\n",
    "    \"\"\"\n",
    "    minibatches = get_minibatches(len(x1), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_x1 = [x1[t] for t in minibatch]\n",
    "        mb_x2 = [x2[t] for t in minibatch]\n",
    "        #mb_l = l[minibatch]\n",
    "        mb_y = [y[t] for t in minibatch]\n",
    "        mb_y = np.array(mb_y)\n",
    "        mb_x1, x1_len = prepare_data(mb_x1, max_d)\n",
    "        mb_x2, x2_len = prepare_data(mb_x2, max_q)\n",
    "        all_ex.append((mb_x1, x1_len, mb_x2, x2_len, mb_y))\n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embeddings_glove(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict` from glove pretrained embeddings.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(in_file)==False:\n",
    "        print('Glove pretrained embedding missing.. Downloading...')\n",
    "        if not os.path.exists('./data/glove/'):\n",
    "            os.makedirs('./data/glove/')\n",
    "        os.system('wget http://nlp.stanford.edu/data/glove.6B.zip -P ./data/glove/')\n",
    "        zip_ref = zipfile.ZipFile('./data/glove/glove.6B.zip', 'r')\n",
    "        zip_ref.extractall('./data/glove/')\n",
    "        zip_ref.close()\n",
    "        \n",
    "    \n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        for line in open(in_file, encoding='utf-8').readlines():\n",
    "            sp = line.split()\n",
    "            \n",
    "            #print(\"Length = \",len(sp))\n",
    "            \n",
    "            assert len(sp) == dim + 1 \n",
    "            if sp[0] in word_dict:\n",
    "                pre_trained += 1\n",
    "                embeddings[word_dict[sp[0]]] = [float(x) for x in sp[1:]]\n",
    "        print('Pre-trained: %d (%.2f%%)' %\n",
    "              (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings_fasttext(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`from fasttext pretrained embeddings.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(in_file)==False:\n",
    "        print('Fasttext pretrained embedding missing.. Downloading...')\n",
    "        if not os.path.exists('./data/fasttext/'):\n",
    "            os.makedirs('./data/fasttext/')\n",
    "        os.system('wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec -P ./data/fasttext/')\n",
    "\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading fasttext embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        with io.open(in_file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 0:\n",
    "                    split = line.split()\n",
    "                    assert len(split) == 2\n",
    "                    #assert _emb_dim_file == int(split[1])\n",
    "                else:\n",
    "                    word, vect = line.rstrip().split(' ', 1)        \n",
    "                    if word in word_dict:\n",
    "                        pre_trained += 1\n",
    "                        vect = np.fromstring(vect, sep=' ')\n",
    "                        #print(vect.shape)\n",
    "                        embeddings[word_dict[word]] = vect\n",
    "        print('Pre-trained: %d (%.2f%%)' % (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings_random(word_dict, dim):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict` using uniform random.\n",
    "    \"\"\"\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings(word_dict, dim, in_file=None, init_from='random'):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`\n",
    "    \"\"\"\n",
    "    if init_from == 'glove':\n",
    "        return gen_embeddings_glove(word_dict, dim, in_file)\n",
    "    elif init_from == 'fasttext':\n",
    "        return gen_embeddings_fasttext(word_dict, dim, in_file)\n",
    "    else:\n",
    "        return gen_embeddings_random(word_dict, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LSTM Model \n",
    "\n",
    "Since the data to be processed sequentially as a baseline we thought of testing with a bi-directional LSTM model as it is proven good for sequential modelling.\n",
    "\n",
    "In this model the tokens of the sentences are embedded and the imbedded tokens for the document related to the questions are sent through a bi-directional LSTM cell and then those for the question are send. Afterwards the final outputs of the final token of the document sequence and that of the question sequence are summed up together to produce the final output answer. These two quantities were summed together in order to get a connection between the document and the question for the prediction of final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, num_layers=1, drp_rate=0.25):\n",
    "        super(Basic_LSTM_model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):   \n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            \n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, (hidden, c) = self.context_lstm(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            #Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            c = c.index_select(1, idx_original)\n",
    "            \n",
    "            return output, hidden, c\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ##For Documents/questions\n",
    "        doc_output, doc_hidden, doc_c = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden, ques_c = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        # output shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        #Obtaining the final output of documents\n",
    "        docs_fwd_h = doc_hidden[0:doc_hidden.size(0):2]\n",
    "        docs_bwd_h = doc_hidden[1:doc_hidden.size(0):2]\n",
    "        docs_hidden = torch.cat([docs_fwd_h, docs_bwd_h], dim=2) \n",
    "        #docs_hidden shape:  torch.Size([1, bs, 256])\n",
    "        \n",
    "        #Obtaining the final output of questions\n",
    "        ques_fwd_h = ques_hidden[0:ques_hidden.size(0):2]\n",
    "        ques_bwd_h = ques_hidden[1:ques_hidden.size(0):2]\n",
    "        ques_hidden = torch.cat([ques_fwd_h, ques_bwd_h], dim=2) \n",
    "        #ques_hidden shape:  torch.Size([1, bs, 256])\n",
    "        \n",
    "        final_op = docs_hidden.squeeze() + ques_hidden.squeeze() \n",
    "        #final_op shape:  torch.Size([bs, 256])\n",
    "        \n",
    "        logits = self.linear(final_op) #logits shape:  torch.Size([bs, numClasses])\n",
    "        #print(\"logits shape: \", logits.size())\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net,optimizer,tr_data):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    run_loss = 0\n",
    "    run_nb_data = 0\n",
    "    run_acc = 0\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(tr_data)\n",
    "    \n",
    "    for ids, (mb_x1, x1_len, mb_x2, x2_len, mb_y) in enumerate(tr_data):\n",
    "        \n",
    "        \n",
    "        x1 = Variable( torch.LongTensor(mb_x1).type(dtypeLong) , requires_grad=False)\n",
    "        x2 = Variable( torch.LongTensor(mb_x2).type(dtypeLong) , requires_grad=False)\n",
    "        target = Variable( torch.LongTensor(mb_y).type(dtypeLong) , requires_grad=False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = net.forward(x1,x2,x1_len,x2_len)\n",
    "        \n",
    "        loss = net.loss(pred,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        run_loss += (loss.detach().item())*float(mb_x1.shape[0])\n",
    "        run_nb_data += float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "        _, batch_predicted_labels = torch.max(pred, dim=1) \n",
    "        \n",
    "        \n",
    "        acc = metrics.accuracy_score(np.array(target), batch_predicted_labels.cpu().numpy())\n",
    "        run_acc += acc*float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "    acc_tr = run_acc/run_nb_data\n",
    "    loss_tr = run_loss/run_nb_data\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    return loss_tr, acc_tr\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(net,tst_data):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    run_loss = 0\n",
    "    run_nb_data = 0\n",
    "    run_acc = 0\n",
    "    \n",
    "    for ids, (mb_x1, x1_len, mb_x2, x2_len, mb_y) in enumerate(tst_data):\n",
    "        \n",
    "        \n",
    "        x1 = Variable( torch.LongTensor(mb_x1).type(dtypeLong) , requires_grad=False)\n",
    "        x2 = Variable( torch.LongTensor(mb_x2).type(dtypeLong) , requires_grad=False)\n",
    "        target = Variable( torch.LongTensor(mb_y).type(dtypeLong) , requires_grad=False)\n",
    "        \n",
    "        \n",
    "        pred = net.forward(x1,x2,x1_len,x2_len)\n",
    "        \n",
    "        loss = net.loss(pred,target)\n",
    "        \n",
    "        run_loss += (loss.detach().item())*float(mb_x1.shape[0])\n",
    "        run_nb_data += float(mb_x1.shape[0])\n",
    "        \n",
    "        _, batch_predicted_labels = torch.max(pred, dim=1) \n",
    "        \n",
    "        \n",
    "        acc = metrics.accuracy_score(np.array(target), batch_predicted_labels.cpu().numpy())\n",
    "        run_acc += acc*float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "    acc_dev = run_acc/run_nb_data\n",
    "    loss_dev = run_loss/run_nb_data\n",
    "    \n",
    "    return loss_dev, acc_dev\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check and prepare data set\n",
    "\n",
    "def check_CNN_dataset_exists(path_data='./data/'):\n",
    "    flag_train_data = os.path.isfile(path_data + 'cnn/train.txt')  \n",
    "    flag_test_data = os.path.isfile(path_data + 'cnn/test.txt') \n",
    "    flag_dev_data = os.path.isfile(path_data + 'cnn/dev.txt') \n",
    "    if flag_train_data==False or flag_test_data==False or flag_dev_data==False:\n",
    "        print('CNN dataset missing - downloading...')\n",
    "        if not os.path.exists(path_data):\n",
    "            os.makedirs(path_data)\n",
    "        url = \"http://cs.stanford.edu/~danqi/data/cnn.tar.gz\"\n",
    "        os.system('wget http://cs.stanford.edu/~danqi/data/cnn.tar.gz -P ./data/')\n",
    "        tar = tarfile.open('./data/cnn.tar.gz', \"r:gz\")\n",
    "        tar.extractall('./data/')\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"CNN dataset is already there!\")\n",
    "        \n",
    "\n",
    "check_CNN_dataset_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 380298\n",
      "#Examples: 3924\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "\n",
    "fin_train = \"./data/cnn/train.txt\" #path of the training data text file.\n",
    "fin_dev = \"./data/cnn/dev.txt\"     #path of the validation data text file.\n",
    "\n",
    "\n",
    "train_exps = load_data(fin_train, relabeling=True)\n",
    "dev_exps = load_data(fin_dev, relabeling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Words: 118432 -> 118432\n",
      "('the', 15383021)\n",
      "(',', 13757778)\n",
      "('.', 11782121)\n",
      "('to', 7208903)\n",
      "('\"', 6967510)\n",
      "...\n",
      "('slingers', 1)\n",
      "('multi-planet', 1)\n",
      "('johnstons', 1)\n",
      "('shir', 1)\n",
      "('khurma', 1)\n"
     ]
    }
   ],
   "source": [
    "#Building dictionaries\n",
    "\n",
    "\n",
    "word_dict = build_dict(train_exps[0] + train_exps[1])\n",
    "entity_markers = list(set([w for w in word_dict.keys() if w.startswith('@entity')] + train_exps[2]))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 118436 x 300\n",
      "Loading embedding file: glove_embed/glove.42B.300d.txt\n",
      "Pre-trained: 102079 (86.19%)\n"
     ]
    }
   ],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: processed 0 / 380298\n",
      "Vectorization: processed 10000 / 380298\n",
      "Vectorization: processed 20000 / 380298\n",
      "Vectorization: processed 30000 / 380298\n",
      "Vectorization: processed 40000 / 380298\n",
      "Vectorization: processed 50000 / 380298\n",
      "Vectorization: processed 60000 / 380298\n",
      "Vectorization: processed 70000 / 380298\n",
      "Vectorization: processed 80000 / 380298\n",
      "Vectorization: processed 90000 / 380298\n",
      "Vectorization: processed 100000 / 380298\n",
      "Vectorization: processed 110000 / 380298\n",
      "Vectorization: processed 120000 / 380298\n",
      "Vectorization: processed 130000 / 380298\n",
      "Vectorization: processed 140000 / 380298\n",
      "Vectorization: processed 150000 / 380298\n",
      "Vectorization: processed 160000 / 380298\n",
      "Vectorization: processed 170000 / 380298\n",
      "Vectorization: processed 180000 / 380298\n",
      "Vectorization: processed 190000 / 380298\n",
      "Vectorization: processed 200000 / 380298\n",
      "Vectorization: processed 210000 / 380298\n",
      "Vectorization: processed 220000 / 380298\n",
      "Vectorization: processed 230000 / 380298\n",
      "Vectorization: processed 240000 / 380298\n",
      "Vectorization: processed 250000 / 380298\n",
      "Vectorization: processed 260000 / 380298\n",
      "Vectorization: processed 270000 / 380298\n",
      "Vectorization: processed 280000 / 380298\n",
      "Vectorization: processed 290000 / 380298\n",
      "Vectorization: processed 300000 / 380298\n",
      "Vectorization: processed 310000 / 380298\n",
      "Vectorization: processed 320000 / 380298\n",
      "Vectorization: processed 330000 / 380298\n",
      "Vectorization: processed 340000 / 380298\n",
      "Vectorization: processed 350000 / 380298\n",
      "Vectorization: processed 360000 / 380298\n",
      "Vectorization: processed 370000 / 380298\n",
      "Vectorization: processed 380000 / 380298\n",
      "Vectorization: processed 0 / 3924\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 128, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 20\n",
    "drp = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Basic_LSTM_model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict),drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results in a .txt file\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"unidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"lstm_basic_sgd\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 3198\n",
      "Vectorization: processed 0 / 3198\n"
     ]
    }
   ],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"   #path of the testing data text file.\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss and accuracy =  1.8046578242675895 \t 0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After 20 epochs with 0.25 drop-out rate the training/validation/testing accuracies obtained were 0.460/0.442/0.462 respectively.\n",
    "\n",
    "From these results it is evident that a basic LSTM module itself is not sufficiently powerful enough to capture and process the relations between the document and the question to predict the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
