{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Comprehension (QA) - Pointer Network-based Model\n",
    "Existing state-of-the-art solutions to the Cloze-form reading comprehension on the CNN dataset typically calculate a probability distribution over the entities in the vocabulary. Without using explicit information about entities (which basically provides an answer candidate list - unintended consequences of anonymising the entities), one way to solve the reading comprehension question-answering problem in an ideal case is to try to model the position/index of the answer in the input passage itself. Pointer Networks (Vinyals, 2015) is a solution that models attention directly over the input.\n",
    "\n",
    "This is a pointer-networks inspired solution to RC, but there are various issues that come up with such an adaptation. One of the main problems is that the dataset provides the solution entity, but does not specify the exact context/sentence where this entity occurs - which means that the entity may appear any number of times in the passage, in different contexts, but we do not know which context is the most useful for predicting the answer to the query. This is problematic because it is necessary to know how many positions are being predicted during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import corenlp\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import corenlp\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "import torch.nn.utils.rnn as R\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "The code expects the CNN dataset (downloaded from https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM) and the Glove 6B file for 300 dimensions. This code extracts words and vectors from the glove.6B.300d.txt file (glove_file = path_to_Glove_file) and saves them as a dictionary.\n",
    "\n",
    "Get Part-of-Speech tags for passage and query sentences (Using Stanford's CoreNLP parser taken from https://stanfordnlp.github.io/CoreNLP/index.html; also need to install python interface from https://github.com/stanfordnlp/python-stanford-corenlp.) Run the Stanford server e.g.: java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 \" and set environment variable  export CORENLP_HOME=~/path_to_stanford-corenlp\n",
    "\n",
    "Extracts word vectors from Glove embeddings dictionary (random initialization for unknown, @entities and @placeholders) and generates two sets of files: one set with glove embeddings for passage and query, another set with glove embeddings + one-hot part-of-speech tags concatenated with glove embeddings. Also generates one-hot representation of answer indices in the same shape as the passage: zeros that correspond to the words in input passage, ones for every answer entity occurrence. Since there are multiple possible answer indices with ones, it is not _technically_ a one-hot representation, but will be referred to as such throughout this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGloveDict(glove_file):\n",
    "\n",
    "    words={}\n",
    "    vectors = []\n",
    "    f = open(glove_file,'r',encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    #lines = lines[0:100000]\n",
    "    print('lines:',len(lines))\n",
    "    for (n,i) in enumerate(tqdm(lines)):\n",
    "    #print(n)\n",
    "    #i=i.split()\n",
    "        i = i.replace('\\r','').replace('\\n','').split(' ')\n",
    "        #print(i)\n",
    "        j = 1\n",
    "        v = []\n",
    "        while j < len(i):\n",
    "            v.append(float(i[j]))\n",
    "            j += 1\n",
    "        words[i[0]]=v\n",
    "\n",
    "    pickle.dump(words, open('gloveDict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(passage, query, answer, glove_vectors, pos_dict, max_sentence_length, max_passage_length, dataOrigin):\n",
    "    \n",
    "    #index top-36 POS tags\n",
    "    pos_dict = {'NNP':0, 'NN':1, 'CC':2, 'CD':3, 'DT':4, 'EX':5, 'FW':6, 'IN':7, 'JJ':8, 'JJR':9, 'JJS':10, 'LS':11, 'MD':12, 'NNS':13, 'NNPS':14, 'PDT':15, 'POS':16, 'PRP':17, 'PRP$':18, 'RB':19, 'RBR':20, 'RBS':21, 'RP':22, 'SYM':23, 'TO':24, 'UH':25, 'VB':26, 'VBD':27, 'VBG':28, 'VBN':29, 'VBP':30, 'VBZ':31, 'WDT':32, 'WP':33, 'WP$':34, 'WRB':35} \t\n",
    "    \n",
    "    #Run Stanford parser to tokenise + generate POS tags\n",
    "    with corenlp.CoreNLPClient(annotators=\"tokenize ssplit pos\".split()) as client:\n",
    "        passagePos = client.annotate(passage)\n",
    "        queryPos = client.annotate(query)\n",
    "        passageSentences = passagePos.sentence #list of sentences!\n",
    "        queryTokens = queryPos.sentence[0].token #list of tokens in only sentence\n",
    "\n",
    "    #print(passageSentences[0])\n",
    "    #print(queryTokens[0])\n",
    "\n",
    "    if len(passageSentences) > max_passage_length:\n",
    "        max_passage_length = len(passageSentences)\n",
    "\n",
    "    passage_vector = []\n",
    "    passage_pos_vector = []\n",
    "    answer_vector = []\n",
    "    entity_vectors = {}\n",
    "    answer_index = 0\n",
    "\n",
    "    for idx,sentence in enumerate(passageSentences):\n",
    "        tokens = sentence.token\n",
    "        sentence_vector = []\n",
    "        sentence_pos_vector = []\n",
    "\n",
    "        if len(tokens) > max_sentence_length:\n",
    "            max_sentence_length = len(tokens)\n",
    "        for eachtoken in tokens:\n",
    "            token = eachtoken.word\n",
    "            tokenpos_vector = torch.zeros(37)\n",
    "            if re.match('@entity', token):\n",
    "                try:\n",
    "                    sentence_vector.append(entity_vectors[token])\n",
    "                except:\n",
    "                    entity_vectors[token] = torch.rand(300)\n",
    "                    sentence_vector.append(entity_vectors[token])\n",
    "                try:\n",
    "                    tokenpos_vector[pos_dict[eachtoken.pos]] = 1\n",
    "                except:\n",
    "                    tokenpos_vector[-1] = 1\n",
    "                sentence_pos_vector.append(torch.cat((entity_vectors[token],tokenpos_vector),0))\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    word_vec = glove_vectors[token.lower]\n",
    "                    sentence_vector.append(word_vec)\n",
    "                except:\n",
    "                    word_vec = torch.rand(300)\n",
    "                sentence_vector.append(word_vec)\n",
    "                try:\n",
    "        \n",
    "                    tokenpos_vector[pos_dict[eachtoken.pos]] = 1\n",
    "                except:\n",
    "                    tokenpos_vector[-1] = 1\n",
    "                \n",
    "                sentence_pos_vector.append(torch.cat((word_vec,tokenpos_vector),0)) #default to Noun\n",
    "\n",
    "                if token==answer:\n",
    "                    answer_vector.append(1)\n",
    "                else:\n",
    "                    answer_vector.append(0)\n",
    "            answer_index+=1\n",
    "\n",
    "\n",
    "        assert len(sentence_vector) == len(sentence_pos_vector)\n",
    "        passage_vector.extend(sentence_vector)\n",
    "        passage_pos_vector.extend(sentence_pos_vector)\n",
    "\n",
    "    query_vector = []\n",
    "    query_pos_vector = []\n",
    "    sentence_vector = []\n",
    "    sentence_pos_vector = []\n",
    "    if len(queryTokens) > max_sentence_length:\n",
    "        max_sentence_length = len(queryTokens)\n",
    "    for eachtoken in queryTokens:\n",
    "        token = eachtoken.word\n",
    "        tokenpos_vector = torch.zeros(37)\n",
    "        if re.match('@placeholder', token):\n",
    "            sentence_vector.append(torch.zeros(300))\n",
    "            try:\n",
    "                tokenpos_vector[pos_dict[eachtoken.pos]] = 1\n",
    "            except:\n",
    "                tokenpos_vector[-1] = 1\n",
    "            sentence_pos_vector.append(torch.cat((torch.rand(300),tokenpos_vector),0))\n",
    "        elif re.match('@entity', token):\n",
    "            try:\n",
    "                sentence_vector.append(entity_vectors[token])\n",
    "            except:\n",
    "                entity_vectors[token] = torch.rand(300)\n",
    "                sentence_vector.append(entity_vectors[token])\n",
    "                try:\n",
    "                    tokenpos_vector[pos_dict[eachtoken.pos]] = 1\n",
    "                except:\n",
    "                    tokenpos_vector[-1] = 1\n",
    "                sentence_pos_vector.append(torch.cat((entity_vectors[token],tokenpos_vector),0))\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                word_vec = glove_vectors[token.lower]\n",
    "                sentence_vector.append(word_vec)\n",
    "            except:\n",
    "                word_vec = torch.rand(300)\n",
    "                sentence_vector.append(word_vec)\n",
    "\n",
    "            try:\n",
    "                tokenpos_vector[pos_dict[eachtoken.pos]] = 1\n",
    "            except:\n",
    "                tokenpos_vector[-1] = 1\n",
    "            sentence_pos_vector.append(torch.cat((word_vec,tokenpos_vector),0))\n",
    "\n",
    "\n",
    "    assert len(sentence_vector) == len(sentence_pos_vector)\n",
    "    query_vector.extend(sentence_vector)\n",
    "    query_pos_vector.extend(sentence_pos_vector)\n",
    "\n",
    "    return passage_vector, passage_pos_vector, query_vector, query_pos_vector, answer_vector, max_passage_length, max_sentence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9.!?@]+\", r\" \", s) #keep @ which is pre-pended to entities and placeholders \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change dataOrigin to \"test\" or \"dev\" to generate input files for each type. Assumes cnn dataset and Glove 300d vectors files are available in the same folder (change dataPath accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOrigin = \"train\"\n",
    "\n",
    "if dataOrigin==\"train\":\n",
    "    dataPath = 'cnn/questions/training'\n",
    "elif dataOrigin==\"dev\":\n",
    "    dataPath = 'cnn/questions/validation'\n",
    "elif dataOrigin==\"test\":\n",
    "    dataPath = 'cnn/questions/test'\n",
    "\n",
    "glove_file = 'glove.6B.300d.txt'\n",
    "\n",
    "getGloveDict(glove_file)\n",
    "\n",
    "max_sentence_length = 0\n",
    "max_passage_length = 0\n",
    "save_folder = dataOrigin+\"_size_full\" \n",
    "os.mkdir(save_folder)\n",
    "\n",
    "glove_vectors = pickle.load(open('gloveDict.pkl', 'rb'))\n",
    "passage_out = open(save_folder+\"/passage_\"+dataOrigin, 'ab')\n",
    "query_out = open(save_folder+\"/query_\"+dataOrigin, 'ab')\n",
    "passage_pos_out = open(save_folder+\"/passage_pos_\"+dataOrigin, 'ab')\n",
    "query_pos_out = open(save_folder+\"/query_pos_\"+dataOrigin, 'ab')\n",
    "ans_out = open(save_folder+\"/ans_\"+dataOrigin, 'ab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_array = []\n",
    "query_array = []\n",
    "passage_pos_array = []\n",
    "query_pos_array = []\n",
    "answer_array = []\n",
    "\n",
    "for i, eachfile in enumerate(os.listdir(dataPath)):\n",
    "\n",
    "    with open(os.path.join(dataPath, eachfile), 'r') as fileint:\n",
    "        text = fileint.read()\n",
    "        inputs = text.split('\\n\\n')\n",
    "\n",
    "        passage = normalizeString(inputs[1])\n",
    "        query = normalizeString(inputs[2])\n",
    "        answer = inputs[3]\n",
    "       \n",
    "\n",
    "        passage_vector, passage_pos_vector, query_vector, query_pos_vector, answer_vector, max_passage_length, max_sentence_length = prepare(passage, query, answer, glove_vectors, pos_dict, max_sentence_length, max_passage_length, dataOrigin)\n",
    "\n",
    "        pickle.dump(passage_vector, passage_out)\n",
    "        pickle.dump(query_vector, query_out)\n",
    "        pickle.dump(passage_pos_vector, passage_pos_out)\n",
    "        pickle.dump(query_pos_vector, query_pos_out)\n",
    "        pickle.dump(answer_vector, ans_out)\n",
    "\n",
    "        #print(i)\n",
    "        \n",
    "\n",
    "print(\"max_passage_length:\", max_passage_length)\n",
    "print(\"max_sentence_length:\", max_sentence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network-based Model\n",
    "The original pointer network attention is $v(\\tanh (W_1 E + W_2 D))$ where E is the encoder output and D is the decoder output, and W_1, W_2 and V are outputs from linear layers. Here, the addition is changed to a dot product, since similarity between the passage (E) and query (D) representations (here obtained through a GRU layer) is required and SELU is used instead of tanh as the non-linear activation function. The final output is probabilities for each index in the passage obtained from a softmax operation.\n",
    "\n",
    "Uncommment/Comment relevant lines under pointer_attention function to use original implementation or ReLU as activation functions. (Number of layers were reduced and GRU used to save time and to enable faster word-wise dot products. Also remove softmax from final layer if decoder [input_passage * attention_weights] is being used - (but) less successful than using softmax over pointer attention weights directly)\n",
    "\n",
    "Number of parameters: 12,717,312 (with POS-tags) 12,660,480 (without POS-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qapointernet(nn.Module):\n",
    "    def __init__(self, hidden_size, dimensions, max_passage_length):\n",
    "        super(qapointernet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #self.passage_gru_layer = nn.GRU(dimensions, hidden_size, num_layers=4, dropout=0.2, batch_first=True,bidirectional=True)\n",
    "        self.passage_gru_layer = nn.GRU(dimensions, hidden_size, batch_first=True,bidirectional=True)\n",
    "        \n",
    "        #self.query_gru_layer = nn.GRU(dimensions, hidden_size, num_layers=4, dropout=0.02, batch_first=True,bidirectional=True)\n",
    "        self.query_gru_layer = nn.GRU(dimensions, hidden_size, batch_first=True,bidirectional=True)\n",
    "        \n",
    "        #self.linear_layer = nn.Linear(hidden_size, max_passage_length)\n",
    "        self.SELU = nn.SELU()\n",
    "        \n",
    "        self.W1 = nn.Linear(2*hidden_size, 2*hidden_size, bias=False)\n",
    "        self.W2 = nn.Linear(2*hidden_size, 2*hidden_size, bias=False)\n",
    "        self.V = nn.Linear(max_passage_length, max_passage_length, bias=False)\n",
    "        \n",
    "\n",
    "    def passage_forward(self, input_passage, h_init, c_init):\n",
    "        \n",
    "        embedded  = input_passage\n",
    "        h_seq, h_final = self.passage_gru_layer(embedded, h_init)\n",
    "        #print(h_final, c_final)\n",
    "        return h_seq, h_final\n",
    "\n",
    "    def query_forward(self, input_query, h_init, c_init):\n",
    "        \n",
    "        embedded = input_query\n",
    "        h_seq, h_final = self.query_gru_layer(embedded, h_init)\n",
    "        return h_seq, h_final\n",
    "\n",
    "    def pointer_attention(self, left_input, right_input):\n",
    "        W_left = self.W1(left_input)\n",
    "        W_right = self.W2(right_input)\n",
    "\n",
    "        #print(W_left.size(), W_right.size())\n",
    "        \n",
    "        #mix_weights = self.V(F.relu(torch.einsum('ijk,ik->ij', (W_left,W_right)))) #relu as non-linear\n",
    "        mix_weights = self.V(self.SELU(torch.einsum('ijk,ik->ij', (W_left,W_right)))) \n",
    "        #mix_weights = self.V(self.SELU(torch.matmul(W_left,W_right.t()))) #word-wise dot product\n",
    "        #mix_weights = self.V(torch.tanh(torch.add(W_left,W_right))) #original pointer network eqn implemention (change V input dims to 2*hidden size and pass h_final states for p and q)\n",
    "        \n",
    "        mix_weights = F.softmax(mix_weights, dim=1) #remove softmax if training with decoder\n",
    "        return mix_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(max_passage_length, input_vector, dimensions=300):\n",
    "    \n",
    "    difference = max_passage_length - len(input_vector)\n",
    "    padding = torch.zeros(difference, dimensions)\n",
    "    input_vector.extend(padding)\n",
    "    \n",
    "    assert len(input_vector) == max_passage_length\n",
    "    return input_vector\n",
    "\n",
    "def repeat_query(max_passage_length, query_vector, dimensions=300):\n",
    "    \n",
    "    num_loops = (max_passage_length//len(query_vector)) + 1\n",
    "    num_loops += num_loops + len(query_vector)\n",
    "    paddingsize = max_passage_length % len(query_vector)\n",
    "    padding = torch.zeros(paddingsize, dimensions)\n",
    "\n",
    "    query_vector = query_vector.repeat(num_loops,1)\n",
    "    query_vector = query_vector[:max_passage_length]\n",
    "\n",
    "    assert len(query_vector) == max_passage_length\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to output files from prepData function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_passage_file = 'train_size_full/passage_train'\n",
    "training_passage_posfile = 'train_size_full/passage_pos_train'\n",
    "training_query_file = 'train_size_full/query_train'\n",
    "training_query_posfile = 'train_size_full/query_pos_train'\n",
    "training_answer_file = 'train_size_full/ans_train'\n",
    "dev_passage_file = 'dev_size_full/passage_dev'\n",
    "dev_passage_posfile = 'dev_size_full/passage_pos_dev'\n",
    "dev_query_file = 'dev_size_full/query_dev'\n",
    "dev_query_posfile = 'dev_size_full/query_pos_dev'\n",
    "dev_answer_file = 'dev_size_full/ans_dev'\n",
    "test_passage_file = 'test_size_full/passage_test'\n",
    "test_passage_posfile = 'test_size_full/passage_pos_test'\n",
    "test_query_file = 'test_size_full/query_test'\n",
    "test_query_posfile = 'test_size_full/query_pos_test'\n",
    "test_answer_file = 'test_size_full/ans_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change dimensions and criterion by commenting/uncommenting accordingly. NLL/CrossEntropy are typically for classification so they are not particularly useful here. MSE and L1 losses cause gradients/weights to reduce to all zeros within 2-3 epochs, since the target output is a sparse matrix with mostly zeros of size max_passage_length. Binary Cross-Entropy loss has slightly better success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_passage_length = 2000\n",
    "max_query_length = 2000\n",
    "dimensions=337 #300 if running without Part of Speech \n",
    "hidden_size = 128\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.BCELoss()\n",
    "#criterion = torch.nn.MSELoss() \n",
    "#criterion = torch.nn.NLLLoss() #CrossEntropyLoss()\n",
    "#criterion = torch.nn.L1Loss()\n",
    "device = torch.device(\"cuda\") #(\"cpu\")\n",
    "start = time.time()\n",
    "total_sample_size=0\n",
    "\n",
    "net = qapointernet(hidden_size, dimensions, max_passage_length)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Strategies\n",
    "\n",
    "Train directly with input or input_with_PartofSpeech (comment/uncomment accordingly). Four different strategies were tried to find a loss that works in this scenario (multiple, differing number of ones in answer one-hot matrix), for which accuracies on the development set (without POS) are reported. The accuracy is calculated by getting the index with the highest predicted probability, and if it matches _any_ index in the list of known answer indices, it is considered a match - since the referred entity would be the same no matter which index in the list is predicted. \n",
    "\n",
    "(1) Train directly with one-hot answer matrix, and do \n",
    "    (a) similarity between passage and query final representations: Unsuccessful (dev acc: __0.4%__)\n",
    "    (b) dot-product for each word in the passage with query representation. Unsuccessful, but works better with (4). (dev acc: __1.86%__)\n",
    "    (c) similarity between passage and query as encoder output, with answer-matrix as decoder output. Unsuccessful. (dev acc: __1.04%__)\n",
    "\n",
    "(2) Train with query expanded to passage length. Essentially trying to match query-length context in the passage; since the correct entity context is just one sentence in the passage. Unsuccessful, but works better when used with (4). (dev acc: __2.37%__)\n",
    "\n",
    "(3) Train with one-hot answer matrix for first 3 epochs or so; then reduce the number of ones predicted in the answer by picking the top K predicted already to train. Reduce K for each epoch until K=1. Also tried with initialising net weights to identity matrices before training. The idea is, if the passage words and query similarity provides a strong enough signal, the answer entity with the correct context is more likely to be preferred early, and with repeated filtering, the answer entity in the correct context might be in the top K and eventually top 1. Unsuccessful. (dev acc: __1.03%__)\n",
    "\n",
    "(4) Sum over the probabilities of all the known answer/gold indices. Target sum is one - basically predict zero probability for all words except answer entities, but individual probabilities are not important. Collectively try to maximise all index probabilities, under the assumption that the correct context will provide maximum contribution. Two parts to this: \n",
    "    (a) Train with decoder input=input_passage. Maximise sum(softmax(input_passage * attention weights)): Unsuccessful. (dev acc: __3.628%__) \n",
    "    (b) Remove decoder. Directly maximise sum(softmax(pointer_attention_weights)): somewhat (relatively) better. (dev acc: __6.67%__)\n",
    "\n",
    "(5) __Possible future work__: \n",
    "    (a) Represent each word in passage by similarity with bi-directional query-size-window context, not including the word itself. Essentially, because the entities/placeholders are randomly initialised and have no similarity with the query, the dependence is entirely on the surrounding context.\n",
    "    (b) Model by stride: pick a reasonable stride size in the passage within which answer entity may not repeat. Learn attention weights for similarity with query with each stride then combine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    with open(training_passage_file, 'rb') as trainpassageint, open(training_passage_posfile, 'rb') as trainpassageposint, open(training_query_file, 'rb') as trainqueryint, open(training_query_posfile, 'rb') as trainqueryposint, open(training_answer_file, 'rb') as trainansint, open(\"logfile\", \"a+\") as logint:\n",
    "        match = 0\n",
    "        if epoch >=2:\n",
    "            learning_rate = learning_rate/3\n",
    "        \n",
    "        optimizer=torch.optim.SGD(net.parameters(), lr=learning_rate ) #RMSprop/Adam stagnate too quickly\n",
    "        running_loss = 0\n",
    "        current_batch_size = 0\n",
    "        current_sample_size = 1\n",
    "        up_batch_passage = torch.Tensor()#unpadded; pad after shuffling\n",
    "        up_batch_query = torch.Tensor()\n",
    "        up_batch_answer = torch.FloatTensor()\n",
    "        batch_input_indices = random.sample(range(batch_size),batch_size) #shuffle \n",
    "        batch_passage = torch.Tensor()#(max_passage_length, dimensions)\n",
    "        batch_query = torch.Tensor()\n",
    "        batch_answer = torch.FloatTensor()\n",
    "        \n",
    "        running_loss = 0\n",
    "\n",
    "        passage_h = torch.zeros(2,batch_size, hidden_size)\n",
    "        passage_c =torch.zeros(2,batch_size, hidden_size)\n",
    "        query_h = torch.zeros(2,batch_size, hidden_size)\n",
    "        query_c = torch.zeros(2,batch_size, hidden_size)\n",
    "\n",
    "        passage_h = passage_h.to(device)\n",
    "        passage_c = passage_c.to(device)\n",
    "        query_h = query_h.to(device)\n",
    "        query_c = query_c.to(device)\n",
    "        \n",
    "        #while trainpassageint: #without part-of-speech\n",
    "        while trainpassageposint: #part-of-speech included\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                #passage = pickle.load(trainpassageint) #without part-of-speech\n",
    "                passage = pickle.load(trainpassageposint) #part-of-speech included\n",
    "                \n",
    "                #query = pickle.load(trainqueryint) #without part-of-speech\n",
    "                query = pickle.load(trainqueryposint) #part-of-speech included\n",
    "                \n",
    "                ans = pickle.load(trainansint)\n",
    "                \n",
    "                current_passage_length = len(passage)\n",
    "                passage_vector = torch.stack(add_padding(max_passage_length, passage, dimensions))\n",
    "              \n",
    "                \n",
    "                #Slightly more successful than expand_as\n",
    "                query_vector = repeat_query(max_query_length, torch.stack(query), dimensions) #torch.stack(add_padding(max_query_length, query, dimensions)) #repeat_query(max_passage_length, torch.stack(query), dimensions)\n",
    "              \n",
    "\n",
    "                ans_pad = torch.zeros(max_passage_length-len(ans))\n",
    "                ans.extend(ans_pad)\n",
    "                answer_vector = torch.FloatTensor(ans)\n",
    "\n",
    "\n",
    "                current_batch_size += 1\n",
    "                current_sample_size += 1\n",
    "                \n",
    "\n",
    "                batch_passage = torch.cat((batch_passage,passage_vector),0)\n",
    "                batch_query = torch.cat((batch_query,query_vector),0)\n",
    "                #up_batch_passage = torch.cat((up_batch_passage,passage),0)\n",
    "                #up_batch_query = torch.cat((up_batch_query,query),0)\n",
    "\n",
    "                batch_answer = torch.cat((batch_answer,answer_vector),0)\n",
    "                #up_batch_answer = torch.cat((up_batch_answer,ans),0)\n",
    "\n",
    "\n",
    "                if current_batch_size == batch_size:\n",
    "                    #do stuff\n",
    "                    #at the end, reset current_batch_size = 0\n",
    "                    #at the end, reset batch_x_vector = []\n",
    "                    #batch_sort_indices = sorted(range(len(up_batch_passage)), key=len(up_batch_passage.__getitem__))\n",
    "                    \n",
    "                    total_sample_size += batch_size\n",
    "                    passage_h=passage_h.detach()\n",
    "                    passage_c=passage_c.detach()\n",
    "                    query_h = query_h.detach()\n",
    "                    query_c = query_c.detach()\n",
    "                    passage_h=passage_h.requires_grad_()\n",
    "                    passage_c=passage_c.requires_grad_()\n",
    "                    query_h = query_h.requires_grad_()\n",
    "                    query_c=query_c.requires_grad_()\n",
    "                     \n",
    "\n",
    "                    input_batch_passage = batch_passage.view(batch_size, max_passage_length,-1)\n",
    "                    input_batch_query = batch_query.view(batch_size, max_query_length, -1)\n",
    "\n",
    "\n",
    "                    input_batch_answer = batch_answer.view(batch_size, max_passage_length)\n",
    "\n",
    "\n",
    "\n",
    "                    input_batch_passage=input_batch_passage.to(device)\n",
    "                    input_batch_query=input_batch_query.to(device)\n",
    "                    #input_batch_answer=input_batch_answer.to(device) #send mutated answer batch to device\n",
    "\n",
    "                    passage_hseq, passage_hfinal = net.passage_forward(input_batch_passage, passage_h, passage_c)\n",
    "                    query_hseq, query_hfinal = net.query_forward(input_batch_query, query_h, query_c)\n",
    "                    query_hfinal_ =  query_hfinal.permute(1,0,2).contiguous().view(batch_size,-1)\n",
    "                    \n",
    "\n",
    "                    #Train with similarity between overall representations of passage and query instead of word by word. Unsuccessful.\n",
    "                    #pointer_attention = net.pointer_attention(passage_hfinal.permute(1,0,2).contiguous().view(batch_size,-1), query_hfinal.permute(1,0,2).contiguous().view(batch_size,-1))\n",
    "                    pointer_attention = net.pointer_attention(passage_hseq, query_hfinal_)\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    batch_gold_indices = torch.nonzero(batch_answer) #get indices for answer_entity (ones) \n",
    "\n",
    "                    print(current_sample_size)\n",
    "\n",
    "                    pred_probs = pointer_attention.view(batch_size*max_passage_length,-1)\n",
    "\n",
    "                                       \n",
    "                    samplewise_pred_probs = torch.empty(batch_size) #Variable(torch.FloatTensor(batch_size), requires_grad=True)\n",
    "                    for i in range(0,batch_size):\n",
    "                        ## Gather indices of answers. Use these to compute sum of possible answer indices.\n",
    "                        ans_idx = torch.nonzero(batch_answer[max_passage_length*i:max_passage_length*i+max_passage_length])\n",
    "                        \n",
    "\n",
    "                        ##Decoder: multiplying input passage embedding with attention weights. Unsucessful.\n",
    "                        #samplewise_pred_probs[i] = torch.sum(F.softmax(torch.einsum('ijk,ij->ij', (passage_hseq,pointer_attention)),dim=1).view(batch_size*max_passage_length,-1).gather(0,ans_idx))\n",
    "\n",
    "                        ##Direct sum over pointer attention. Slightly more successful.\n",
    "                        samplewise_pred_probs[i] = torch.sum(pointer_attention.view(batch_size*max_passage_length,-1).gather(0,ans_idx))\n",
    "\n",
    "                    ### Trains on K answer indices set to ones. Trains on all for the first 3 epochs, then picks top K predicted indicesto train on further, until top 1 is reached. Unsuccessful.\n",
    "                    '''K = initialize before epoch to len(batch_gold_indices)\n",
    "                   \n",
    "                    if K > 2 and epoch>3:\n",
    "                        K = K//2\n",
    "                    elif epoch>3:\n",
    "                        K=1\n",
    "                        \n",
    "                    #print(K)\n",
    "                    #(aval, aidx) = torch.topk(batch_answer,batch_size*K)\n",
    "                    (aval, aidx) = torch.topk(short_list, K, dim=0)\n",
    "                    topk_gold_indices = batch_gold_indices.gather(0,aidx) #batch_gold_indices = short_list indices                    \n",
    "                    batch_gold_outputs = torch.zeros(batch_size*max_passage_length,1)\n",
    "                    short_list = pred_probs.gather(0,batch_gold_indices)\n",
    "                    short_list_sum = torch.sum(short_list)#.to(device)\n",
    "                    batch_gold_outputs.scatter_(0,topk_gold_indices,1) #modified batch gold outputs with topk predictions=1\n",
    "                    batch_gold_outputs=batch_gold_outputs.view(batch_size, max_passage_length)\n",
    "                    #batch_gold_outputs=batch_gold_outputs.to(device)'''\n",
    "                    \n",
    "                    \n",
    "                    ## Train directly on one-hot answer index matrix. Unsuccessful.\n",
    "                    #loss = criterion(pointer_attention, input_batch_answer) \n",
    "                    \n",
    "                    ##Train on sum of answer indices from softmaxed attention. Sum of all probs at the indices corresponding to answer entity from each passage must sum to 1. Collective loss: somewhat successful.\n",
    "                    input_batch_answer_sum=torch.ones(batch_size) #Sum of probs of all answer_indices for each passage in the batch = 1\n",
    "                    \n",
    "                    loss = criterion(samplewise_pred_probs,input_batch_answer_sum)\n",
    "                    #print(loss.item())\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                    #print(\"PRE\")\n",
    "                    #[print(p.size(),p[0][0]) for p in net.parameters()]\n",
    "                    #a = list(net.parameters())[0].clone()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #b = list(net.parameters())[0].clone()\n",
    "                    #print(\"POST: \", torch.equal(a.data, b.data)) # check if gradients are changing\n",
    "                    #print(\"PARAM GRAD:\", list(net.parameters())[0].grad)\n",
    "                    #print(\"POST\")\n",
    "                    #[print(p) for p in net.named_parameters()]\n",
    "                    \n",
    "                    #reset for next batch\n",
    "                    current_batch_size = 0\n",
    "                    batch_passage = torch.Tensor()#(max_passage_length, dimensions)\n",
    "                    batch_query = torch.Tensor()\n",
    "                    batch_answer = torch.FloatTensor()\n",
    "                    \n",
    "                    #Running accuracy. A prediction is correct if the index with highest probability is one of the answer indices. \n",
    "                    (pval, pidx) = torch.topk(pointer_attention, 1, dim=1) \n",
    "                    \n",
    "                    for pred_idx in pidx.view(-1):\n",
    "                        if pred_idx in batch_gold_indices:\n",
    "                            match += 1\n",
    "                \n",
    "\n",
    "                \n",
    "            except EOFError:\n",
    "                break\n",
    "            \n",
    "            \n",
    "        elapsed = time.time()-start\n",
    "        total_loss = running_loss/batch_size\n",
    "        print(\"epoch=\", epoch, \"\\t time=\", elapsed, \"\\t exp(loss)=\", math.exp(total_loss), \"\\t train_acc=\", match, match/total_sample_size)\n",
    "        logint.write(\"epoch=\"+str(epoch)+\"\\t time=\"+str(elapsed)+\"\\t exp(loss)=\"+str(math.exp(total_loss))+\"\\t train_acc=\"+str(match)+\" \"+str( match/total_sample_size)+\"bs: \"+str(batch_size)+\" size:\"+size+\"\\n\")\n",
    "        torch.save({'epoch':epoch, 'model_state_dict':net.state_dict(), 'optimizer_state_dict':optimizer.state_dict(), 'loss': running_loss, 'tsize':size, 'bsize':batch_size, 'hsize':hidden_size},\"model_file_\"+str(epoch))\n",
    "\n",
    "        check_accuracy(dev_passage_file, dev_passage_posfile, dev_query_file, dev_query_posfile, dev_answer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(passage_file, passage_posfile, query_file, query_posfile, answer_file):\n",
    "    with open(passage_file, 'rb') as passageint, open(passage_posfile, 'rb') as passageposint, open(query_file, 'rb') as queryint, open(query_posfile, 'rb') as queryposint, open(answer_file, 'rb') as ansint, open(\"pred_logfile\", \"a+\") as logint:\n",
    "            match = 0\n",
    "\n",
    "            current_batch_size = 0\n",
    "            current_sample_size = 1\n",
    "            batch_passage = torch.Tensor() #(max_passage_length, dimensions)\n",
    "            batch_query = torch.Tensor()\n",
    "            batch_answer = torch.FloatTensor()\n",
    "\n",
    "\n",
    "            passage_h = torch.zeros(2,batch_size, hidden_size)\n",
    "            passage_c =torch.zeros(2,batch_size, hidden_size)\n",
    "            query_h = torch.zeros(2,batch_size, hidden_size)\n",
    "            query_c = torch.zeros(2,batch_size, hidden_size)\n",
    "\n",
    "            passage_h = passage_h.to(device)\n",
    "            passage_c = passage_c.to(device)\n",
    "            query_h = query_h.to(device)\n",
    "            query_c = query_c.to(device)\n",
    "\n",
    "            #while passageposint:\n",
    "            while passageint:\n",
    "\n",
    "                try:\n",
    "                    #passage = pickle.load(trainpassageint)\n",
    "                    passage = pickle.load(passageposint) #part-of-speech included\n",
    "                    #query = pickle.load(trainqueryint)\n",
    "                    query = pickle.load(trainqueryposint) #part-of-speech included\n",
    "                    ans = pickle.load(trainansint)\n",
    "\n",
    "                    current_passage_length = len(passage)\n",
    "                    passage_vector = torch.stack(add_padding(max_passage_length, passage, dimensions))\n",
    "\n",
    "                    query_vector = repeat_query(max_query_length, torch.stack(query), dimensions) #torch.stack(add_padding(max_query_length, query, dimensions)) #repeat_query(max_passage_length, torch.stack(query), dimensions)\n",
    "\n",
    "                    ans_pad = torch.zeros(max_passage_length-len(ans))\n",
    "                    ans.extend(ans_pad)\n",
    "                    answer_vector = torch.FloatTensor(ans)\n",
    "\n",
    "\n",
    "                    current_batch_size += 1\n",
    "                    current_sample_size += 1\n",
    "\n",
    "                    batch_passage = torch.cat((batch_passage,passage_vector),0)\n",
    "                    batch_query = torch.cat((batch_query,query_vector),0)\n",
    "                    batch_answer = torch.cat((batch_answer,answer_vector),0)\n",
    "\n",
    "\n",
    "\n",
    "                    if current_batch_size == batch_size:\n",
    "                        passage_h=passage_h.detach()\n",
    "                        passage_c=passage_c.detach()\n",
    "                        query_h = query_h.detach()\n",
    "                        query_c = query_c.detach()\n",
    "                        passage_h=passage_h.requires_grad_()\n",
    "                        passage_c=passage_c.requires_grad_()\n",
    "                        query_h = query_h.requires_grad_()\n",
    "                        query_c=query_c.requires_grad_()\n",
    "\n",
    "\n",
    "                        input_batch_passage = batch_passage.view(batch_size, max_passage_length,-1)\n",
    "                        input_batch_query = batch_query.view(batch_size, max_query_length, -1)\n",
    "\n",
    "                        input_batch_answer = batch_answer.view(batch_size, max_passage_length)\n",
    "\n",
    "                        input_batch_passage=input_batch_passage.to(device)\n",
    "                        input_batch_query=input_batch_query.to(device)\n",
    "\n",
    "                        passage_hseq, passage_hfinal = net.passage_forward(input_batch_passage, passage_h, passage_c)\n",
    "                        query_hseq, query_hfinal = net.query_forward(input_batch_query, query_h, query_c)\n",
    "                        query_hfinal_ =  query_hfinal.permute(1,0,2).contiguous().view(batch_size,-1)\n",
    "\n",
    "                        pointer_attention = net.pointer_attention(passage_hseq, query_hfinal_)\n",
    "\n",
    "\n",
    "                        batch_gold_indices = torch.nonzero(batch_answer)\n",
    "\n",
    "                        print(current_sample_size)\n",
    "\n",
    "                        pred_probs = pointer_attention.view(batch_size*max_passage_length,-1)\n",
    "\n",
    "\n",
    "                        current_batch_size = 0\n",
    "                        batch_passage = torch.Tensor()#(max_passage_length, dimensions)\n",
    "                        batch_query = torch.Tensor()\n",
    "                        batch_answer = torch.FloatTensor()\n",
    "\n",
    "                        (pval, pidx) = torch.topk(pointer_attention, 1, dim=1) #pick top (max prob) index\n",
    "\n",
    "                        for pred_idx in pidx.view(-1): #if top index is in the list of gold_indices, consider it as correct\n",
    "                            if pred_idx in batch_gold_indices:\n",
    "                                match += 1\n",
    "\n",
    "\n",
    "\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "            print(\"accuracy: \", match/total_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(test_passage_file, test_passage_posfile, test_query_file, test_query_posfile, test_answer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "__Final test data accuracies__: __6.44%__ (without POS) __5.62%__ (with POS) \n",
    "\n",
    "Possible reasons for poor performance:\n",
    "- Target is a unique, sparse matrix of document size, the relevant answer entities are randomly initialised, and to predict indices, the design does not necessarily provide signals from the most useful context.\n",
    "- Loss calculation is not entirely straightforward.\n",
    "- Using a better contextual embedding or more layers/parameters may be helpful. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
