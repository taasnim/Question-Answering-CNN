{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloze Form Question Answering Using Deep Learning \n",
    "\n",
    "This project has been hosted on GitHub at https://github.com/tasnim007/Question-Answering-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    " \n",
    "Machine reading comprehension or question answering is a significant research problem in the field of natural language processing. Traditional natural language processing techniques used heavy, hand-crafted pre-processing of the context, like parsing the document and the question through grammatical and syntactical parsers, extracting verb frames or predicate-argument pairs, etc.\n",
    " \n",
    "Deep learning has emerged as a promising candidate for the task of question answering mainly due to the fact that deep learning models can learn features without heavy pre-processing. Therefore we thought it would be interesting to investigate the application of different deep learning techniques for this question answering task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    " \n",
    "Reading comprehension is a form of a question-answering problem that tests the ability of a machine to read and understand documents by providing a model with a document/context from which the model has to find the answer to a given query. The task which we specifically focused on is called `cloze form question answering`. The speciality of **cloze form** question answering is that the query only has a single word answer, which is present in the document/passage. \n",
    "\n",
    "For our task, we use the CNN dataset proposed by [Hermann et al (2015)](https://arxiv.org/abs/1506.03340). It was created by using news articles from CNN as the document/passage, and generating the query by using an abstractive summary of the passage and replacing one of the words in the query with a placeholder. The task is to predict this placeholder. Since it may be possible to solve the task by training a language model on other news articles, the creators anonymise the words (persons/organizations/locations, etc.) which are possible answer candidates by replacing them with unique, numbered placeholders, henceforth referred to as “entities”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    " \n",
    "The total data set of $387,420$ stories was already divided into a training set of $380,298$ stories, a validation set of $3,924$ stories and a testing set of $3,198$ stories. For the purpose of exploring this data set we analysed how the tokens and entities (candidate answers) are distributed in the passage. Also, another thing we tried to visualize is that how many times the correct answer occurs in the documents along with the distribution of tokens and entities in questions.\n",
    " \n",
    "Therefore we analysed separately the statistics for these divided training, validation and testing sets of data using the histogram distributions and central tendency measures for the above mentioned scenarios with the aid of matplotlib python library for data visualization. (Notebook 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Training dataset\n",
    " \n",
    "In the training data set which consisted of $380,298$ examples, there were $118,432$ unique tokens and $527$ unique entities (candidate answers).\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in training documents:\n",
    "\n",
    "<figure><img src='images/vs_img_1.png'></figure>\n",
    "\n",
    "According to this histogram distribution, in a majority of the documents in the training corpus it can be seen that there are around $250$ to $800$ words and the distribution is skewed leftwards.\n",
    " \n",
    "In a training document it can include from a minimum number of $8$ tokens to a maximum number of $2000$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in training documents are given below.\n",
    " \n",
    "`The mode =  474`\n",
    "\n",
    "`The median = 700.0`\n",
    "\n",
    "`The mean = 761.81`\n",
    "\n",
    "### The Histogram for the distribution of number of entities in training documents.\n",
    " \n",
    "<figure><img src='images/vs_img_2.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the training corpus it can be seen that there are around $30$ to $80$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a training document it can include from a minimum number of $1$ entity to a maximum number of $745$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in training documents are  given below.\n",
    " \n",
    "`The mode =  34`\n",
    "\n",
    "`The median = 49.0`\n",
    "\n",
    "`The mean = 56.09`\n",
    "\n",
    "\n",
    "### The Histogram for the distribution of number of occurrences of the answer entity in training docs.\n",
    " \n",
    "<figure><img src='images/vs_img_3.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the training corpus it can be seen that the answer entity only occurs once and the distribution is skewed leftwards.\n",
    " \n",
    "In a training document the answer entity can occur from a minimum of $1$ time to a maximum of $61$ times.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of occurrences of the answer entity in training documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 4.0`\n",
    "\n",
    "`The mean = 6.2`\n",
    "\n",
    "### The Histogram for the distribution of number of tokens in training questions.\n",
    " \n",
    "<figure><img src='images/vs_img_4.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the training corpus it can be seen that there are around $8 $to $18$ tokens and the distribution is skewed leftwards.\n",
    " \n",
    "In a training question it can include from a minimum number of $1$ tokens to a maximum number of $47$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in training questions are  given below.\n",
    " \n",
    "`The mode =  12`\n",
    "\n",
    "`The median = 12.0`\n",
    "\n",
    "`The mean = 12.47`\n",
    "\n",
    "### The histogram for the distribution of number of entities in training questions.\n",
    " \n",
    "<figure><img src='images/vs_img_5.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the training corpus it can be seen that there are around $0$ to $3$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a training question it can include from a minimum number of $0$ entities to a maximum number of $9$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in training documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 1.0`\n",
    "\n",
    "`The mean = 1.11`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Validation dataset\n",
    " \n",
    "In the validation data set which consisted of $3,924$ examples, there were $24,204$ unique tokens and $187$ unique entities (candidate answers).\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in validation documents.\n",
    "\n",
    "<figure><img src='images/vs_img_6.png'></figure>\n",
    "\n",
    "According to this histogram distribution, in a majority of the documents in the validation corpus it can be seen that there are around $300$ to $700$ words and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation document it can include from a minimum number of 94 tokens to a maximum number of 1989 tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in validation documents are given below.\n",
    " \n",
    "`The mode =  1102`\n",
    "\n",
    "`The median = 710.0`\n",
    "\n",
    "`The mean = 762.74`\n",
    "\n",
    "### The Histogram for the distribution of number of entities in validation documents.\n",
    " \n",
    "<figure><img src='images/vs_img_7.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the validation corpus it can be seen that there are around $20$ to $70$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation document it can include from a minimum number of $2$ entity to a maximum number of $223$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in validation documents are  given below.\n",
    " \n",
    "`The mode =  40`\n",
    "\n",
    "`The median = 48.0`\n",
    "\n",
    "`The mean = 56.57`\n",
    "\n",
    "### The Histogram for the distribution of number of occurrences of the answer entity in validation docs.\n",
    " \n",
    "<figure><img src='images/vs_img_8.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the validation corpus it can be seen that the answer entity is only occurred $1$ time and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation document the answer entity can occur from a minimum number of $1$ times to a maximum number of $44$ times.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of occurrences of the answer entity in validation documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 4.0`\n",
    "\n",
    "`The mean = 6.480632008154944`\n",
    "\n",
    "### The Histogram for the distribution of number of tokens in validation questions.\n",
    " \n",
    "<figure><img src='images/vs_img_9.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the validation corpus it can be seen that there are around $10$ to $20$ tokens and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation question it can include from a minimum number of $3$ tokens to a maximum number of $37$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in validation questions are  given below.\n",
    " \n",
    "`The mode =  12`\n",
    "\n",
    "`The median = 13.0`\n",
    "\n",
    "`The mean = 13.289755351681958`\n",
    "\n",
    "### The Histogram for the distribution of number of entities in validation questions.\n",
    " \n",
    "<figure><img src='images/vs_img_10.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the validation corpus it can be seen that there are around $0$ to $3$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a validation question it can include from a minimum number of $0$ entities to a maximum number of $8$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in validation documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 1.0`\n",
    "\n",
    "`The mean = 1.2313965341488278`\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Testing dataset\n",
    " \n",
    "In the testing data set which consisted of $3,198$ examples, there were $23,145$ unique tokens and $396$ unique entities (candidate answers).\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in testing documents.\n",
    " \n",
    "<figure><img src='images/vs_img_11.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the testing corpus it can be seen that there are around $250$ to $800$ words and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing document it can include from a minimum number of $89$ tokens to a maximum number of $1989$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in testing documents are given below.\n",
    " \n",
    "`The mode =  1095`\n",
    "\n",
    "`The median = 652.0`\n",
    "\n",
    "`The mean = 716.442464040025`\n",
    " \n",
    "### The Histogram for the distribution of number of entities in testing documents.\n",
    " \n",
    "<figure><img src='images/vs_img_12.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the testing corpus it can be seen that there are around $1$ to $80$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing document it can include from a minimum number of $4$ entity to a maximum number of $584$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in testing documents are  given below.\n",
    " \n",
    "`The mode =  31`\n",
    "\n",
    "`The median = 43.0`\n",
    "\n",
    "`The mean = 51.515322076297686`\n",
    " \n",
    "### The Histogram for the distribution of number of occurrences of the answer entity in testing docs.\n",
    " \n",
    "<figure><img src='images/vs_img_13.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the documents in the testing corpus it can be seen that the answer entity is only occurred $1$ time and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing document the answer entity can occur from a minimum number of $1$ times to a maximum number of $47$ times.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of occurrences of the answer entity in testing documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 4.0`\n",
    "\n",
    "`The mean = 6.363352095059412`\n",
    " \n",
    "### The Histogram for the distribution of number of tokens in testing questions.\n",
    " \n",
    "<figure><img src='images/vs_img_14.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the testing corpus it can be seen that there are around $8$ to $15$ tokens and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing question it can include from a minimum number of $3$ tokens to a maximum number of $37$ tokens.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of tokens in testing questions are  given below.\n",
    " \n",
    "`The mode =  13`\n",
    "\n",
    "`The median = 13.0`\n",
    "\n",
    "`The mean = 13.72514071294559`\n",
    " \n",
    "### The Histogram for the distribution of number of entities in testing questions.\n",
    " \n",
    "<figure><img src='images/vs_img_15.png'></figure>\n",
    " \n",
    "According to this histogram distribution, in a majority of the questions in the testing corpus it can be seen that there are around $0$ to $2$ entities and the distribution is skewed leftwards.\n",
    " \n",
    "In a testing question it can include from a minimum number of $0$ entities to a maximum number of $6$ entities.\n",
    " \n",
    "Three statistic values for the central tendency of this distribution of number of entities in testing documents are  given below.\n",
    " \n",
    "`The mode =  1`\n",
    "\n",
    "`The median = 1.0`\n",
    "\n",
    "`The mean = 1.1106941838649156`\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation in the whole data set is that most frequent words are the articles like “the”, “to”. “of”, “an”, “a” and “in” as expected.\n",
    " \n",
    "Comparing all these distributions with respect to tokens and entities in all training, validation, and testing data sets we can see that there is a common left skewness and in general, all the distributions tend to be similar in the 3 sets of training, validation and testing data. Therefore, we can assume that the training of our models will work for the validation and testing sets. Also, another observation is that in all the 3 sets most of the times the answer entity only occurs one time in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Models Code and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you are running on the virtual environment mentioned in the course [Github repository](https://github.com/xbresson/CE7454_2018). If not please run the `environment.yml` file. \n",
    "\n",
    "\n",
    "If you are running for the first time please pip install the `tarfile` by executing the following command. It will be needed for extracting the downloaded `CNN dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries:\n",
    "First we need to import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import zipfile\n",
    "import tarfile\n",
    "import gzip\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU:\n",
    "\n",
    "It is highly recommended to run this code on GPU.\n",
    "\n",
    "- For *Unidirectional Attention Model* time for a single epoch on GPU (Nvidia GTX 1080 Ti): 0.7hr\n",
    "- For *Bidirectional Attention Model* time for a single epoch on GPU (Nvidia GTX 1080 Ti): 10.2hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0 #select gpu\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    \n",
    "else:\n",
    "    print('cuda not available')\n",
    "    gpu_id = -1\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CNN Dataset:\n",
    "\n",
    "Our CNN dataset's default directory is `./data/cnn/`. There are three files: `train.txt`, `test.txt`, and `dev.txt`.\n",
    "The following function checks whether the CNN dataset is in the default directory or not. If not, then it will download the dataset from [here](http://cs.stanford.edu/~danqi/data/cnn.tar.gz) and extract it to the default directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_CNN_dataset_exists(path_data='./data/'):\n",
    "    flag_train_data = os.path.isfile(path_data + 'cnn/train.txt')  \n",
    "    flag_test_data = os.path.isfile(path_data + 'cnn/test.txt') \n",
    "    flag_dev_data = os.path.isfile(path_data + 'cnn/dev.txt') \n",
    "    if flag_train_data==False or flag_test_data==False or flag_dev_data==False:\n",
    "        print('CNN dataset missing - downloading...')\n",
    "        if not os.path.exists(path_data):\n",
    "            os.makedirs(path_data)\n",
    "        url = \"http://cs.stanford.edu/~danqi/data/cnn.tar.gz\"\n",
    "        os.system('wget http://cs.stanford.edu/~danqi/data/cnn.tar.gz -P ./data/')\n",
    "        tar = tarfile.open('./data/cnn.tar.gz', \"r:gz\")\n",
    "        tar.extractall('./data/')\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"CNN dataset is already there!\")\n",
    "        \n",
    "\n",
    "check_CNN_dataset_exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train, test, and dev data:\n",
    "\n",
    "Original entity markers' indices are arbitrarily generated. [Danqi et. al.](https://arxiv.org/pdf/1606.02858v2.pdf) reported that relabeling the entity markers based on their first occurrence in the passage and question makes training to converge faster.\n",
    "\n",
    "Each of the `train.txt`, `test.txt`, and `dev.txt` file contains contents in the format: `Question, Answer, Document`.\n",
    "\n",
    "In the `load_data()` function, we read a file (`train.txt/test.txt/dev.txt`) and return the all the documents, questions, and answers in separate lists. If `relabeling=True`, we relabel the entity markers based on their first occurrence as stated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file, max_example=None, relabeling=True):\n",
    "    \"\"\"\n",
    "        load CNN data from {train | dev | test}.txt\n",
    "        max_example: if it is None, all the examples will be read.\n",
    "        relabeling: relabel the entities by their first occurence if it is True.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    f = open(in_file, 'r', encoding='utf-8')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        question = line.strip().lower()\n",
    "        answer = f.readline().strip()\n",
    "        document = f.readline().strip().lower()\n",
    "\n",
    "        if relabeling:\n",
    "            q_words = question.split(' ')\n",
    "            d_words = document.split(' ')\n",
    "            assert answer in d_words\n",
    "\n",
    "            entity_dict = {}\n",
    "            entity_id = 0\n",
    "            for word in d_words + q_words:\n",
    "                if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                    entity_dict[word] = '@entity' + str(entity_id)\n",
    "                    entity_id += 1\n",
    "\n",
    "            q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "            d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "            answer = entity_dict[answer]\n",
    "\n",
    "            question = ' '.join(q_words)\n",
    "            document = ' '.join(d_words)\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        num_examples += 1\n",
    "\n",
    "        f.readline()\n",
    "        if (max_example is not None) and (num_examples >= max_example):\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    print('#Examples: %d' % len(documents))\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dictionary\n",
    "\n",
    "In the `build_dict()` function, we build a dictionary of vocabularies. For building the dictionary, we keep the most frequent 120K words. We tried with smaller vocabulary size like 50K, but it did not give the optimal result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=120000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "    print('#Words: %d -> %d' % (len(word_count), len(ls)))\n",
    "    for key in ls[:5]:\n",
    "        print(key)\n",
    "    print('...')\n",
    "    for key in ls[-5:]:\n",
    "        print(key)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    \n",
    "    vocab_dict = {w[0]: index + 2 for (index, w) in enumerate(ls)}\n",
    "    vocab_dict['<UNK>'] = 0\n",
    "    vocab_dict['<PAD>'] = 1\n",
    "    \n",
    "    \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Data (train/test/dev):\n",
    "\n",
    "We need to vectorize our data (documents/questions/answers). For documents and questions, we use the `word_dict` dictionary for vectorizing, while for vectorizing answers we use the `entity_dict` dictionary. Basically, the indices of the dictionaries corresponding to the relevant words are replaced in input data in order to process the inputs into our deep learning models. Meanwhile, the maximum document and question lengths are also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(examples, word_dict, entity_dict):\n",
    "    \"\"\"\n",
    "        Vectorize `examples`.\n",
    "        in_x1, in_x2: sequences for document and question respecitvely.\n",
    "        in_y: label\n",
    "        in_l: whether the entity label occurs in the document.\n",
    "    \n",
    "    \"\"\"\n",
    "    in_x1 = []\n",
    "    in_x2 = []\n",
    "    in_l = np.zeros((len(examples[0]), len(entity_dict)))#.astype(config._floatX)\n",
    "    in_y = []\n",
    "    \n",
    "    max_d = 0\n",
    "    max_q = 0\n",
    "    \n",
    "    \n",
    "    for idx, (d, q, a) in enumerate(zip(examples[0], examples[1], examples[2])):\n",
    "        d_words = d.split(' ')\n",
    "        q_words = q.split(' ')\n",
    "        assert (a in d_words)\n",
    "        seq1 = [word_dict[w] if w in word_dict else 0 for w in d_words]\n",
    "        seq2 = [word_dict[w] if w in word_dict else 0 for w in q_words]\n",
    "        \n",
    "        if max_d < len(seq1):\n",
    "            max_d = len(seq1)\n",
    "        if max_q < len(seq2):\n",
    "            max_q = len(seq2)\n",
    "        \n",
    "        if (len(seq1) > 0) and (len(seq2) > 0):\n",
    "            in_x1.append(seq1)\n",
    "            in_x2.append(seq2)\n",
    "            in_l[idx, [entity_dict[w] for w in d_words if w in entity_dict]] = 1.0\n",
    "            in_y.append(entity_dict[a] if a in entity_dict else 0)\n",
    "        if (idx % 10000 == 0):\n",
    "            print('Vectorization: processed %d / %d' % (idx, len(examples[0])))\n",
    "\n",
    "\n",
    "    return in_x1, in_x2, in_l, in_y, max_d, max_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mini Batches:\n",
    "\n",
    "In order to get the advantage of using GPUs for running our models, we create minibatches of data before inputting them to our model. From this function, we can create the minibatches of data according to desired minibatch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data with Padding:\n",
    "\n",
    "This function is used to pad the data inputs which are less lengthier than the maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs, max_l):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    #max_len = np.max(lengths)\n",
    "    x = np.zeros((n_samples, max_l)).astype('int32')\n",
    "    #x_len = np.zeros((n_samples, 1)).astype(config._floatX)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        #x_len[idx,0] = len(seq)\n",
    "    return x, np.array(lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrating Final Input  Data to Model:\n",
    "\n",
    "This function calls the function to prepare minibatches and then the padding function to pad the the input data accordingly inorder to prepare the final input data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_examples(x1, x2, l, y, batch_size, max_d, max_q):\n",
    "    \"\"\"\n",
    "        Divide examples into batches of size `batch_size`.\n",
    "    \"\"\"\n",
    "    minibatches = get_minibatches(len(x1), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_x1 = [x1[t] for t in minibatch]\n",
    "        mb_x2 = [x2[t] for t in minibatch]\n",
    "        #mb_l = l[minibatch]\n",
    "        mb_y = [y[t] for t in minibatch]\n",
    "        mb_y = np.array(mb_y)\n",
    "        mb_x1, x1_len = prepare_data(mb_x1, max_d)\n",
    "        mb_x2, x2_len = prepare_data(mb_x2, max_q)\n",
    "        all_ex.append((mb_x1, x1_len, mb_x2, x2_len, mb_y))\n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pretrained Embeddings:\n",
    "\n",
    "We tried with three types of pretrained embeddings: [**Glove** from Stanford University](https://nlp.stanford.edu/projects/glove/), [**Fasttext** from facebook](https://fasttext.cc/), and `uniform random embeddings`. We used embedding dimension of 300. Following functions generates pretrained embeddings from glove/fasttext/random according to our specification.\n",
    "\n",
    "The default directory for glove pretrained embeddings is `./data/glove/`.\n",
    "The default directory for glove pretrained embeddings is `./data/fasttext/`\n",
    "If you want to use pretrained embeddings (glove/fasttext) and they are not in the default directory, it will be downloaded by the following codes. \n",
    "\n",
    "**Caution:<font color='red'> Dowloading pretrained embeddings may take some times. </font>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_embeddings_glove(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict` from glove pretrained embeddings.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(in_file)==False:\n",
    "        print('Glove pretrained embedding missing.. Downloading...')\n",
    "        if not os.path.exists('./data/glove/'):\n",
    "            os.makedirs('./data/glove/')\n",
    "        os.system('wget http://nlp.stanford.edu/data/glove.6B.zip -P ./data/glove/')\n",
    "        zip_ref = zipfile.ZipFile('./data/glove/glove.6B.zip', 'r')\n",
    "        zip_ref.extractall('./data/glove/')\n",
    "        zip_ref.close()\n",
    "        \n",
    "    \n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        for line in open(in_file, encoding='utf-8').readlines():\n",
    "            sp = line.split()\n",
    "            \n",
    "            #print(\"Length = \",len(sp))\n",
    "            \n",
    "            assert len(sp) == dim + 1 \n",
    "            if sp[0] in word_dict:\n",
    "                pre_trained += 1\n",
    "                embeddings[word_dict[sp[0]]] = [float(x) for x in sp[1:]]\n",
    "        print('Pre-trained: %d (%.2f%%)' %\n",
    "              (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings_fasttext(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`from fasttext pretrained embeddings.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(in_file)==False:\n",
    "        print('Fasttext pretrained embedding missing.. Downloading...')\n",
    "        if not os.path.exists('./data/fasttext/'):\n",
    "            os.makedirs('./data/fasttext/')\n",
    "        os.system('wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec -P ./data/fasttext/')\n",
    "\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading fasttext embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        with io.open(in_file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 0:\n",
    "                    split = line.split()\n",
    "                    assert len(split) == 2\n",
    "                    #assert _emb_dim_file == int(split[1])\n",
    "                else:\n",
    "                    word, vect = line.rstrip().split(' ', 1)        \n",
    "                    if word in word_dict:\n",
    "                        pre_trained += 1\n",
    "                        vect = np.fromstring(vect, sep=' ')\n",
    "                        #print(vect.shape)\n",
    "                        embeddings[word_dict[word]] = vect\n",
    "        print('Pre-trained: %d (%.2f%%)' % (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings_random(word_dict, dim):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict` using uniform random.\n",
    "    \"\"\"\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "    return embeddings\n",
    "\n",
    "def gen_embeddings(word_dict, dim, in_file=None, init_from='random'):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`\n",
    "    \"\"\"\n",
    "    if init_from == 'glove':\n",
    "        return gen_embeddings_glove(word_dict, dim, in_file)\n",
    "    elif init_from == 'fasttext':\n",
    "        return gen_embeddings_fasttext(word_dict, dim, in_file)\n",
    "    else:\n",
    "        return gen_embeddings_random(word_dict, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional Attention Model\n",
    "\n",
    "The following figure depicts our `Unidirectional Attention Model`.\n",
    "\n",
    "<figure><img src='images/unidirectional_attention.jpg'><figcaption>Figure: Unidirectional Attention Model</figcaption></figure>\n",
    "\n",
    "We first convert each word of the passage to the corresponding **word embedding vectors**. We run our model with both random and pretrained word embeddings like `glove` and `fasttext`. Our word embeddings are trainable in both of the cases. \n",
    "\n",
    "The word embeddings do not capture the context of the words in the sentence. For this reason, we use Recurrent Neural Networks (RNN). We use bidirectional RNNs. The benefit of bidirectional RNN over unidirectional one is that *in bidirectional RNNs a word does not only have the previous words information from the forward RNNs but also have the information from the future words from the backward RNNs*. By concatenating the outputs of the forward and the backward RNNs, we get the **Contextual Embeddings** for each word.\n",
    "\n",
    "For the RNNs, we tried with both LSTMs and GRUs.\n",
    "\n",
    "Similarly for questions, we first convert them to word embeddings. Then by using bidirectional RNNs, we create the **Query Vector** by taking only the last RNN output. Since our query size is small (One single sentence), the bidirectional RNNs (LSTM/GRU) can retain all the information into a single vector. Thus the Query Vector has all the information regarding the corresponding query input. \n",
    "\n",
    "Now the **goal** is to *compare the Query Vector and all the contextual embeddings, and select the pieces of information that are relevant to the question*. For this purpose, we use the `attention mechanism`. We use the *Multiplicative Attention* mechanism to pay attention from the query vector to each of the passage words represented by the contextual embeddings. This gives us the **Attention Scores**. To compute the probability distribution of the attention scores, we use **softmax**. \n",
    "\n",
    "Now we have the **Attention Distribution** values. We compute the **Weighted Score** by *multiplying the attention score to their respective contextual embeddings and summing them up*.\n",
    "\n",
    "Finally, we use a *linear layer* to get the **Logits**.\n",
    "\n",
    "\n",
    "We use **Cross Entropy Loss** for the loss calculation. \n",
    "\n",
    "For optimization, we use  **Stochastic Gradient Descent (SGD)** starting with learning rate $0.1$. We reduce the learning rate by a factor of $1.2$, if `dev_loss > 0.99* dev_loss_old`.\n",
    "\n",
    "We use minibatch SGD with batch size $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unidirectional_Attention_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, num_layers=1, drp_rate=0.25):\n",
    "        super(Unidirectional_Attention_Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_gru = nn.GRU(embedding_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.ws = nn.Linear(hidden_size*2, hidden_size*2) # mult by 2 for bidirectional\n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):   \n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, hidden = self.context_gru(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            #Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            \n",
    "            return output, hidden\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ##For Documents/passages\n",
    "        doc_output, doc_hidden = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        # output shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        ques_fwd_h = ques_hidden[0:ques_hidden.size(0):2]\n",
    "        ques_bwd_h = ques_hidden[1:ques_hidden.size(0):2]\n",
    "        ques_hidden = torch.cat([ques_fwd_h, ques_bwd_h], dim=2)   \n",
    "        #print(\"After hid: \", ques_hidden.size())\n",
    "        \n",
    "        q_ws = self.ws(ques_hidden) #q_ws shape:  torch.Size([1, bs, 256])\n",
    "        #print(\"q_ws shape: \", q_ws.size()) \n",
    "        q_ws = q_ws.squeeze().unsqueeze(1) #q_ws shape:  torch.Size([bs, 1, 256])\n",
    "        q_ws.transpose_(1,2) #q_ws shape:  torch.Size([bs, 256, 1])\n",
    "        q_ws_p = torch.bmm(doc_output, q_ws).squeeze() # q_ws_p shape: torch.Size([bs, timesteps])\n",
    "        #print(\"q_ws_p shape: \", q_ws_p.size())\n",
    "        alpha = F.softmax(q_ws_p, dim=1) #alpha shape:  torch.Size([bs, 1808])\n",
    "        #print(\"alpha shape: \", alpha.size())\n",
    "        attention = torch.mul(alpha.unsqueeze(2), doc_output) #attention shape:  torch.Size([bs, 1808, 256])\n",
    "        #print(\"attention shape: \", attention.size())\n",
    "        attention = torch.sum(attention, dim=1) #After summing attention shape:  torch.Size([bs, 256])\n",
    "        #print(\"After summing attention shape: \", attention.size())\n",
    "        \n",
    "        logits = self.linear(attention) #logits shape:  torch.Size([bs, numClasses])\n",
    "        #print(\"logits shape: \", logits.size())\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        #loss = nn.MSELoss()(y,y_target)\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Train One Single Epoch:\n",
    "\n",
    "This function takes the input data and makes the input minibatches randomly shuffle first. Then once the epoch is over it updates the learning rate according to the given policy and outputs the training loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(net,optimizer,tr_data):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    run_loss = 0\n",
    "    run_nb_data = 0\n",
    "    run_acc = 0\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(tr_data)\n",
    "    \n",
    "    for ids, (mb_x1, x1_len, mb_x2, x2_len, mb_y) in enumerate(tr_data):\n",
    "        \n",
    "        \n",
    "        x1 = Variable( torch.LongTensor(mb_x1).type(dtypeLong) , requires_grad=False)\n",
    "        x2 = Variable( torch.LongTensor(mb_x2).type(dtypeLong) , requires_grad=False)\n",
    "        target = Variable( torch.LongTensor(mb_y).type(dtypeLong) , requires_grad=False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = net.forward(x1,x2,x1_len,x2_len)\n",
    "        \n",
    "        loss = net.loss(pred,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        run_loss += (loss.detach().item())*float(mb_x1.shape[0])\n",
    "        run_nb_data += float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "        _, batch_predicted_labels = torch.max(pred, dim=1) \n",
    "        \n",
    "        \n",
    "        acc = metrics.accuracy_score(np.array(target), batch_predicted_labels.cpu().numpy())\n",
    "        run_acc += acc*float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "    acc_tr = run_acc/run_nb_data\n",
    "    loss_tr = run_loss/run_nb_data\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    return loss_tr, acc_tr\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Test the Validation and the Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_one_epoch(net,tst_data):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    run_loss = 0\n",
    "    run_nb_data = 0\n",
    "    run_acc = 0\n",
    "    \n",
    "    for ids, (mb_x1, x1_len, mb_x2, x2_len, mb_y) in enumerate(tst_data):\n",
    "        \n",
    "        \n",
    "        x1 = Variable( torch.LongTensor(mb_x1).type(dtypeLong) , requires_grad=False)\n",
    "        x2 = Variable( torch.LongTensor(mb_x2).type(dtypeLong) , requires_grad=False)\n",
    "        target = Variable( torch.LongTensor(mb_y).type(dtypeLong) , requires_grad=False)\n",
    "        \n",
    "        \n",
    "        pred = net.forward(x1,x2,x1_len,x2_len)\n",
    "        \n",
    "        loss = net.loss(pred,target)\n",
    "        \n",
    "        run_loss += (loss.detach().item())*float(mb_x1.shape[0])\n",
    "        run_nb_data += float(mb_x1.shape[0])\n",
    "        \n",
    "        _, batch_predicted_labels = torch.max(pred, dim=1) \n",
    "        \n",
    "        \n",
    "        acc = metrics.accuracy_score(np.array(target), batch_predicted_labels.cpu().numpy())\n",
    "        run_acc += acc*float(mb_x1.shape[0])\n",
    "        \n",
    "        \n",
    "    acc_dev = run_acc/run_nb_data\n",
    "    loss_dev = run_loss/run_nb_data\n",
    "    \n",
    "    return loss_dev, acc_dev\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 380298\n",
      "#Examples: 3924\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "\n",
    "fin_train = \"./data/cnn/train.txt\"\n",
    "fin_dev = \"./data/cnn/dev.txt\"\n",
    "\n",
    "\n",
    "train_exps = load_data(fin_train, relabeling=True)\n",
    "dev_exps = load_data(fin_dev, relabeling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Words: 118432 -> 118432\n",
      "('the', 15383021)\n",
      "(',', 13757778)\n",
      "('.', 11782121)\n",
      "('to', 7208903)\n",
      "('\"', 6967510)\n",
      "...\n",
      "('slingers', 1)\n",
      "('multi-planet', 1)\n",
      "('johnstons', 1)\n",
      "('shir', 1)\n",
      "('khurma', 1)\n"
     ]
    }
   ],
   "source": [
    "#Building dictionaries\n",
    "\n",
    "\n",
    "word_dict = build_dict(train_exps[0] + train_exps[1])\n",
    "entity_markers = list(set([w for w in word_dict.keys() if w.startswith('@entity')] + train_exps[2]))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Embeddings\n",
    "Here you can choose your initial embeddings. Use either `pretrained embeddings` (glove/fasttext) or `uniform random embeddings`. **Uncomment the one you want to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 118436 x 300\n",
      "Loading fasttext embedding file: wiki.en.vec\n",
      "Pre-trained: 79469 (67.10%)\n"
     ]
    }
   ],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: processed 0 / 380298\n",
      "Vectorization: processed 10000 / 380298\n",
      "Vectorization: processed 20000 / 380298\n",
      "Vectorization: processed 30000 / 380298\n",
      "Vectorization: processed 40000 / 380298\n",
      "Vectorization: processed 50000 / 380298\n",
      "Vectorization: processed 60000 / 380298\n",
      "Vectorization: processed 70000 / 380298\n",
      "Vectorization: processed 80000 / 380298\n",
      "Vectorization: processed 90000 / 380298\n",
      "Vectorization: processed 100000 / 380298\n",
      "Vectorization: processed 110000 / 380298\n",
      "Vectorization: processed 120000 / 380298\n",
      "Vectorization: processed 130000 / 380298\n",
      "Vectorization: processed 140000 / 380298\n",
      "Vectorization: processed 150000 / 380298\n",
      "Vectorization: processed 160000 / 380298\n",
      "Vectorization: processed 170000 / 380298\n",
      "Vectorization: processed 180000 / 380298\n",
      "Vectorization: processed 190000 / 380298\n",
      "Vectorization: processed 200000 / 380298\n",
      "Vectorization: processed 210000 / 380298\n",
      "Vectorization: processed 220000 / 380298\n",
      "Vectorization: processed 230000 / 380298\n",
      "Vectorization: processed 240000 / 380298\n",
      "Vectorization: processed 250000 / 380298\n",
      "Vectorization: processed 260000 / 380298\n",
      "Vectorization: processed 270000 / 380298\n",
      "Vectorization: processed 280000 / 380298\n",
      "Vectorization: processed 290000 / 380298\n",
      "Vectorization: processed 300000 / 380298\n",
      "Vectorization: processed 310000 / 380298\n",
      "Vectorization: processed 320000 / 380298\n",
      "Vectorization: processed 330000 / 380298\n",
      "Vectorization: processed 340000 / 380298\n",
      "Vectorization: processed 350000 / 380298\n",
      "Vectorization: processed 360000 / 380298\n",
      "Vectorization: processed 370000 / 380298\n",
      "Vectorization: processed 380000 / 380298\n",
      "Vectorization: processed 0 / 3924\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 128, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_data[6][2][50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1 #learning rate\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 20\n",
    "drp = 0.25 #drop out rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damith/miniconda3/envs/deeplearning_project/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Unidirectional_Attention_Model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict),drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [epoch length:1852s | time from start:0.5h] \tlr=1.00e-01\tloss=2.650/2.245\tacc:0.286/0.392 \n",
      "Epoch 1 [epoch length:1839s | time from start:1.0h] \tlr=1.00e-01\tloss=2.259/2.001\tacc:0.410/0.492 \n",
      "Epoch 2 [epoch length:1840s | time from start:1.5h] \tlr=1.00e-01\tloss=1.870/1.610\tacc:0.521/0.579 \n",
      "Epoch 3 [epoch length:1839s | time from start:2.0h] \tlr=1.00e-01\tloss=1.580/1.420\tacc:0.600/0.634 \n",
      "Epoch 4 [epoch length:1848s | time from start:2.6h] \tlr=1.00e-01\tloss=1.410/1.333\tacc:0.641/0.659 \n",
      "Epoch 5 [epoch length:1841s | time from start:3.1h] \tlr=1.00e-01\tloss=1.303/1.298\tacc:0.669/0.663 \n",
      "Epoch 6 [epoch length:1836s | time from start:3.6h] \tlr=1.00e-01\tloss=1.225/1.242\tacc:0.688/0.682 \n",
      "Epoch 7 [epoch length:1840s | time from start:4.1h] \tlr=1.00e-01\tloss=1.168/1.229\tacc:0.703/0.688 \n",
      "Epoch 8 [epoch length:1836s | time from start:4.6h] \tlr=8.33e-02\tloss=1.118/1.242\tacc:0.715/0.681 \n",
      "Epoch 9 [epoch length:1843s | time from start:5.1h] \tlr=8.33e-02\tloss=1.058/1.199\tacc:0.730/0.693 \n",
      "Epoch 10 [epoch length:1840s | time from start:5.6h] \tlr=8.33e-02\tloss=1.025/1.165\tacc:0.739/0.712 \n",
      "Epoch 11 [epoch length:1837s | time from start:6.1h] \tlr=6.94e-02\tloss=0.993/1.194\tacc:0.747/0.708 \n",
      "Epoch 12 [epoch length:1837s | time from start:6.6h] \tlr=6.94e-02\tloss=0.947/1.155\tacc:0.759/0.712 \n",
      "Epoch 13 [epoch length:1839s | time from start:7.2h] \tlr=5.79e-02\tloss=0.922/1.182\tacc:0.766/0.701 \n",
      "Epoch 14 [epoch length:1842s | time from start:7.7h] \tlr=4.82e-02\tloss=0.883/1.175\tacc:0.776/0.707 \n",
      "Epoch 15 [epoch length:1840s | time from start:8.2h] \tlr=4.02e-02\tloss=0.849/1.164\tacc:0.786/0.710 \n",
      "Epoch 16 [epoch length:1839s | time from start:8.7h] \tlr=3.35e-02\tloss=0.819/1.189\tacc:0.794/0.703 \n",
      "Epoch 17 [epoch length:1835s | time from start:9.2h] \tlr=2.79e-02\tloss=0.793/1.188\tacc:0.801/0.707 \n",
      "Epoch 18 [epoch length:1835s | time from start:9.7h] \tlr=2.33e-02\tloss=0.772/1.183\tacc:0.807/0.707 \n",
      "Epoch 19 [epoch length:1841s | time from start:10.2h] \tlr=1.94e-02\tloss=0.753/1.186\tacc:0.812/0.712 \n"
     ]
    }
   ],
   "source": [
    "# save results in a .txt file\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "if not os.path.exists('./logs/'):\n",
    "    os.makedirs('./logs/')\n",
    "file_name = 'logs'+'/'+time_stamp + \"unidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"unidr_fastext_sgd\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 3198\n",
      "Vectorization: processed 0 / 3198\n"
     ]
    }
   ],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss and accuracy =  1.1368197198582113 \t 0.7229518449030644\n"
     ]
    }
   ],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Attention Model\n",
    "\n",
    "The following figure depicts our `Bidirectional Attention Model`.\n",
    "\n",
    "<figure><img src='images/bidirectional_attention.jpg'><figcaption>Figure: Bidirectional Attention Model</figcaption></figure>\n",
    "\n",
    "Here the first couple of layers are similar to `Unidirectional Attention Model`. Unlike unidirectional one, from the query we do not create a single *Query Vector*. Instead, we take all the output of the RNNs to create `Query Contextual Embeddings` just like the `Passage Contextual Embeddings`. \n",
    "Then we use the **Bidirectional Attention Mechanism**. Here every word in `Passage Contextual Embeddings` pays attention to every word in `Query Contextual Embeddings`, resulting `Passage to Query Attention Scores`. Similarly, every word in `Query Contextual Embeddings` pays attention to every word in `Passage Contextual Embeddings`, resulting `Query to Passage Attention Scores`. Finally, these attention scores and contextual embeddings are combined together. We followed the approach mentioned in [Seo et al.](https://arxiv.org/abs/1611.01603). The resultant one encodes *query aware representations* of context words.\n",
    "Next, we use another bidirectional RNN to capture the interactions among passage words conditioned on the query.\n",
    "Finally, we use three linear layers sequentially to get the **Logits**.\n",
    "\n",
    "We use **Cross Entropy Loss** for the loss calculation. \n",
    "\n",
    "For optimization, we use  **Stochastic Gradient Descent (SGD)** starting with learning rate $0.1$. We reduce the learning rate by a factor of $1.2$, if `dev_loss > 0.99* dev_loss_old`.\n",
    "\n",
    "We use minibatch SGD with batch size $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bidirectional_Attention_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, maxlen, num_layers=1, drp_rate=0.25):\n",
    "        super(Bidirectional_Attention_Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.maxlen = maxlen\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_gru = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.sim_W = nn.Linear(hidden_size*6, 1)\n",
    "        \n",
    "        self.modeling_layer = nn.GRU(hidden_size*8, hidden_size, num_layers=2, \n",
    "                                     batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.output_layer_1 = nn.Linear(hidden_size*10, 1)\n",
    "        self.output_layer_2 = nn.Linear(maxlen, 1000)\n",
    "        self.output_layer_3 = nn.Linear(1000, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):\n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, hidden = self.context_gru(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            # Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            # output shape: (batch_size, seq_len, hidden_size * num_directions) --seq_len is the largest lengths in the minibatch\n",
    "            # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "            \n",
    "            return output, hidden\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ## contextual embedding for documents and ques\n",
    "        doc_output, doc_hidden = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        \n",
    "        ## Attention Flow\n",
    "        # Similarity Matrix calcuation\n",
    "        doc_seq_len = doc_output.size(1) # T\n",
    "        ques_seq_len = ques_output.size(1) # J\n",
    "        shape = (doc_output.size(0), doc_seq_len, ques_seq_len, 2*self.hidden_size) # (N, T, J, 2d)\n",
    "        #print(\"T: \", doc_seq_len, \" J: \", ques_seq_len, \" Shape: \", shape)\n",
    "        doc_output_extra = doc_output.unsqueeze(2)  # (N, T, 1, 2d)\n",
    "        doc_output_extra = doc_output_extra.expand(shape) # (N, T, J, 2d)\n",
    "        ques_output_extra = ques_output.unsqueeze(1)  # (N, 1, J, 2d)\n",
    "        ques_output_extra = ques_output_extra.expand(shape) # (N, T, J, 2d)\n",
    "        elmwise_mul = torch.mul(doc_output_extra, ques_output_extra) # (N, T, J, 2d)\n",
    "        #print(\"doc: \", doc_output_extra.size(), \" ques: \", ques_output_extra.size(), \" elem: \", elmwise_mul.size())\n",
    "        cat_data = torch.cat((doc_output_extra, ques_output_extra, elmwise_mul), 3) # (N, T, J, 6d), [h;u;h◦u]\n",
    "        similarity_matrix = self.sim_W(cat_data).squeeze() # (N, T, J)\n",
    "        #print(\"cat: \", cat_data.size(), \" similarity_matrix: \", similarity_matrix.size())\n",
    "        \n",
    "        \n",
    "        a = F.softmax(similarity_matrix, dim=2) # (bs, T, J)\n",
    "        doc2ques_attention = torch.bmm(a, ques_output) # (bs, T, 2*d)\n",
    "        #print(\"a: \", a.size(), \" doc2ques: \", doc2ques_attention.size())\n",
    "        b = F.softmax(torch.max(similarity_matrix, dim=2)[0], dim=1).unsqueeze(1) # (bs, 1, T)\n",
    "        ques2doc_attention = torch.bmm(b, doc_output).squeeze()  # (bs, 2d)\n",
    "        ques2doc_attention = ques2doc_attention.unsqueeze(1).expand(-1, doc_seq_len, -1) # (bs, T, 2*d)\n",
    "        # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
    "\n",
    "        G = torch.cat((doc_output, doc2ques_attention, doc_output.mul(doc2ques_attention), \n",
    "                       doc_output.mul(ques2doc_attention)), 2) # (bs, T, 8d)\n",
    "        #print(\"b: \", b.size(), \" ques2doc: \", ques2doc_attention.size(), \" G: \", G.size())\n",
    "        \n",
    "        ## Modeling Layer\n",
    "        M, _h = self.modeling_layer(G) # M: (bs, T, 2d)\n",
    "        \n",
    "        ## Output Layer\n",
    "        G_M = torch.cat((G, M), 2) # (bs, T, 10d)\n",
    "        G_M = self.output_layer_1(G_M).squeeze() # (bs, T)\n",
    "        G_M = F.pad(G_M, pad=(0,self.maxlen-G_M.size(1),0,0), mode='constant', value=0) # (bs, self.maxlen)\n",
    "\n",
    "        logits = F.relu(self.output_layer_2(G_M)) # (N, 1000)\n",
    "        logits = self.output_layer_3(logits) # (N, output_size)\n",
    "        #print(\"M: \", M.size(), \" G_M: \", G_M.size(), \" logits: \", logits.size())\n",
    "        \n",
    "        #return F.softmax(logits, dim=-1) \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        #loss = nn.MSELoss()(y,y_target)\n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 8, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 8, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 10\n",
    "drp = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Bidirectional_Attention_Model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict), maxlen=max_d, drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save results in a .txt file\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"bidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"Bidr_fasttext_gru\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Our plan was to incrementally improve the model. So we first started with a simple model. The model **does not consider attention**. This is our baseline model. The following figure demonstrates the model.\n",
    "\n",
    "<figure><img src='images/baseline.jpg'><figcaption>Figure: Baseline Model</figcaption></figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_LSTM_model(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size, output_size, num_layers=1, drp_rate=0.25):\n",
    "        super(Basic_LSTM_model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drp = drp_rate\n",
    "        \n",
    "        weight = torch.Tensor(embeddings)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "            \n",
    "        self.context_lstm = nn.LSTM(embedding_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=self.drp)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, doc_x, ques_x, doc_seq_lengths, ques_seq_lengths):   \n",
    "        '''\n",
    "        doc_x: torch tensor. size - (batch_size, doc_seq_len)\n",
    "        ques_x: torch tensor. size - (batch_size, ques_seq_len)\n",
    "        doc_seq_lengths: 1d numpy array containing lengths of each document in doc_x\n",
    "        ques_seq_lengths: 1d numpy array containing lengths of each question in ques_x\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def contextual_embedding(data, seq_lengths):\n",
    "            \n",
    "            # Sort by length (keep idx)\n",
    "            seq_lengths, idx_sort = np.sort(seq_lengths)[::-1], np.argsort(-seq_lengths)\n",
    "            idx_original = np.argsort(idx_sort)\n",
    "            idx_sort = torch.from_numpy(idx_sort).type(dtypeLong)\n",
    "            data = data.index_select(0, idx_sort)\n",
    "\n",
    "            packed_input = pack_padded_sequence(data, seq_lengths, batch_first=True)\n",
    "            packed_output, (hidden, c) = self.context_lstm(packed_input)\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            #print(\"out: \", output.size(), \" hid: \", hidden.size())\n",
    "\n",
    "            #Un-sort by length\n",
    "            idx_original = torch.from_numpy(idx_original).type(dtypeLong)\n",
    "            output = output.index_select(0, idx_original)\n",
    "            hidden = hidden.index_select(1, idx_original)\n",
    "            c = c.index_select(1, idx_original)\n",
    "            \n",
    "            return output, hidden, c\n",
    "\n",
    "        doc_data = self.embeddings(doc_x) # doc_data shape: (batch_size, doc_seq_len, embedding_dim)\n",
    "        ques_data = self.embeddings(ques_x) # ques_data shape: (batch_size, ques_seq_len, embedding_dim)\n",
    "         \n",
    "        ##For Documents/questions\n",
    "        doc_output, doc_hidden, doc_c = contextual_embedding(doc_data, doc_seq_lengths)\n",
    "        ques_output, ques_hidden, ques_c = contextual_embedding(ques_data, ques_seq_lengths)\n",
    "        # output shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        #Obtaining the final output of documents\n",
    "        docs_fwd_h = doc_hidden[0:doc_hidden.size(0):2]\n",
    "        docs_bwd_h = doc_hidden[1:doc_hidden.size(0):2]\n",
    "        docs_hidden = torch.cat([docs_fwd_h, docs_bwd_h], dim=2) \n",
    "        #docs_hidden shape:  torch.Size([1, bs, 256])\n",
    "        \n",
    "        #Obtaining the final output of questions\n",
    "        ques_fwd_h = ques_hidden[0:ques_hidden.size(0):2]\n",
    "        ques_bwd_h = ques_hidden[1:ques_hidden.size(0):2]\n",
    "        ques_hidden = torch.cat([ques_fwd_h, ques_bwd_h], dim=2) \n",
    "        #ques_hidden shape:  torch.Size([1, bs, 256])\n",
    "        \n",
    "        final_op = docs_hidden.squeeze() + ques_hidden.squeeze() \n",
    "        #final_op shape:  torch.Size([bs, 256])\n",
    "        \n",
    "        logits = self.linear(final_op) #logits shape:  torch.Size([bs, numClasses])\n",
    "        #print(\"logits shape: \", logits.size())\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def loss(self, y, y_target):\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr )\n",
    "        \n",
    "        return update\n",
    "    \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating Embeddings\n",
    "\n",
    "embedding_size = 300\n",
    "\n",
    "# embedding can be initialized from glove or fastest pretrained embeddings or from random ones.\n",
    "\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/glove/glove.6B.300d.txt', init_from='glove')\n",
    "#embeddings = gen_embeddings(word_dict, embedding_size, in_file='./data/fasttext/wiki.en.vec', init_from='fasttext')\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, in_file=None, init_from='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorizing and minibatch creation\n",
    "\n",
    "trn_x1, trn_x2, trn_l, trn_y, max_d, max_q = vectorize(train_exps, word_dict, entity_dict)\n",
    "train_data = gen_examples(trn_x1, trn_x2, trn_l, trn_y, 8, max_d, max_q)\n",
    "\n",
    "\n",
    "dev_x1, dev_x2, dev_l, dev_y, _, _ = vectorize(dev_exps, word_dict, entity_dict)\n",
    "dev_data = gen_examples(dev_x1, dev_x2, dev_l, dev_y, 8, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "lr = 0.1\n",
    "decay_rate = 1.2\n",
    "dev_loss_old = 1000000\n",
    "epochs_no = 10\n",
    "drp = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building model\n",
    "\n",
    "\n",
    "net = Basic_LSTM_model(embeddings=embeddings, hidden_size=128, output_size=len(entity_dict),drp_rate=drp)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "opt = net.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save results in a .txt file\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"bidr\" + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "mystr = \"Basic_LSTM\"\n",
    "file.write(mystr+'\\n\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_no):\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc = train_one_epoch(net,opt,train_data)\n",
    "    \n",
    "    dev_loss, dev_acc = test_one_epoch(net,dev_data)\n",
    "    \n",
    "    \n",
    "    # update learning rate \n",
    "    if dev_loss > 0.99* dev_loss_old:\n",
    "        lr /= decay_rate\n",
    "    opt = net.update_learning_rate(opt, lr)\n",
    "    dev_loss_old = dev_loss\n",
    "    \n",
    "    ep_details = ( 'Epoch {EP} [epoch length:{epoch_time:.0f}s | time from start:{fromstart:.1f}h] \\t'\n",
    "                   'lr={LR:.2e}\\t'\n",
    "                   'loss={train_loss:.3f}/{test_loss:.3f}\\t'\n",
    "                   'acc:{train_acc:.3f}/{test_acc:.3f} '.format(\n",
    "                    EP=epoch,\n",
    "                    epoch_time=time.time()-start_epoch,\n",
    "                    fromstart=(time.time()-start)/3600,          \n",
    "                    LR= lr,  \n",
    "                    train_loss=tr_loss, test_loss=dev_loss,\n",
    "                    train_acc=tr_acc, test_acc=dev_acc ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file.write(ep_details+'\\n')\n",
    "    print(ep_details)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_test = \"./data/cnn/test.txt\"\n",
    "test_exps = load_data(fin_test, relabeling=True)\n",
    "\n",
    "tst_x1, tst_x2, tst_l, tst_y, max_d, max_q = vectorize(test_exps, word_dict, entity_dict)\n",
    "test_data = gen_examples(tst_x1, tst_x2, tst_l, tst_y, 128, max_d, max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_loss, tst_acc = test_one_epoch(net,test_data)\n",
    "print(\"Test loss and accuracy = \",tst_loss,\"\\t\",tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Models\n",
    "\n",
    "\n",
    "### Numbers of parameters required for each of the models:\n",
    "\n",
    "\n",
    "| Model                           | #Parameters                  | \n",
    "| ------------------------------- |:----------------------------:| \n",
    "| Baseline                        |                              | \n",
    "| Unidirectional Attention        |  $13057628$ ($13.06$ million)| \n",
    "| Bidirectional Attention         |  $16559438$ ($16.56$ million)| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('There are {} ({:.2f} million) parameters in this neural network'.format(nb_param, nb_param/1e6))\n",
    "    print(nb_param)\n",
    "\n",
    "display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Required for a single epoch \n",
    "\n",
    "| Model                    | RNN Type | Single Epoch Time |\n",
    "|-------------------------:|----------|-------------------|\n",
    "| Baseline                 | GRU      | 0.3hr             |\n",
    "| Unidirectional Attention | GRU      | 0.7hr             |\n",
    "| Unidirectional Attention | LSTM     |                   |\n",
    "| Bidirectional Attention  | GRU      | 5.2hrs            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the Models\n",
    "\n",
    "#### Baseline Model:\n",
    "\n",
    "| RNN Type | Pretrained Embedding | Accuracy |\n",
    "|----------|----------------------|----------|\n",
    "| GRU      | Random               | $46.2$   |\n",
    "\n",
    "\n",
    "#### Unidirectional Attention Model\n",
    "\n",
    "| RNN Type | Pretrained Embedding | Accuracy |\n",
    "|----------|----------------------|----------|\n",
    "| GRU      | Random               | $66.87$  |\n",
    "| GRU      | Glove                | $71.20$  |\n",
    "| GRU      | Fasttext             | $72.29$  |\n",
    "| LSTM     | Random               |          |\n",
    "| LSTM     | Glove                | $71.48$  |\n",
    "| LSTM     | Fasttext             | $70.76$  |\n",
    "\n",
    "\n",
    "#### Bidirectional Attention Model\n",
    "\n",
    "| RNN Type | Pretrained Embedding | Accuracy |\n",
    "|----------|----------------------|----------|\n",
    "| GRU      | Random               |   |\n",
    "| GRU      | Glove                |   |\n",
    "| GRU      | Fasttext             |   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "\n",
    "From the above tables, we can see that we get the best accuracy from `Unidirectional Attention Model` using `GRU` with `Fasttext pretrained embedding`. \n",
    "\n",
    "Our baseline model does not perform on a par with the attention models. The reason is obvious - the model is very simple. It is not possible to capture all the passage information in a single `Passage Vector`. So it performs badly compared to other models.\n",
    "\n",
    "Our `Unidirectional Attention Model` performs quite good. We can see the efficacy of using `pretrained embeddings` like glove or fasttext over `random embeddings` from the results. `Fasttext` is outperforming `glove`. The reason behind this is - alongside contexts fasttext also considers `character n-grams`. This enables fasttext to generate better word embeddings for rare words. \n",
    "We tried with both GRU and LSTM. From the result, we can see there performances are almost similar. But LSTM takes much more time to train.\n",
    "\n",
    "Our `Bidirectional Attention Model` seemed to be promising. But it fails to produce a better result compared to `Unidirectional Attention Model`. We analyzed the model and found that our `Bidirectional Attention Model` overfits the training data. That is why it fails to generalize and performs poorly on test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
